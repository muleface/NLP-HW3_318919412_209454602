{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed15338",
   "metadata": {},
   "source": [
    "## Part 0: Dataset Analysis\n",
    "\n",
    "### Motivation, Contributions and Methodology\n",
    "\n",
    "Existing datasets in the open-domain QA, up until the release of PragmaticCQA, largely focused on evaluating QA systems' accuracy regarding the literal answers to given questions. They did not examine or evaluate a system's ability to infer the questioner's unmentioned needs from context; whether they be in the form of follow-up questions, or relevant information that the questioner is not even necessarily aware of, due to the lack of knowledge in the topic being questioned. This ability to grasp intent is key to efficient and productive conversations, and the fact that it so far went largely ignored by common metrics and evaluation datasets is what motivated the paper's authors to create this dataset, along with the corresponding metrics, to allow for a meaningful examination of this ability in NLP models. \n",
    "\n",
    "Specifically, what they've produced is:\n",
    "\n",
    "* A crowd-sourcing framework that achieves \"incentive alignment\". The authors claim that many of the recent datasets that are crowd-sourced suffer from this \"incentive misalignment\", which essentially means that annotators are rewarded for producing as many examples as they can, and so they create 'basic' examples that they can churn out quickly. These examples tend to lack nuance, or are often similar, and thus allow the model to learn surface-level patterns in order to achieve good results. This naturally goes against the intent of the dataset creators, as it does not truly test for a model's reasoning abilities, which is where the supposed \"incentive misalignment\" stems from.     \n",
    "The authors claim to have solved this issue by allowing the annotators to work on topics they're interested in, and having actually discuss these topics between themselves, which ends up increasing their engagement with the task and producing examples that feel natural and resemble standard human interaction better than other datasets.\n",
    "\n",
    "* An open-domain ConvQA dataset that follows the prior framework, and features pragmatic answers and metrics that allow for the evaluation of pragmatic reasoning.\n",
    "\n",
    "* An analysis of their dataset that shows that it presents a challenge to existing models, proving its relevance in the field.\n",
    "\n",
    "I will now focus on the third point, which is their analysis of the dataset, and why it proves to be challenging to current NLP nodels.\n",
    "\n",
    "The split datasets each contains separate topics, meaning there's no overlap between the topics and thus no overlap of information between questions of different topics. This forces the model to actually generalize and rely on its internal reasoning capabilities.\n",
    "\n",
    "The answers in the dataset tend to be constructed with information from different elements, substantially more so than other, commonly used datasets in the field. This proves to be quite hard for the NLP, as it requires it to collate information from a large number of different sources.\n",
    "\n",
    "The answers are also often formed of a combination of small factoids, and a larger narrative that ties these factoids together with the answer to the original questions. The model should be able to replicate this, and that is more complex than giving literal answers, such as labelling or providing a direct, literal answer to a question without further consideration.\n",
    "\n",
    "These aspects tell us the dataset seeks specific pragmatic phenomena:\n",
    "* The recognition of potential follow-up questions and the inclusion of their answers.\n",
    "* Being cooperative in the conversation: A model should attempt to keep the conversation flowing with the provided answers, whether it be by including relevant information that allows for further discussion, or other such methods that humans employ (Another one would be trying to return the question, or other follow up questions to the student after providing sufficient information, but I'm not sure if the dataset actually covers this case as well).\n",
    "* Being selective in providing information: A model shouldn't just provide a list of connected data, but rather consider the question, the context in which it's asked, like the background of the questioner, and providing relevant information based on these elements.\n",
    "\n",
    "These aspects all serve to complicate the task for NLP models, and thus challenge them.\n",
    "\n",
    "\n",
    "### Sample Analysis\n",
    "\n",
    "1) The topic is 'Vampires', with the starting question being \"So, what is a vampire, exactly?\"\n",
    "\n",
    "   The literal answer we'd expect from a non-cooperative teacher would be something along the lines of \"An undead monster\", which doesn't elaborate greatly on what differentiates vampires from the plethora of other undead monsters in various fictions, like zombies, or even ghosts. \n",
    "\n",
    "   The given answer in the dataset was as follows: \"Vampires are a kind of undead monster that feeds on the life essence of living creatures like humans.\"\n",
    "   This answer provides the full information we expect to see from a literal interpretation of the question - \"A kind of undead monster\", and further specifies that it 'feeds on the life essence of living creatures', prompting the student to equate them to beasts of prey, as they feed on other living beings. This lets the student distinguish between vampires and say, ghosts, that are depicted as malevolent, metaphysical beings that exist to haunt people.\n",
    "\n",
    "   * I've got to say that I don't actually like this answer since life essence is such a weird term. When has anyone ever seen a depiction of vampires that sustain themselves on something other than blood? just say blood...\n",
    "\n",
    "2) The topic is 'The Wheel of Time', with the starting question being \"who was the writer of the wheel of time?\"\n",
    "\n",
    "    The literal answer we'd expect from a non-cooperative teacher could simply state that the writer was Robert Jordan, as he is both an author and the one who wrote the majority of the books, as the student asked about a singular writer. However, it is known that Brandon Sanderson is the one who wrote the latter books, so we expect a pragmatic answer to include this fact, along with the reason why Brandon Sanderson ended up writing the last few books instead of Robert Jordan (Robert Jordan died).\n",
    "    \n",
    "\n",
    "    The given answer in the dataset was as follows: \"Robert Jordan is the author but he sadly passed away and his books were finished by Brandon Sanderson.\"\n",
    "    This answer is more pragmatic as we can easily see the added information as something necessary - Most people wouldn't think beforehand that a book series was written by more than one person, and they would default to asking about a singular writer or author. This is despite them actually wanting to know about all the potential writers, if there were indeed multiple writers. We expect this basic level of inference in daily conversation, and this answer provides that. The literal answer, however, does not.\n",
    "\n",
    "\n",
    "3) The topic is 'Cats Musical Wiki', with the starting question being \"I am a student and know nothing about cats musical wiki\".\n",
    "\n",
    "    This is an interesting 'question' as it's not phrased as a question, but rather, it is a simple statement when taken literally. If used to initiate a conversation, the other party would recognize this as a request to learn about the topic, or an attempt to make small talk by giving the teacher leeway to introduce tidbits of information of their own choosing to the conversation, thus steering it in their desired direction. \n",
    "\n",
    "    Surprisingly enough, both the literal and pragmatic answer spans do not even mention the 'wiki', but rather just talk about the cats musical itself.\n",
    "\n",
    "    The text provided by the dataset for the literal answer just includes details about the creator, the source material, and a range of dates and locations when and where it was played.\n",
    "\n",
    "    The answer was as follows: \"Cats was one of the longest running plays ever, starting in London and running for 21 years. I was lucky enough to sit in the audience in New York city for a performance once.\"\n",
    "\n",
    "    We can see that the teacher actually chose to share his own experience regarding the musical, despite not being prompted for such a thing; they actively chose to steer the conversation to talk about their own experience, and thus proving to be a cooperative conversationalist (And the conversation actually continued down that path, with questions like \"Where were you seated\" and so on), rather than simply providing basic details and closing off the conversation, like we'd expect from a literal answer by a non-cooperative teacher.\n",
    "\n",
    "4) The topic is 'Edward Elric', with the starting question being \"Who is Edward Elric?\".\n",
    "\n",
    "    So a literal answer to this question would be quite succint, such as \"A fictional alchemist in 'The Fullmetal Alchemist'\", \"The main protagonist of 'The Fullmetal Alchemist' series\" and so on. We'd expect a pragmatic answer to both combine these details, and then enrich the answer by giving context; sharing information about the character's traits or background.\n",
    "\n",
    "    The answer given does indeed fulfill these expectations: \"Edward Elric is the main protagonist of the Fullmetal Alchemist series. Edward lost hist right arm and left leg due to a failed Human Transplantation attempt and became the youngest State Alchemist in history at the age of twelve.\"\n",
    "\n",
    "    The answer includes further details than we'd expect from a purely literal interpretation, in line with what one would likely want to know when asking about a fictional character - like a background that hints at the character's motivation, and thus also the main plot of the story.\n",
    "\n",
    "I will stop here since I think this section is wordy enough already.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdaf54",
   "metadata": {},
   "source": [
    "## Part 1: The \"Traditional\" NLP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35ae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import dspy\n",
    "import numpy as np\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from dspy.evaluate import SemanticF1 #no longer necessary\n",
    "import configparser\n",
    "from dspy.evaluate.auto_evaluation import (\n",
    "    SemanticRecallPrecision,\n",
    "    DecompositionalSemanticRecallPrecision\n",
    ")\n",
    "from dspy.predict.chain_of_thought import ChainOfThought\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('grok_key.ini')\n",
    "api_key = config['DEFAULT']['XAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7689f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_questions(filepath='../PragmatiCQA/data/val.jsonl'):\n",
    "\n",
    "    questions = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            conv = json.loads(line)\n",
    "            first_qa = conv['qas'][0]\n",
    "\n",
    "            #the spans will only include the text strings, not the keys\n",
    "            literal_spans = []\n",
    "            pragmatic_spans = []\n",
    "\n",
    "            if 'literal_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['literal_obj']:\n",
    "                    literal_spans.append(span_obj['text'])\n",
    "                    \n",
    "            if 'pragmatic_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['pragmatic_obj']:\n",
    "                    pragmatic_spans.append(span_obj['text'])\n",
    "\n",
    "            questions.append({\n",
    "            'question': first_qa['q'],\n",
    "            'gold_answer': first_qa['a'],\n",
    "            'topic': conv['topic'],\n",
    "            'genre': conv.get('genre', ''),\n",
    "            'community': conv.get('community', ''),\n",
    "            'literal_spans': literal_spans,\n",
    "            'pragmatic_spans': pragmatic_spans\n",
    "            })\n",
    "\n",
    "    return questions\n",
    "\n",
    "questions = get_first_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c94acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'who is freddy krueger?', 'gold_answer': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20N'], 'pragmatic_spans': ['Cannot GET /wiki/A%20N']}\n",
      "{'question': 'who was the star on this movie?', 'gold_answer': \"Robert Englund IS Freddy Kruger, the bad guy for these films. Note to you and to Adam, the Pragmatic one, the link here is broken and I can't paste relevant things, as has always been Nightmare's case, I'm perfectly good with answering your questions and will quickly do it, but have to open a tab in another window separate from the hit, I WILL go quickly and answer at rapid speed though, don't worry.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'What is the movie about?', 'gold_answer': 'Ok, here goes, I\\'m getting \"Cannot get\"..so, Nightmare on Elm street centers around the fact that in your dreams, Freddie Kruger, a dark figure can chase you and if you are killed while sleeping you die.', 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html', 'Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Who directed the new film?', 'gold_answer': \"It was Directed by: Samuel Bayer. Note that the link here is broken. So I'm having to get some of this from memory. I copied what I have (this is ALL I have).\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Is the Batman comic similar to the movies?', 'gold_answer': 'I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane , two very wealthy and charitable Gotham City socialites'], 'pragmatic_spans': ['While returning home one night, his parents were killed by a small-time criminal named Joe Chill ']}\n",
      "{'question': \"what is batman's real name?\", 'gold_answer': 'Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne'], 'pragmatic_spans': ['Batman is a superhero co-created by artist Bob Kane and writer Bill Finger . The character made his first appearance in Detective Comics #27 (May, 1939). Batman is the secret identity of Bruce Wayne .']}\n",
      "{'question': 'How old was batman when he first became batman?', 'gold_answer': \"I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [\"I don't know\"], 'pragmatic_spans': ['his parents were killed', \"Bruce swears an oath to rid the city of the evil that had taken his parents' lives.\\n\\n\"]}\n",
      "{'question': 'Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?', 'gold_answer': 'No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['No'], 'pragmatic_spans': ['intellect, detective skills, science and technology, wealth, physical prowess, and intimidation in his war on crime.']}\n",
      "{'question': \"Who are Batman's biggest enemies?\", 'gold_answer': 'The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' the Joker and Catwoman'], 'pragmatic_spans': [' the Joker and Catwoman', 'a new supervillain called Mr. Bloom ']}\n",
      "{'question': 'What is Batmans real name?', 'gold_answer': \"Batman's real identity is Bruce Wayne. He lives in Gotham City and is the CEO of Wayne Enterprises.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' Batman is the secret identity of Bruce Wayne .'], 'pragmatic_spans': [' CEO of Wayne Enterprises, ']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(questions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775021",
   "metadata": {},
   "source": [
    "As can be seen from this excerpt, there are a few questions with no literal or pragmatic spans at all, and this is not an issue on my end as even the teachers themselves state that they cannot access these wikis in their answers (see first, third and fourth questions). \n",
    "Considering that, and considering that the NLP model requires context, I'm left with two choices:\n",
    "\n",
    "1) Filter out the problematic questions\n",
    "2) Ignore them and set the context to be the same as the question, which will likely lead to errors and underplay distilbert's performance.\n",
    "\n",
    "We'll go with the filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6def7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original questions: 179\n",
      "Valid questions: 174\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_questions(questions):\n",
    "    \n",
    "    valid_questions = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # Check if any spans are invalid (start with \"Cannot GET /wiki/\")\n",
    "        invalid_literal = any(span.startswith(\"Cannot GET /wiki/\") for span in q['literal_spans'])\n",
    "        invalid_pragmatic = any(span.startswith(\"Cannot GET /wiki/\") for span in q['pragmatic_spans'])\n",
    "        \n",
    "        # Keep only questions with valid spans in both configurations\n",
    "        if not invalid_literal and not invalid_pragmatic:\n",
    "            valid_questions.append(q)\n",
    "    \n",
    "    return valid_questions\n",
    "\n",
    "# Execute filtering\n",
    "print(f\"Original questions: {len(questions)}\")\n",
    "questions = filter_valid_questions(questions)\n",
    "print(f\"Valid questions: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b3b81",
   "metadata": {},
   "source": [
    "Five questions have been filtered, which is not a very substantial amount, so it shouldn't really affect our testing.\n",
    "\n",
    "Below is our model which will handle all three contexts - literal, pragmatic and retrieved spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertRAG:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "        retriever = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "        self.embedder = dspy.Embedder(retriever.encode)\n",
    "\n",
    "        self.search_dict = {}  # a cache for retrievers\n",
    "\n",
    "    def create_search(self, community, topk_docs_to_retrieve=5):\n",
    "        \n",
    "        if community in self.search_dict:\n",
    "            return self.search_dict[community]\n",
    "\n",
    "        if not community:\n",
    "            return \"No community given.\"\n",
    "        \n",
    "        directory = f'../PragmatiCQA-sources/{community}'\n",
    "        corpus = []\n",
    "        #just the read_html from rag.ipynb \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".html\"):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    corpus.append(soup.get_text())\n",
    "\n",
    "        search = dspy.retrievers.Embeddings(embedder=self.embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "        self.search_dict[community] = search\n",
    "\n",
    "        return search\n",
    "    \n",
    "    def answer(self, question, context_type):\n",
    "        if context_type == 'literal':\n",
    "            context = \" \".join(question['literal_spans'])\n",
    "\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = \" \".join(question['pragmatic_spans'])\n",
    "\n",
    "        elif context_type == 'rag':\n",
    "            search = self.create_search(question['community'], topk_docs_to_retrieve=3)\n",
    "            result = search(question['question'])\n",
    "            \n",
    "            # truncating each passage since we want to include multiple docs within a small token limit\n",
    "            truncated_passages = []\n",
    "            for passage in result.passages:\n",
    "                truncated_passages.append(passage[:500] + \"...\" if len(passage) > 500 else passage)\n",
    "            \n",
    "            context = \" \".join(truncated_passages)\n",
    "\n",
    "        else:\n",
    "            return \"[Invalid context_type]\"  \n",
    "\n",
    "        # calculate available space for context, this is necessary since we want to minimize context token length since we feed it to the LLM afterwards... and that costs money.\n",
    "        question_tokens = self.tokenizer.encode(question['question'], add_special_tokens=False)\n",
    "        max_context_length = 512 - len(question_tokens) - 3  # 3 for special tokens [CLS], [SEP], [SEP]\n",
    "        \n",
    "        context_tokens = self.tokenizer.encode(context, add_special_tokens=False)\n",
    "        if len(context_tokens) > max_context_length:\n",
    "            context_tokens = context_tokens[:max_context_length]\n",
    "            context = self.tokenizer.decode(context_tokens)\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question['question'], \n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512, #the model can't handle more than 512 tokens as input anyway, so we cap it to prevent errors.\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        start_idx = torch.argmax(outputs.start_logits).item()\n",
    "        end_idx = torch.argmax(outputs.end_logits).item()\n",
    "        \n",
    "        if end_idx < start_idx:\n",
    "            end_idx = start_idx\n",
    "            \n",
    "        tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "        answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer.strip() if answer.strip() else \"[No answer found]\",\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bae14",
   "metadata": {},
   "source": [
    "So I have checked out SemanticF1 - it does not compute all three scores, it really only computes the F1 score, and that's it. I checked all of its attributes, and found nothing else regarding precision and recall. So I'll also be using the function SemanticF1 calls (According to the documentation) instead.\n",
    "\n",
    "Below is a small test with two evaluator LMs as I wanted to see if I can handle using a local evaluator on my 6GB VRAM GFX. \n",
    "\n",
    "Spoilers: The local model performed pretty badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7df6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SigClass = DecompositionalSemanticRecallPrecision\n",
    "sig_module = ChainOfThought(SigClass)\n",
    "\n",
    "def semantic_scores(example, pred):\n",
    "    scores = sig_module(\n",
    "        question=example.question,\n",
    "        ground_truth=example.response,\n",
    "        system_response=pred.response\n",
    "    )\n",
    "    precision = scores.precision\n",
    "    recall = scores.recall\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def evaluate_model(model, n=5):\n",
    "    for i, q in enumerate(questions[:n]):\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Question {i+1}: {q['question']}\")\n",
    "        print(\"Gold Answer:\", q['gold_answer'])\n",
    "\n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            ans = model.answer(q, context_type)\n",
    "\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "\n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "\n",
    "            print(f\"\\n{context_type.capitalize()} Answer: {ans['answer']}\")\n",
    "            print(f\"  Precision: {scores['precision']:.2f}, Recall: {scores['recall']:.2f}, F1: {scores['f1']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c35c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== EVALUATION WITH QWEN 2.5 ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.25, Recall: 0.25, F1: 0.25\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 0.25, Recall: 0.50, F1: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================== EVALUATION WITH GROK-3-MINI ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.50, Recall: 0.25, F1: 0.33\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 1.00, Recall: 0.25, F1: 0.40\n",
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH QWEN 2.5\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_qwen = dspy.LM('ollama_chat/qwen2.5:3b', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=evaluator_lm_qwen)\n",
    "\n",
    "evaluate_model(model, n=3)\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH GROK-3-MINI\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "evaluate_model(model, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c87a9",
   "metadata": {},
   "source": [
    "As can be seen above, the qwen 2.5 model is not good at evaluating. Frankly, neither is grok-3-mini. Answering \"Bruce\" to the question \"How old was batman when he first became batman\" should get a precision of near-zero, if not zero. Definitely not '1'. Anyway, I'll be proceeding with grok-3-mini as the evaluator model. \n",
    "Woe to the grok budget..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ea9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_questions(model, questions):\n",
    "    import time\n",
    "    import warnings\n",
    "    # i'm suppressing the output format warnings since they're frequent and annoying.\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    detailed_results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions...\")\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"Processing question {i + 1}/{len(questions)}.\")\n",
    "            \n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            \n",
    "            ans = model.answer(q, context_type)\n",
    "            \n",
    "            #creating an example like in the semanticf1 example. not sure if passing the context is strictly necessary; it'll be a massive waste of tokens in part 2.\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "            \n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "            results[context_type].append(scores)\n",
    "            \n",
    "            # storing data for future analysis\n",
    "            detailed_results[context_type].append({\n",
    "                \"question\": q['question'],\n",
    "                \"gold_answer\": q['gold_answer'],\n",
    "                \"predicted_answer\": ans['answer'],\n",
    "                \"context\": ans['context'],\n",
    "                \"scores\": scores\n",
    "            })\n",
    "            \n",
    "            # short delay to avoid hitting limits\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # note that i can't use dspy.Evaluate since i need all three (or at least the first two) metrics individually...\n",
    "    avg_results = {}\n",
    "    for context_type in results:\n",
    "        if results[context_type]:  \n",
    "            avg_results[context_type] = {\n",
    "                \"precision\": sum(s[\"precision\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"recall\": sum(s[\"recall\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"f1\": sum(s[\"f1\"] for s in results[context_type]) / len(results[context_type])\n",
    "            }\n",
    "    \n",
    "    json_data = {\n",
    "        \"summary\": avg_results,\n",
    "        \"detailed_results\": detailed_results\n",
    "    }\n",
    "    \n",
    "    with open('part_1_eval.json', 'w') as f:\n",
    "        json.dump(json_data, f)\n",
    "    \n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "def print_results_table(avg_results):\n",
    "    \n",
    "    print(\"\\nEVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Context Type':<20} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "        if context_type in avg_results:\n",
    "            precision = avg_results[context_type][\"precision\"]\n",
    "            recall = avg_results[context_type][\"recall\"]\n",
    "            f1 = avg_results[context_type][\"f1\"]\n",
    "            print(f\"{context_type.capitalize():<20} {precision:<12f} {recall:<12f} {f1:<12}\")\n",
    "        else:\n",
    "            print(f\"{context_type.capitalize():<20} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179ba0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 174 questions...\n",
      "Processing question 1/174.\n",
      "Processing question 2/174.\n",
      "Processing question 3/174.\n",
      "Processing question 4/174.\n",
      "Processing question 5/174.\n",
      "Processing question 6/174.\n",
      "Processing question 7/174.\n",
      "Processing question 8/174.\n",
      "Processing question 9/174.\n",
      "Processing question 10/174.\n",
      "Processing question 11/174.\n",
      "Processing question 12/174.\n",
      "Processing question 13/174.\n",
      "Processing question 14/174.\n",
      "Processing question 15/174.\n",
      "Processing question 16/174.\n",
      "Processing question 17/174.\n",
      "Processing question 18/174.\n",
      "Processing question 19/174.\n",
      "Processing question 20/174.\n",
      "Processing question 21/174.\n",
      "Processing question 22/174.\n",
      "Processing question 23/174.\n",
      "Processing question 24/174.\n",
      "Processing question 25/174.\n",
      "Processing question 26/174.\n",
      "Processing question 27/174.\n",
      "Processing question 28/174.\n",
      "Processing question 29/174.\n",
      "Processing question 30/174.\n",
      "Processing question 31/174.\n",
      "Processing question 32/174.\n",
      "Processing question 33/174.\n",
      "Processing question 34/174.\n",
      "Processing question 35/174.\n",
      "Processing question 36/174.\n",
      "Processing question 37/174.\n",
      "Processing question 38/174.\n",
      "Processing question 39/174.\n",
      "Processing question 40/174.\n",
      "Processing question 41/174.\n",
      "Processing question 42/174.\n",
      "Processing question 43/174.\n",
      "Processing question 44/174.\n",
      "Processing question 45/174.\n",
      "Processing question 46/174.\n",
      "Processing question 47/174.\n",
      "Processing question 48/174.\n",
      "Processing question 49/174.\n",
      "Processing question 50/174.\n",
      "Processing question 51/174.\n",
      "Processing question 52/174.\n",
      "Processing question 53/174.\n",
      "Processing question 54/174.\n",
      "Processing question 55/174.\n",
      "Processing question 56/174.\n",
      "Processing question 57/174.\n",
      "Processing question 58/174.\n",
      "Processing question 59/174.\n",
      "Processing question 60/174.\n",
      "Processing question 61/174.\n",
      "Processing question 62/174.\n",
      "Processing question 63/174.\n",
      "Processing question 64/174.\n",
      "Processing question 65/174.\n",
      "Processing question 66/174.\n",
      "Processing question 67/174.\n",
      "Processing question 68/174.\n",
      "Processing question 69/174.\n",
      "Processing question 70/174.\n",
      "Processing question 71/174.\n",
      "Processing question 72/174.\n",
      "Processing question 73/174.\n",
      "Processing question 74/174.\n",
      "Processing question 75/174.\n",
      "Processing question 76/174.\n",
      "Processing question 77/174.\n",
      "Processing question 78/174.\n",
      "Processing question 79/174.\n",
      "Processing question 80/174.\n",
      "Processing question 81/174.\n",
      "Processing question 82/174.\n",
      "Processing question 83/174.\n",
      "Processing question 84/174.\n",
      "Processing question 85/174.\n",
      "Processing question 86/174.\n",
      "Processing question 87/174.\n",
      "Processing question 88/174.\n",
      "Processing question 89/174.\n",
      "Processing question 90/174.\n",
      "Processing question 91/174.\n",
      "Processing question 92/174.\n",
      "Processing question 93/174.\n",
      "Processing question 94/174.\n",
      "Processing question 95/174.\n",
      "Processing question 96/174.\n",
      "Processing question 97/174.\n",
      "Processing question 98/174.\n",
      "Processing question 99/174.\n",
      "Processing question 100/174.\n",
      "Processing question 101/174.\n",
      "Processing question 102/174.\n",
      "Processing question 103/174.\n",
      "Processing question 104/174.\n",
      "Processing question 105/174.\n",
      "Processing question 106/174.\n",
      "Processing question 107/174.\n",
      "Processing question 108/174.\n",
      "Processing question 109/174.\n",
      "Processing question 110/174.\n",
      "Processing question 111/174.\n",
      "Processing question 112/174.\n",
      "Processing question 113/174.\n",
      "Processing question 114/174.\n",
      "Processing question 115/174.\n",
      "Processing question 116/174.\n",
      "Processing question 117/174.\n",
      "Processing question 118/174.\n",
      "Processing question 119/174.\n",
      "Processing question 120/174.\n",
      "Processing question 121/174.\n",
      "Processing question 122/174.\n",
      "Processing question 123/174.\n",
      "Processing question 124/174.\n",
      "Processing question 125/174.\n",
      "Processing question 126/174.\n",
      "Processing question 127/174.\n",
      "Processing question 128/174.\n",
      "Processing question 129/174.\n",
      "Processing question 130/174.\n",
      "Processing question 131/174.\n",
      "Processing question 132/174.\n",
      "Processing question 133/174.\n",
      "Processing question 134/174.\n",
      "Processing question 135/174.\n",
      "Processing question 136/174.\n",
      "Processing question 137/174.\n",
      "Processing question 138/174.\n",
      "Processing question 139/174.\n",
      "Processing question 140/174.\n",
      "Processing question 141/174.\n",
      "Processing question 142/174.\n",
      "Processing question 143/174.\n",
      "Processing question 144/174.\n",
      "Processing question 145/174.\n",
      "Processing question 146/174.\n",
      "Processing question 147/174.\n",
      "Processing question 148/174.\n",
      "Processing question 149/174.\n",
      "Processing question 150/174.\n",
      "Processing question 151/174.\n",
      "Processing question 152/174.\n",
      "Processing question 153/174.\n",
      "Processing question 154/174.\n",
      "Processing question 155/174.\n",
      "Processing question 156/174.\n",
      "Processing question 157/174.\n",
      "Processing question 158/174.\n",
      "Processing question 159/174.\n",
      "Processing question 160/174.\n",
      "Processing question 161/174.\n",
      "Processing question 162/174.\n",
      "Processing question 163/174.\n",
      "Processing question 164/174.\n",
      "Processing question 165/174.\n",
      "Processing question 166/174.\n",
      "Processing question 167/174.\n",
      "Processing question 168/174.\n",
      "Processing question 169/174.\n",
      "Processing question 170/174.\n",
      "Processing question 171/174.\n",
      "Processing question 172/174.\n",
      "Processing question 173/174.\n",
      "Processing question 174/174.\n"
     ]
    }
   ],
   "source": [
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "avg_results = evaluate_all_questions(model, questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235f59",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68642cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Context Type         Precision    Recall       F1          \n",
      "--------------------------------------------------\n",
      "Literal              0.816571     0.289534     0.40379242303802887\n",
      "Pragmatic            0.744828     0.268122     0.3715092992171465\n",
      "Rag                  0.247222     0.085625     0.11761999779137157\n"
     ]
    }
   ],
   "source": [
    "print_results_table(avg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d533",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Below I'll be printing some excerpts of questions, and then analyzing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08bd2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS OF EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "1. RAG EXAMPLES WITH F1 SCORE = 0.0\n",
      "----------------------------------------\n",
      "Found 126 RAG examples with F1 = 0.0\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1 (Question #1):\n",
      "Question: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "RAG Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 2 (Question #3):\n",
      "Question: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "RAG Answer: February 23, 1948\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 3 (Question #4):\n",
      "Question: Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?\n",
      "Gold Answer: No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.\n",
      "RAG Answer: invisibility\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 4 (Question #5):\n",
      "Question: Who are Batman's biggest enemies?\n",
      "Gold Answer: The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 5 (Question #7):\n",
      "Question: Ok, Is batman a superhero?\n",
      "Gold Answer: Yes, he has been the protector of Gotham City for a long time.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('part_1_eval.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"ANALYSIS OF EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. RAG EXAMPLES WITH F1 SCORE = 0.0\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "rag_zero_examples = []\n",
    "for i, result in enumerate(data['detailed_results']['rag']):\n",
    "    if result['scores']['f1'] == 0.0:\n",
    "        rag_zero_examples.append((i, result))\n",
    "\n",
    "print(f\"Found {len(rag_zero_examples)} RAG examples with F1 = 0.0\")\n",
    "print(\"\\nShowing first 5 examples:\\n\")\n",
    "\n",
    "for idx, (i, example) in enumerate(rag_zero_examples[:5]):\n",
    "    print(f\"Example {idx + 1} (Question #{i + 1}):\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"RAG Answer: {example['predicted_answer']}\")\n",
    "    print(f\"F1 Score: {example['scores']['f1']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a973e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS: LITERAL OUTPERFORMING PRAGMATIC\n",
      "============================================================\n",
      "Example 1 (Question #11):\n",
      "Gap: 0.500 (Literal: 0.500, Pragmatic: 0.000)\n",
      "Question: how old is batman?\n",
      "Gold Answer: Batman made his first appearence in media in May of 1939, so he is quite old. He premiered as a vigilante.\n",
      "Literal Answer: May, 1939\n",
      "Pragmatic Answer: Batman\n",
      "Literal Scores - P: 1.000, R: 0.333\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 2 (Question #13):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: what is batman's real name? \n",
      "Gold Answer: Bruce Wayne is the real name of Batman who, after witnessing the murder of his parents as a child, donned a bat-themed costume in order to fight crime.\n",
      "Literal Answer: Bruce Wayne\n",
      "Pragmatic Answer: his parents\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 3 (Question #15):\n",
      "Gap: 0.667 (Literal: 0.667, Pragmatic: 0.000)\n",
      "Question: what year was it release? \n",
      "Gold Answer: He first appeared in May 1939. The comic's name was Detective comics.\n",
      "Literal Answer: 1939\n",
      "Pragmatic Answer: 27\n",
      "Literal Scores - P: 1.000, R: 0.500\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 4 (Question #24):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: Hi. What is Batman's name?\n",
      "Gold Answer: Batman's real name is Bruce Wayne. Bruce Wayne, born to Thomas Wayne and Martha Kane, is the CEO of Wayne Enterprise, who also protects Gotham as Batman.\n",
      "Literal Answer: Bruce Wayne\n",
      "Pragmatic Answer: Martha Kane\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 5 (Question #25):\n",
      "Gap: 0.500 (Literal: 0.500, Pragmatic: 0.000)\n",
      "Question: When did the Batman comics first appear?\n",
      "Gold Answer: The Batman first appeared in Detective Comics #27 in May 1939, and soon got the title Batman after his introduction\n",
      "Literal Answer: 1939\n",
      "Pragmatic Answer: soon after his introduction\n",
      "Literal Scores - P: 1.000, R: 0.333\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 6 (Question #30):\n",
      "Gap: 0.667 (Literal: 0.667, Pragmatic: 0.000)\n",
      "Question: I filled out the test & clicked submit.  \n",
      "Gold Answer: Batman is known for high level resources and intelligence, a master in martial arts and physical conditions and equipped with high tech gears. He is also a member of Justice League and his alter ego is Bruce Wayne\n",
      "Literal Answer: clicked submit. Genius - level intelligence Master detective Master escapologist Peak human physical condition Master martial artist Access to high tech equipment\n",
      "Pragmatic Answer: clicked\n",
      "Literal Scores - P: 0.667, R: 0.667\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 7 (Question #41):\n",
      "Gap: 1.000 (Literal: 1.000, Pragmatic: 0.000)\n",
      "Question: what year was the show release ? \n",
      "Gold Answer: Good morning!The first American Supernanny show began airing on ABC on January 7, 2005.\n",
      "Literal Answer: 2005\n",
      "Pragmatic Answer: COVID - 19 Pandemic\n",
      "Literal Scores - P: 1.000, R: 1.000\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 8 (Question #43):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: What is Supernanny?\n",
      "Gold Answer: Supernanny is a TV show that started as a British series and was successful enough to broadcast overseas versions including an the US version in 2005.\n",
      "Literal Answer: British TV series\n",
      "Pragmatic Answer: The\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 9 (Question #44):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: What is Supernanny about?\n",
      "Gold Answer: Supernanny is a British reality television programme about parents struggling with their children's behaviour, mealtime, potty training, etc. The show features professional nanny Jo Frost, who devotes each episode to helping a family where the parents are struggling with child-rearing.\n",
      "Literal Answer: The British Supernanny\n",
      "Pragmatic Answer: Moonshine music\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 10 (Question #52):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: what is Supernanny?\n",
      "Gold Answer: Supernanny started off as a British TV series. The show was successful enough that overseas versions of Supernanny were broadcast in France, Germany, Brazil, Portugal, Spain, Denmark, The Netherlands, Israel, Belgium, Austria, Croatia, Poland, Romania, Russia, and Sweden with five further local format versions in production from Chile to Slovakia. Added to this, the UK and US English-language version have been broadcast in 47 different territories.\n",
      "Literal Answer: The first American Supernanny show\n",
      "Pragmatic Answer: child therapist Mike Ruggles\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"ANALYSIS: LITERAL OUTPERFORMING PRAGMATIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate gaps where literal F1 > pragmatic F1\n",
    "literal_results = data['detailed_results']['literal']\n",
    "pragmatic_results = data['detailed_results']['pragmatic']\n",
    "\n",
    "gaps = []\n",
    "for i, (lit_result, prag_result) in enumerate(zip(literal_results, pragmatic_results)):\n",
    "    lit_f1 = lit_result['scores']['f1']\n",
    "    prag_f1 = prag_result['scores']['f1']\n",
    "    \n",
    "    gap = lit_f1 - prag_f1\n",
    "    \n",
    "    # Only consider cases where literal is better by more than 0.2\n",
    "    if gap > 0.2:\n",
    "        gaps.append({\n",
    "            'question_idx': i,\n",
    "            'question': lit_result['question'],\n",
    "            'gold_answer': lit_result['gold_answer'],\n",
    "            'literal_answer': lit_result['predicted_answer'],\n",
    "            'pragmatic_answer': prag_result['predicted_answer'],\n",
    "            'literal_f1': lit_f1,\n",
    "            'pragmatic_f1': prag_f1,\n",
    "            'gap': gap,\n",
    "            'literal_scores': lit_result['scores'],\n",
    "            'pragmatic_scores': prag_result['scores']\n",
    "        })\n",
    "\n",
    "for idx, example in enumerate(gaps[:10]):\n",
    "    print(f\"Example {idx + 1} (Question #{example['question_idx'] + 1}):\")\n",
    "    print(f\"Gap: {example['gap']:.3f} (Literal: {example['literal_f1']:.3f}, Pragmatic: {example['pragmatic_f1']:.3f})\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"Literal Answer: {example['literal_answer']}\")\n",
    "    print(f\"Pragmatic Answer: {example['pragmatic_answer']}\")\n",
    "    print(f\"Literal Scores - P: {example['literal_scores']['precision']:.3f}, R: {example['literal_scores']['recall']:.3f}\")\n",
    "    print(f\"Pragmatic Scores - P: {example['pragmatic_scores']['precision']:.3f}, R: {example['pragmatic_scores']['recall']:.3f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89165699",
   "metadata": {},
   "source": [
    "The most glaring issue of the experiment is the low scoring on the retrieved answers. It's quite easy to see why this is the case.\n",
    "First, we must consider the distilbert model's token limit, which is rather small (512), together with the fact that at the end of our RAG module's embeddings, we end up with the text of entire documents per document, instead of specific, more relevant sentences or passages. Even after stripping all the HTML code and such (Through BeautifulSoup), we still end up with quite a lot of \"fluff\", like section titles and so on, that don't provide a lot of meaningful info. This ends up consuming the majority of the allocated tokens per document, making it so that even if the document is relevant, the retrieved passages are not all that likely to include the relevant information unless it's at the very start of the document.\n",
    "\n",
    "For example, the second question has a retrieved answer of 'February 23, 1948', which is nonsensical in the context of the question - a question about batman's age at a certain period of time. It becomes apparent after a little bit of searching that this is the birth date of some writer of the batman series, and this date was likely just present at the very start of one of the document the module retrieved, and one of the very few pieces of information that are even tangentially related to 'age' or 'dates'.\n",
    "\n",
    "Regarding pragmatic answers, I've printed out a few examples where the literal answer outperforms the pragmatic one, to see if the model repeatedly fails at providing more details. After looking at some examples (I've checked out more examples than shown, this is truncated), the reality is that the pragmatic answers were both short and very literal, while being wrong. I believe this shows a clear limitation of the distilbert model to extract answers, even literal answers, from a span that contains more information on top of what's immediately relevant to the question.\n",
    "\n",
    "The model seems to succeed at extracting relatively simple contextual information, like correlating \"when\" in the question with a date in the span, and so on.\n",
    "\n",
    "Overall, this model proved to be incapable of proper pragmatic inference / answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f75e81",
   "metadata": {},
   "source": [
    "## Part 2: The LLM Multi-Step Prompting Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a062c0",
   "metadata": {},
   "source": [
    "A brief explanation of the model:\n",
    "1) Retriever - it's basically the same as the retriever I've embedded directly in the previous model for the traditional approach. Using a dictionary as a corpus cache since I can spare the memory for it (Besides, we're only evaluating part of the dataset at once, and not the entire thing).\n",
    "\n",
    "2) QAModel - It's a model designed to handle a single question of a conversation, with I/O fields defined specifically for that purpose. I've decided to incorporate the reasoning and summary strategies to try and improve performance as they don't require a whole lot of effort to implement, and are likely to improve results. The summary is fed to the model with every question, and the model is expected to return an updated summary, in a form that's hopefully not a list of QA pairs, which includes the current QA pair.\n",
    "\n",
    "    There should be an emphasis on the questions, rather than the answers within the summary, as the answers stem from the model itself and are not ground_truth; they could throw off future answers.\n",
    "\n",
    "3) ConversationModel - This model is mainly just a wrapper for the QAModel that maintains the summary object across questions within a single conversation. The method of running this model is simply calling process_question iteratively for every question in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "532fb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        retriever = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "        self.embedder = dspy.Embedder(retriever.encode)\n",
    "        self.search_dict = {}\n",
    "\n",
    "    def create_search(self, community, topk_docs_to_retrieve=5):\n",
    "        if community in self.search_dict:\n",
    "            return self.search_dict[community]\n",
    "\n",
    "        if not community:\n",
    "            return None\n",
    "        \n",
    "        directory = f'../PragmatiCQA-sources/{community}'\n",
    "        corpus = []\n",
    "        \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".html\"):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    corpus.append(soup.get_text())\n",
    "\n",
    "        search = dspy.retrievers.Embeddings(embedder=self.embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "        self.search_dict[community] = search\n",
    "        return search\n",
    "    \n",
    "\n",
    "class CooperativeTeaching(dspy.Signature):\n",
    "            \n",
    "            question = dspy.InputField(desc=\"The current question from the student.\")\n",
    "            topic = dspy.InputField(desc=\"The main topic being discussed.\")\n",
    "            context = dspy.InputField(desc=\"Retrieved context relevant to the question (max 10k characters).\")\n",
    "            previous_summary = dspy.InputField(desc=\"Summary of the conversation so far, focusing on student interests and queries\")\n",
    "\n",
    "            reasoning = dspy.OutputField(desc=\"A short justification for your answer, no more than a single sentence\")\n",
    "            answer = dspy.OutputField(desc=\"Cooperative answer that predicts other relevant information the student while need, while staying focused and no longer than two sentences.\")\n",
    "            updated_summary = dspy.OutputField(desc=\" A new summary that outlines both the previous summary and the current question and answer. No more than 200 words.\")\n",
    "        \n",
    "\n",
    "\n",
    "class QAModel(dspy.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base_prompt = (\n",
    "            \"You are a teacher who provides answers to their students' questions. \"\n",
    "            \"You are given a single question, which is part of on-going discussion with a specific student. \" \n",
    "            \"You are tasked with answering their question in a helpful and cooperative manner, that keeps the conversation flow going. \" \n",
    "            \"Your answer should be derived largely from the provided context, which are excerpts from relevant pages of the Fandom wiki which corresponds to the topic of discussion. \"\n",
    "            \"You are also tasked with maintaining an up-to-date summary of all student-teacher interactions in this conversation, which should include the current question and answer. \" \n",
    "            \"The summary should be natural, and be helpful to a teacher who might have to answer the next question of the student in a cooperative manner. \"\n",
    "            \"Also provide a short reasoning for your answer, no more than a sentence long. \"\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.generate_response = dspy.ChainOfThought(\n",
    "            signature=CooperativeTeaching,\n",
    "            prompt=base_prompt\n",
    "        )\n",
    "    \n",
    "    def limit_summary_chars(self, summary, max_chars=1000):\n",
    "        if len(summary) > max_chars:\n",
    "            return summary[:max_chars]\n",
    "        return summary\n",
    "    \n",
    "    def forward(self, question, topic, context, previous_summary=\"New conversation starting\"):\n",
    "        # Ensure previous_summary is within character limits\n",
    "        limited_summary = self.limit_summary_chars(previous_summary)\n",
    "\n",
    "        response = self.generate_response(\n",
    "            question=question,\n",
    "            topic=topic, \n",
    "            context=context,\n",
    "            previous_summary=limited_summary\n",
    "        )\n",
    "        \n",
    "        # Limit the updated summary to character count\n",
    "        limited_updated_summary = self.limit_summary_chars(response.updated_summary)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            reasoning=response.reasoning,\n",
    "            answer=response.answer,\n",
    "            updated_summary=limited_updated_summary\n",
    "        )\n",
    "\n",
    "\n",
    "class ConversationModel:\n",
    "    def __init__(self, qa_model, retriever):\n",
    "        self.qa_model = qa_model\n",
    "        self.retriever = retriever\n",
    "        self.conversation_summary = \"New conversation starting\"\n",
    "    \n",
    "    def process_question(self, question, topic, community):\n",
    "        \n",
    "        search = self.retriever.create_search(community, topk_docs_to_retrieve=3)\n",
    "        if search:\n",
    "            result = search(question)\n",
    "            \n",
    "            # Truncate each passage (document) to 2000 characters\n",
    "            truncated_passages = []\n",
    "            for passage in result.passages:\n",
    "                truncated_passages.append(passage[:2000] + \"...\" if len(passage) > 2000 else passage)\n",
    "            \n",
    "            context = \" \".join(truncated_passages)\n",
    "        else:\n",
    "            context = \"No context available\"\n",
    "        \n",
    "        prediction = self.qa_model(\n",
    "            question=question,\n",
    "            topic=topic,\n",
    "            context=context,\n",
    "            previous_summary=self.conversation_summary\n",
    "        )\n",
    "        \n",
    "        self.conversation_summary = prediction.updated_summary\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset for new conversation\"\"\"\n",
    "        self.conversation_summary = \"New conversation starting\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444a2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup for the model and conversations. the model is used for both prediction and evaluation.\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "filepath = '../PragmatiCQA/data/val.jsonl'\n",
    "\n",
    "conversations = []\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "retriever = Retriever()\n",
    "qa_model = QAModel()\n",
    "conv_model = ConversationModel(qa_model, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bf5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conversation on topic: 'Batman'\n",
      "Number of questions: 9\n",
      "============================================================\n",
      "\n",
      "Question 1:\n",
      "Q: What is Batmans real name?\n",
      "Predicted Answer: Batman's real name is Bruce Wayne, the wealthy billionaire and philanthropist who uses his resources and skills to fight crime in Gotham City as a vigilante. Since you're asking about this, it seems like you might be curious about superhero origins—would you like me to explain more about Bruce Wayne's backstory, his role in the Justice League, or recommendations for Batman stories to dive deeper?\n",
      "Gold Answer: Batman's real identity is Bruce Wayne. He lives in Gotham City and is the CEO of Wayne Enterprises.\n",
      "Reasoning: First, the student's question is \"What is Batman's real name?\", which is a direct inquiry about the identity of the Batman character. The topic is \"Batman\", so I reviewed the provided context, which includes multiple references to Batman's real name. In the context, under sections like \"The Batman\" and \"General Information\", it explicitly states that Batman's real name is Bruce Wayne. For example, it mentions \"Real name: Bruce Wayne\" in the character details and repeats this in the history and general descriptions. This information is consistent across the context, originating from DC Comics sources. \n",
      "\n",
      "Next, since this is a new conversation (as indicated by the previous summary), I need to ensure the answer is cooperative and anticipates the student's potential interests, such as exploring more about Batman's backstory, motivations, or related topics in superhero lore. My reasoning focuses on accuracy based on the context while keeping the response engaging to encourage further learning.\n",
      "\n",
      "Finally, for the updated summary, I must focus only on the student's questions and interests, not on the answers or any model-generated content. The student's query suggests an interest in superhero identities, so I'll update the summary to reflect this without revealing details from my response.\n",
      "Updated Summary: The student is asking about the real name of Batman, indicating an interest in superhero characters and their secret identities, possibly as part of exploring comic book lore or DC Comics in general.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 2:\n",
      "Q: How did Batman become Batman?\n",
      "Predicted Answer: Batman, whose real identity is Bruce Wayne, became the Dark Knight after a pivotal event in his childhood. As a young boy, Bruce witnessed the murder of his parents, which deeply traumatized him and ignited a lifelong vow to fight crime and protect Gotham City. Inspired by heroes like Zorro and the Gray Ghost, he dedicated years to rigorous physical and mental training, mastering martial arts, detective skills, and various disciplines around the world. Upon returning to Gotham, he adopted the Batman persona—using his intellect, gadgets, and fear tactics—to strike terror into criminals' hearts. This origin story highlights themes of justice, resilience, and personal transformation, which are central to many superhero narratives. If you're fascinated by this, would you like to learn about other aspects, like his key allies (such as Robin) or iconic villains (like the Joker), to dive deeper into DC Comics?\n",
      "Gold Answer: As a child, Bruce witnessed the murder of his parents. This motivated him to train to peak performance and wear a bat costume while fighting crime. He does not have any superpowers, but instead he uses intellect, detective skills, wealth, and physical strength to be a hero.\n",
      "Reasoning: First, the student's question is \"How did Batman become Batman?\", which seeks details on Batman's origin story. The topic is \"Batman\", and the context provides relevant information from Batman's history, including his early life as Bruce Wayne, where he idolized heroes like Zorro and the Gray Ghost, and was likely influenced by traumatic events such as his parents' murder. This aligns with standard DC Comics lore, where Bruce Wayne's transformation into Batman stems from witnessing his parents' death, motivating him to fight crime.\n",
      "\n",
      "Step 1: Analyze the question and context. The context mentions Bruce Wayne's early life and his admiration for masked heroes, which sets the stage for his origin. It doesn't explicitly detail the full story, but I can infer from the provided details and general knowledge that the core elements involve his childhood trauma and subsequent training.\n",
      "\n",
      "Step 2: Ensure the answer is cooperative and anticipates student needs. Since the previous summary indicates an interest in superhero characters, secret identities, and comic book lore, I should provide a clear, engaging explanation that ties into this interest, perhaps by suggesting related topics like other DC heroes or Batman's allies/enemies to encourage further exploration.\n",
      "\n",
      "Step 3: Structure the answer to be informative yet concise, focusing on key events without overwhelming the student, and end with a question to keep the conversation going.\n",
      "\n",
      "Step 4: Update the summary. The previous summary focused on the student's interest in Batman's real name and secret identities. Now, with this new question, I need to expand it to include their curiosity about Batman's origin, while keeping the focus on questions and interests only, not providing any answers or details from the response.\n",
      "\n",
      "Step 5: Verify output constraints. The updated summary must be under 1200 characters and center solely on the student's evolving questions and interests.\n",
      "Updated Summary: The student is asking about Batman's origin story, building on their previous interest in his real name and secret identities. This suggests a growing curiosity in superhero characters, their backstories, and elements of comic book lore, possibly as part of exploring themes like justice, trauma, and heroism in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 3:\n",
      "Q: Is Batman good at his job?\n",
      "Predicted Answer: Yes, Batman is generally portrayed as exceptionally good at his job as a crimefighter! In the DC Comics universe, Bruce Wayne transforms into Batman through rigorous training, using his genius-level intellect, detective skills, and mastery of martial arts to outsmart and defeat villains. For instance, he's often called \"The World's Greatest Detective\" because of his ability to solve complex crimes and anticipate threats. This effectiveness stems from his traumatic origin, where he vowed to protect Gotham City after witnessing his parents' murder. If you're curious, we could explore specific examples, like how he handles iconic villains or uses his gadgets, to see how his skills make him such a compelling hero. What aspect of his job interests you most?\n",
      "Gold Answer: Yes, Batman is motivated by a love for humanity. He continually refrains from killing the criminals he fights. An impressive move, Batman developed a fake persona for Bruce Wayne to protect his identity as the Batman.\n",
      "Reasoning: First, the student's question is \"Is Batman good at his job?\" which refers to Batman's role as a crimefighter, based on the topic and context. The context describes Batman as Bruce Wayne, a highly skilled individual with genius-level intelligence, master detective skills, martial arts expertise, and access to advanced technology, all of which contribute to his effectiveness in combating crime. This builds on the previous summary, where the student showed interest in Batman's origin story, real name, and secret identities, suggesting a broader curiosity about his character and heroic capabilities.\n",
      "\n",
      "Step 1: Analyze the question – It's subjective but can be addressed using evidence from the context, which portrays Batman as successful in his vigilante role through his training, intellect, and tools.\n",
      "\n",
      "Step 2: Draw from context – Key points include Batman's abilities (e.g., master detective, peak physical condition) and his history of fighting crime, which indicate he is highly competent.\n",
      "\n",
      "Step 3: Make it cooperative – Anticipate the student's interests by providing an engaging, evidence-based answer that ties back to his backstory, potentially sparking follow-up questions about specific skills or challenges, while keeping it age-appropriate and encouraging.\n",
      "\n",
      "Step 4: Ensure the answer is balanced – Affirm his strengths without overgeneralizing, as the context is from comic lore, and invite critical thinking.\n",
      "\n",
      "Step 5: Prepare updated summary – Focus on the student's evolving questions (now including Batman's job performance) and interests (e.g., superhero lore, justice themes), without referencing my answer, to track conversation progression.\n",
      "Updated Summary: The student is now asking whether Batman is effective in his role as a crimefighter, expanding on their earlier questions about his origin story, real name, and secret identities. This indicates a deepening interest in superhero characters, their abilities, and how they embody themes of justice and heroism in DC Comics, possibly reflecting curiosity about personal growth, crime-fighting strategies, and moral complexities in comic book narratives.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 4:\n",
      "Q: Does Batman have any friends?\n",
      "Predicted Answer: Yes, Batman, also known as Bruce Wayne, has several key allies who can be considered friends, though his relationships are often complex and built on trust rather than traditional friendship. In DC Comics and related media, his closest companions include Alfred Pennyworth, his loyal butler and confidant who provides emotional support and logistical aid; Robin (various iterations like Dick Grayson), who serves as a sidekick and partner in crime-fighting; and Commissioner James Gordon, a reliable ally in the Gotham City Police Department. These relationships highlight how Batman balances solitude with alliances to enhance his effectiveness as a hero. If you're curious about how these dynamics affect his strategies or personal growth, I can explore that further—let me know!\n",
      "Gold Answer: Yes, most notably Batman interacts with his butler and legal guardian Alfred Pennyworth. As Batman, he is often aligned with Gotham City Police Commissioner James Gordon. Batman also frequently has relationships with women, the most prominent being Catwoman.\n",
      "Reasoning: First, the student's question is \"Does Batman have any friends?\" which builds on their previous interests in Batman's effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests they're exploring Batman's personal relationships, possibly to understand how his social connections impact his heroism and isolation. The topic is \"Batman,\" and the context provided focuses on Batman films, comics, and franchises, such as Batman '89, various adaptations, and Warner Bros. productions, but it doesn't directly address friends. From general DC Comics knowledge, Batman has allies like Alfred Pennyworth, Robin, Commissioner Gordon, and others, which I can use to form a cooperative response.\n",
      "\n",
      "Step 1: Analyze the question in context—it's not explicitly covered in the provided text, so I'll draw from established Batman lore to provide a helpful, accurate answer while anticipating the student's curiosity about themes like loneliness in heroism.\n",
      "\n",
      "Step 2: Connect to previous summary—The student's interests are in superhero abilities, justice, personal growth, and moral complexities, so I'll frame the answer to link Batman's friends to his crime-fighting strategies and emotional support, encouraging deeper exploration.\n",
      "\n",
      "Step 3: Ensure the answer is cooperative—Make it engaging, balanced, and open-ended to anticipate follow-up questions, such as specific allies or their roles.\n",
      "\n",
      "Step 4: Update the summary—Focus only on the student's questions and interests (e.g., add this new query about friends) without mentioning my answers, to track their evolving curiosity in Batman's character dynamics.\n",
      "\n",
      "Step 5: Keep outputs concise—Answer should be informative yet brief, and updated summary under 1200 characters.\n",
      "Updated Summary: The student is asking whether Batman has any friends, expanding on their previous questions about his effectiveness as a crimefighter, origin story, real name, and secret identities. This indicates a growing interest in superhero characters' personal relationships, social dynamics, and how these elements tie into themes of justice, heroism, personal growth, crime-fighting strategies, and moral complexities in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 5:\n",
      "Q: How many movies is Batman in?\n",
      "Predicted Answer: Batman has appeared in a variety of films over the years, and based on the information available, the Batman film franchise includes at least nine theatrical live-action films, plus two live-action serials. This count comes from the main franchise entries like the original anthology films (e.g., Batman from 1989), the Dark Knight trilogy, and recent ones like The Batman from 2022. He also features in crossover films such as Batman v Superman and Justice League, which might add to the total depending on how you define it. To give you a more precise number, it's generally around 9-15 feature films if we include crossovers, but this can vary with new releases. If you're curious about specific movies or how Batman's character evolves across them—perhaps tying into his relationships or crime-fighting strategies—I can recommend resources or discuss that next to deepen your interest!\n",
      "Gold Answer: I am unsure of the exact number, but Batman has become one of the most popular icons in the world. He can be found in radio, television, film, comics, and more. \n",
      "Reasoning: First, the student's question is \"How many movies is Batman in?\", which seeks a count of films featuring Batman. The topic is \"Batman\", and the context provides detailed information on Batman's film franchise, including serials and theatrical films. The context explicitly states that there are \"nine theatrical live-action films and two live-action serials\" featuring Batman, and it lists various films across different eras and studios.\n",
      "\n",
      "To answer accurately, I reviewed the context: It mentions two serials (from Columbia), one film from 20th Century Fox, and several from Warner Bros., including anthologies, trilogies, and crossovers. The initial statement in the context specifies \"nine theatrical live-action films,\" which likely refers to the main feature films, excluding serials or TV-related content. However, the question uses \"movies,\" which could be interpreted broadly, so I'll clarify in the answer to avoid confusion.\n",
      "\n",
      "Step-by-step:\n",
      "1. Extract key information: The context confirms nine theatrical live-action films as the core count.\n",
      "2. Consider broader interpretations: Batman appears in additional films like crossovers (e.g., Batman v Superman), but the context doesn't provide an exact total beyond the initial statement.\n",
      "3. Anticipate student needs: The student has shown interest in Batman's personal aspects (from the previous summary), so I'll make the answer cooperative by suggesting related explorations, like how different portrayals affect his character.\n",
      "4. Ensure the answer is helpful and engaging, avoiding definitive claims that might be outdated.\n",
      "5. For the updated summary, I'll incorporate this new question into the previous one, focusing on the student's evolving interests in relationships, backstory, and now media representations, without detailing answers.\n",
      "Updated Summary: The student is asking about the number of movies Batman is in, building on previous questions about his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, media representations, backstory elements, crime-fighting strategies, and moral complexities in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 6:\n",
      "Q: Is Batman a superhero?\n",
      "Predicted Answer: Yes, Batman is indeed a superhero! He's one of the most iconic figures in the DC Comics universe, known for his dedication to justice in Gotham City. Unlike characters with superpowers, Batman relies on his intelligence, martial arts skills, and advanced technology, which makes him a fascinating example of a human-driven hero. Since you've been asking about his friends, origin story, and media appearances, you might be interested in how his \"superhero\" status influences his relationships with allies like the Justice League or his portrayal in movies—feel free to ask more about that!\n",
      "Gold Answer: Yes, Batman is a superhero. He was created by Bob Kane and Bill Finger. He first appeared in Detective Comics #27 in 1939,\n",
      "Reasoning: First, the student's question is \"Is Batman a superhero?\", which is straightforward and directly related to the topic of Batman. From the context provided, Batman is explicitly described as a superhero multiple times, including details about his creation, abilities, and role in fighting crime in Gotham City. This confirms that the answer should be affirmative based on the retrieved information.\n",
      "\n",
      "Next, I need to make the answer cooperative and anticipate the student's needs. The previous summary indicates the student has been exploring various aspects of Batman, such as his movies, friends, effectiveness, origin story, real name, and secret identities. This suggests an ongoing interest in superhero lore, so I should craft an answer that not only addresses the question but also connects to their broader curiosity, perhaps by highlighting Batman's unique traits (like lacking superpowers) and inviting further questions to keep the conversation engaging.\n",
      "\n",
      "For the updated summary, I must focus solely on the student's questions and interests, without referencing any answers or model-generated content. I'll build on the previous summary by incorporating this new question, emphasizing how it fits into their pattern of inquiry about Batman's character, relationships, and representations.\n",
      "\n",
      "Step-by-step reasoning:\n",
      "1. Verify the question against the context: The context clearly states Batman is a superhero, so the response should affirm this.\n",
      "2. Analyze previous summary: The student is building a comprehensive interest in Batman, so link the answer to themes like character definitions and media aspects.\n",
      "3. Ensure answer is cooperative: Make it educational, positive, and forward-looking to encourage learning.\n",
      "4. Update summary: Add the new question to the existing summary, focusing on interests like character classification, while keeping it under 1200 characters.\n",
      "Updated Summary: The student is asking about whether Batman is a superhero, building on previous questions about the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, media representations, backstory elements, crime-fighting strategies, moral complexities, and core classifications in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 7:\n",
      "Q: Who is his arch enemy?\n",
      "Predicted Answer: Batman's most iconic arch enemy is the Joker, a chaotic and unpredictable villain who embodies the opposite of Batman's moral code. He's often depicted as Batman's nemesis in comics, movies, and games, challenging him through psychological battles and crimes that test Batman's limits. If you're curious about why the Joker is so central to Batman's story or want to explore other villains like those from the Secret Society, I can dive deeper—let me know!\n",
      "Gold Answer: Batman's biggest enemy is The Joker. He is a clown themed criminal who is focused on inflicting chaos in Gotham's streets. He is seen as jovial and always changing.\n",
      "Reasoning: First, the question is \"Who is his arch enemy?\" in the context of the topic \"Batman.\" From the provided context, various villains are mentioned, such as the Secret Society of Super Villains, Lex Luthor, the Joker, and Psycho-Pirate, who are all associated with Batman or DC Comics. The Joker is explicitly referenced in the plot summary of Lego DC Super-Villains as a key antagonist, and based on general DC Comics knowledge, the Joker is widely recognized as Batman's primary arch enemy due to their iconic rivalry. \n",
      "\n",
      "To provide a cooperative answer, I need to be helpful and engaging, anticipating the student's interests in Batman's relationships and dynamics (as indicated in the previous summary). I'll confirm the arch enemy, offer a brief explanation to add value, and suggest potential follow-ups, like exploring other villains or the nature of their conflicts, to keep the conversation flowing.\n",
      "\n",
      "For the updated summary, I'll focus solely on the student's questions and evolving interests (e.g., adding this query about arch enemies) without referencing my answer, as per the guidelines. The previous summary already covers interests in relationships and dynamics, so I'll integrate this new question seamlessly.\n",
      "\n",
      "Step-by-step plan:\n",
      "1. State the arch enemy clearly.\n",
      "2. Provide context to make it educational and cooperative.\n",
      "3. Update the summary by appending the new question and refining the interests list.\n",
      "4. Ensure the summary stays under 1200 characters and avoids answer details.\n",
      "Updated Summary: The student is asking about Batman's arch enemy, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, and core classifications in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 8:\n",
      "Q: Is Batman stronger than the joker?\n",
      "Predicted Answer: In the world of DC Comics, Batman is generally depicted as stronger than the Joker in terms of physical strength and combat skills. Batman has undergone intense training to achieve peak human condition, making him a master martial artist with exceptional strength, agility, and endurance. The Joker, on the other hand, relies more on his genius intellect, deadly gadgets, and unpredictable tactics rather than raw physical power—he's a clever villain who uses deception and weapons to challenge Batman. However, \"strength\" can depend on the story; the Joker often outsmarts opponents, which makes him a dangerous foe. If you're curious about this, we could explore more about Batman's training routines or the Joker's schemes, or even how their rivalry affects Gotham's dynamics, based on your interest in superhero relationships and adversaries!\n",
      "Gold Answer: Yes, Batman is stronger than the joker. Although, the joker has been known to cause much havoc in Batman's life including paralyzing Barbara Gordon and killing Jason Todd, Robin. \n",
      "Reasoning: First, the question asks whether Batman is stronger than the Joker, which involves comparing their physical and overall capabilities. From the context provided, the Joker is described as having abilities like genius intellect, brilliant chemistry skills, an arsenal of weapons, deception, expert hand-to-hand combat, and pain resistance, making him a formidable adversary through cunning and gadgets rather than raw physical power. Batman, based on general knowledge of the character, is a highly trained human with peak physical condition, martial arts expertise, and strategic mind, but without superhuman abilities. In DC Comics lore, Batman is typically portrayed as superior in direct physical confrontations due to his rigorous training and discipline, while the Joker excels in psychological warfare and trickery. To answer cooperatively, I need to address the comparison directly, explain it simply for a student's level, and anticipate further interests by linking to themes like adversaries and character dynamics from the previous summary. Step-by-step: 1) Analyze the question as a comparison of strengths. 2) Use context to highlight Joker's attributes. 3) Draw from Batman's established traits to contrast. 4) Provide a balanced, engaging answer that encourages learning. 5) Update the summary to include this new question on character abilities, aligning with the student's growing interest in adversaries and comparisons.\n",
      "Updated Summary: The student is asking about a direct strength comparison between Batman and the Joker, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This indicates an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, core classifications in DC Comics, and now specific ability comparisons like physical strength.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 9:\n",
      "Q: What is Batmans favourite gadget?\n",
      "Predicted Answer: That's a great question about Batman's gear—it's fascinating how his gadgets play a big role in his adventures! Based on the information from various Batman portrayals, the Grapple Gun (or Grappling Hook Pistol) stands out as one of his most iconic and frequently used tools. It's described as a hi-tech device that helps him navigate cities quickly, pull objects, or even engage in combat, making it essential for his crime-fighting style. While Batman uses many gadgets, this one often appears in comics, films, and animations, suggesting it's a favorite for its versatility. If you're curious about why he relies on it or how it compares to other tools, or even want to explore more about his tech in specific stories, feel free to ask—I'm here to dive deeper into Batman's world with you!\n",
      "Gold Answer: The gadget that Batman most often uses is a Batarang, which is a small bat shaped throwing weapon. These were based on Australian boomerangs. He keeps the batarangs and other gadgets in his yellow utility belt.\n",
      "Reasoning: First, the student's question is about Batman's favorite gadget, which builds on their ongoing interest in Batman's character details, as seen in the previous summary. The context provided focuses heavily on the Grappling Hook Pistol (also called the Grapple Gun), describing it as a key tool used by Batman for mobility, offense, and various missions, making it one of his most prominent gadgets across comics, films, and other media. While the context doesn't explicitly state Batman's \"favorite\" gadget, the Grapple Gun is highlighted as constantly used by Batman and the Batman Family, suggesting it's a central element. I need to provide a cooperative answer that is helpful, draws from the context, and anticipates further questions, such as why certain gadgets are important or how they relate to his crime-fighting strategies. Step 2: Acknowledge the student's curiosity and tie it to their broader interests in Batman's abilities, relationships, and media representations to keep the response engaging. Step 3: For the updated summary, I must focus only on the student's questions and interests (e.g., adding this gadget inquiry to their existing pattern), without referencing my answer, to maintain accuracy and avoid potential errors. Step 4: Ensure the summary remains concise, under 1200 characters, by integrating the new question into the previous one. Finally, structure the output as required.\n",
      "Updated Summary: The student is asking about Batman's favorite gadget, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, secret identities, and a direct strength comparison with the Joker. This indicates an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, core classifications in DC Comics, specific ability comparisons like physical strength, and now equipment and gadgets used in his operations.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Conversation complete! Processed 9 questions.\n"
     ]
    }
   ],
   "source": [
    "# We'll do an evaluation of a single conversation to check the model's validity.\n",
    "\n",
    "conversation = conversations[9] \n",
    "\n",
    "print(f\"Testing conversation on topic: '{conversation['topic']}'\")\n",
    "print(f\"Number of questions: {len(conversation['qas'])}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, qa_pair in enumerate(conversation['qas']):\n",
    "    question = qa_pair['q']\n",
    "    gold_answer = qa_pair['a']\n",
    "    \n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    \n",
    "    try:\n",
    "        prediction = conv_model.process_question(\n",
    "            question=question,\n",
    "            topic=conversation['topic'],\n",
    "            community=conversation['community']\n",
    "        )\n",
    "        \n",
    "        predicted_answer = prediction.answer\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print(f\"Gold Answer: {gold_answer}\")\n",
    "        \n",
    "        print(f\"Reasoning: {prediction.reasoning}\")\n",
    "        print(f\"Updated Summary: {prediction.updated_summary}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {str(e)}\")\n",
    "        predicted_answer = \"[ERROR]\"\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print(f\"Gold Answer: {gold_answer}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nConversation complete! Processed {len(conversation['qas'])} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8e257",
   "metadata": {},
   "source": [
    "#### Two main issues can be spotted:\n",
    "\n",
    "1) The model's answers and reasoning are both too long on average. this doesn't really matter in regards to reasoning, but we should try to keep the predicted answers shorter on average, as they are seemingly twice to thrice the gold answers' length.\n",
    "\n",
    "2) The model isn't actually updating its summary to include the entire conversation - the summary is only about the latest question.\n",
    "\n",
    "Both issues stem from the prompt, and so I've adjusted the prompt (and the field descriptions) to hopefully improve on these problems. Now, we'll proceed with the prompt optimization.\n",
    "\n",
    "For the optimization, I've elected to go with using the F1 score on a question-by-question basis, instead of scores over a full conversation. The main reason is honestly because it's the simplest metric to implement, and it's also likely to be the one to produce the best overall results. A metric that looks at entire conversations might help at engineering a prompt that results in better summaries, but it is also likely that it will overly-emphasize the historical context of the conversation, and cause the model to underperform when given questions that are somewhat \"out of left field\", so to speak. \n",
    "\n",
    "All in all, the conversation-based metric potential advantages over a regular question-based metric are nebulous at best, and harder to implement besides. This line of thought is what also dissuaded me from other similar metrics.\n",
    "\n",
    "This also makes it so this optimizer actually optimizes for the 'first questions', since it won't have the carryover summary because we'll be evaluating on a per-question basis (I am optimizing the QAModel, not the full ConversationModel which handles the summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87ce8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 23:14:38 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: True\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 99\n",
      "\n",
      "2025/08/23 23:14:38 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/08/23 23:14:38 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/08/23 23:14:38 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:00<?, ?it/s]2025/08/23 23:14:42 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:14:46 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'how old is snoop dogg?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'Born in California on October 20, 1971, Snoop Dog is now 46 years old'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  1%|          | 1/118 [00:07<15:32,  7.97s/it]2025/08/23 23:14:50 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:14:54 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'I see.. has he ever been incarcerated?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'Yes, after graduating from high school Snoop Dogg was arrested for narcotics possession and spent the next 3 years in and out of prison. As a teenager he frequently ran into trouble with the law.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  2%|▏         | 2/118 [00:15<14:56,  7.72s/it]2025/08/23 23:14:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:01 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'did he ever do time for murder?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'Snoop Dogg was acquitted of murder of February 20, 1996. A short film about the trial was released in 1994.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  3%|▎         | 3/118 [00:23<14:43,  7.69s/it]2025/08/23 23:15:05 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:09 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Who is Snoop Dogg?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'Snoop Dogg is an American rapper who has sold over 23 millions albums nationally and over 35 million albums worldwide. Snoop Dogg sings and writes his own songs and produces records and even acts in films and television with his own personality.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  3%|▎         | 4/118 [00:30<14:30,  7.64s/it]2025/08/23 23:15:13 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:17 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Why  does Snoop call himself?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'After a trip of Jamaica where he converted to Rastafarianism, Snoop Dogg began to call himself Snoop Lion. But, one he released his 13th studio album, Bush, in May 2015, he returned to calling himself Snoop Dogg.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  4%|▍         | 5/118 [00:38<14:21,  7.62s/it]2025/08/23 23:15:20 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:24 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': \"Who is Snoop Dogg's wife? what  happened to her?\", 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'I do not know exactly, but Snoop Dogg did release his first gospel album, titled the Bible of Love, in 2018.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  5%|▌         | 6/118 [00:45<14:13,  7.62s/it]2025/08/23 23:15:28 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:32 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'What is the other names of Snoop Dogg?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'His real name is Calvin Cordozar Broadus Jr., but he used the Snoop Dogg name on his debut album, Doggystyle. Dr. Dre produced this album and Death Row Records released it in 1993.'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  6%|▌         | 7/118 [00:53<14:07,  7.64s/it]2025/08/23 23:15:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:40 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'How many nominations he got in Grammy Awards?', 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': \"He only has received 17 Grammy nominations, and he has never won a Grammy award. But, from his debut album, the singles titled, Who Am I (What's My Name)?, and, Gin and Juice, reached the charts as the top ten most played songs in the United States.\"}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  7%|▋         | 8/118 [01:01<14:06,  7.69s/it]2025/08/23 23:15:43 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:15:47 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': \"How many children's he had? Name of his albums?\", 'topic': 'Snoop Dogg', 'context': '', 'previous_summary': 'New conversation starting', 'gold_answer': 'The name of his second album was Tha Doggfather, which was released in 1996. After this, Snoop Dogg left Death Row Records and joined No Limit Records where he recorded his next three albums: Da Game Is to Be Sold, Not to Be Told (1998), No Limit Top Dogg (1999) and Tha Last Meal (2000).'}) (input_keys={'previous_summary', 'question', 'context', 'topic'}) with <function qa_f1_metric at 0x000000006613B380> due to Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "  8%|▊         | 9/118 [01:09<13:57,  7.68s/it]2025/08/23 23:15:51 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "  8%|▊         | 9/118 [01:16<15:28,  8.52s/it]\n",
      "2025/08/23 23:15:55 INFO dspy.teleprompt.mipro_optimizer_v2: Error generating few-shot examples: Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}\n",
      "2025/08/23 23:15:55 INFO dspy.teleprompt.mipro_optimizer_v2: Running without few-shot examples.\n",
      "2025/08/23 23:15:55 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/08/23 23:15:55 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/08/23 23:15:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:16:03 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting data summary: Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}.\n",
      "\n",
      "Running without data aware proposer.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 23:16:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 23:16:14 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:171\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     response = \u001b[43msync_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    177\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:780\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    779\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:762\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    761\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.x.ai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:1800\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1794\u001b[39m         logging.post_call(\n\u001b[32m   1795\u001b[39m             \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1796\u001b[39m             api_key=api_key,\n\u001b[32m   1797\u001b[39m             original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1798\u001b[39m             additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1799\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mgroq\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:1774\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1793\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:467\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[39m\n\u001b[32m    465\u001b[39m     sync_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    479\u001b[39m     model=model,\n\u001b[32m    480\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m     json_mode=json_mode,\n\u001b[32m    490\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:196\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:2405\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   2399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   2400\u001b[39m         status_code=status_code,\n\u001b[32m   2401\u001b[39m         message=error_text,\n\u001b[32m   2402\u001b[39m         headers=error_headers,\n\u001b[32m   2403\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2405\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   2406\u001b[39m     error_message=error_text,\n\u001b[32m   2407\u001b[39m     status_code=status_code,\n\u001b[32m   2408\u001b[39m     headers=error_headers,\n\u001b[32m   2409\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1184\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3430\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3429\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3433\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:492\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    493\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    494\u001b[39m         model=model,\n\u001b[32m    495\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    496\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    497\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    498\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\chat_adapter.py:42\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\base.py:119\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m    117\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.format(processed_signature, demos, inputs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_postprocess(signature, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\utils\\callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\base_lm.py:96\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\lm.py:127\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m completion, litellm_cache_args = \u001b[38;5;28mself\u001b[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(c.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\cache.py:232\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\lm.py:304\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexponential_backoff_retry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream_completion()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1289\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1288\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m] = num_retries\n\u001b[32m-> \u001b[39m\u001b[32m1289\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001b[32m   1292\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1293\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1294\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_litellm_router_call\n\u001b[32m   1295\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3468\u001b[39m, in \u001b[36mcompletion_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3465\u001b[39m     retryer = tenacity.Retrying(\n\u001b[32m   3466\u001b[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3467\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretryer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1309\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1306\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1307\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1308\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1309\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1184\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3430\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3429\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3433\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:492\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    493\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    494\u001b[39m         model=model,\n\u001b[32m    495\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    496\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    497\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    498\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\json_adapter.py:62\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     structured_output_model = \u001b[43m_get_structured_outputs_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     lm_kwargs[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = structured_output_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\json_adapter.py:220\u001b[39m, in \u001b[36m_get_structured_outputs_response_format\u001b[39m\u001b[34m(signature)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Build the model with extra fields forbidden.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m pydantic_model = \u001b[43mpydantic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDSPyProgramOutputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__config__\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mextra\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforbid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# Generate the initial schema.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\pydantic\\main.py:1763\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_name, __config__, __doc__, __base__, __module__, __validators__, __cls_kwargs__, **field_definitions)\u001b[39m\n\u001b[32m   1761\u001b[39m namespace.update(ns)\n\u001b[32m-> \u001b[39m\u001b[32m1763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_bases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__pydantic_reset_parent_namespace__\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_create_model_module\u001b[49m\u001b[43m=\u001b[49m\u001b[34;43m__module__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:110\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m base_field_names, class_vars, base_private_attributes = mcs._collect_bases_data(bases)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m config_wrapper = \u001b[43mConfigWrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfor_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m namespace[\u001b[33m'\u001b[39m\u001b[33mmodel_config\u001b[39m\u001b[33m'\u001b[39m] = config_wrapper.config_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:138\u001b[39m, in \u001b[36mConfigWrapper.for_model\u001b[39m\u001b[34m(cls, bases, namespace, kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m config_from_namespace = config_dict_from_namespace \u001b[38;5;129;01mor\u001b[39;00m prepare_config(config_class_from_namespace)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43mconfig_new\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_from_namespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(kwargs.keys()):\n",
      "\u001b[31mTypeError\u001b[39m: 'type' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:171\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     response = \u001b[43msync_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    177\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:780\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    779\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:762\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    761\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.x.ai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:1800\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1794\u001b[39m         logging.post_call(\n\u001b[32m   1795\u001b[39m             \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1796\u001b[39m             api_key=api_key,\n\u001b[32m   1797\u001b[39m             original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1798\u001b[39m             additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1799\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mgroq\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:1774\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1793\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:467\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[39m\n\u001b[32m    465\u001b[39m     sync_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    479\u001b[39m     model=model,\n\u001b[32m    480\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m     json_mode=json_mode,\n\u001b[32m    490\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:196\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:2405\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   2399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   2400\u001b[39m         status_code=status_code,\n\u001b[32m   2401\u001b[39m         message=error_text,\n\u001b[32m   2402\u001b[39m         headers=error_headers,\n\u001b[32m   2403\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2405\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   2406\u001b[39m     error_message=error_text,\n\u001b[32m   2407\u001b[39m     status_code=status_code,\n\u001b[32m   2408\u001b[39m     headers=error_headers,\n\u001b[32m   2409\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1184\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3430\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3429\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3433\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:492\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    493\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    494\u001b[39m         model=model,\n\u001b[32m    495\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    496\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    497\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    498\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\json_adapter.py:69\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     68\u001b[39m     lm_kwargs[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AdapterParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# On AdapterParseError, we raise the original error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\chat_adapter.py:50\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\chat_adapter.py:42\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\base.py:119\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m    117\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.format(processed_signature, demos, inputs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_postprocess(signature, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\utils\\callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\base_lm.py:96\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\lm.py:127\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m completion, litellm_cache_args = \u001b[38;5;28mself\u001b[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(c.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\cache.py:232\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\clients\\lm.py:304\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexponential_backoff_retry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream_completion()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1289\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1288\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m] = num_retries\n\u001b[32m-> \u001b[39m\u001b[32m1289\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001b[32m   1292\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1293\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1294\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_litellm_router_call\n\u001b[32m   1295\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3468\u001b[39m, in \u001b[36mcompletion_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3465\u001b[39m     retryer = tenacity.Retrying(\n\u001b[32m   3466\u001b[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3467\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretryer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1309\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1306\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1307\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1308\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1309\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\utils.py:1184\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\main.py:3430\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3429\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3433\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:492\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    493\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    494\u001b[39m         model=model,\n\u001b[32m    495\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    496\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    497\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    498\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     74\u001b[39m     compiled_predictor = optimizer.compile(\n\u001b[32m     75\u001b[39m         predictor,\n\u001b[32m     76\u001b[39m         trainset=train_dspy_examples,\n\u001b[32m     77\u001b[39m         valset=val_dspy_examples,\n\u001b[32m     78\u001b[39m         requires_permission_to_run=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     79\u001b[39m     )\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_predictor\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m compiled_qa_model = \u001b[43mtrain_qa_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain_qa_optimizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     65\u001b[39m optimizer = MIPROv2(\n\u001b[32m     66\u001b[39m     metric=qa_f1_metric,\n\u001b[32m     67\u001b[39m     max_bootstrapped_demos=\u001b[32m3\u001b[39m,\n\u001b[32m     68\u001b[39m     init_temperature=\u001b[32m1.0\u001b[39m,\n\u001b[32m     69\u001b[39m     track_stats=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     70\u001b[39m )\n\u001b[32m     72\u001b[39m predictor = QAModel()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m compiled_predictor = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dspy_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dspy_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequires_permission_to_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     79\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_predictor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\teleprompt\\mipro_optimizer_v2.py:188\u001b[39m, in \u001b[36mMIPROv2.compile\u001b[39m\u001b[34m(self, student, trainset, teacher, valset, num_trials, max_bootstrapped_demos, max_labeled_demos, seed, minibatch, minibatch_size, minibatch_full_eval_steps, program_aware_proposer, data_aware_proposer, view_data_batch_size, tip_aware_proposer, fewshot_aware_proposer, requires_permission_to_run, provide_traceback)\u001b[39m\n\u001b[32m    185\u001b[39m demo_candidates = \u001b[38;5;28mself\u001b[39m._bootstrap_fewshot_examples(program, trainset, seed, teacher)\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Step 2: Propose instruction candidates\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m instruction_candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_propose_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mview_data_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram_aware_proposer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_aware_proposer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtip_aware_proposer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfewshot_aware_proposer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# If zero-shot, discard demos\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m zeroshot_opt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\teleprompt\\mipro_optimizer_v2.py:469\u001b[39m, in \u001b[36mMIPROv2._propose_instructions\u001b[39m\u001b[34m(self, program, trainset, demo_candidates, view_data_batch_size, program_aware_proposer, data_aware_proposer, tip_aware_proposer, fewshot_aware_proposer)\u001b[39m\n\u001b[32m    451\u001b[39m proposer = GroundedProposer(\n\u001b[32m    452\u001b[39m     program=program,\n\u001b[32m    453\u001b[39m     trainset=trainset,\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m     rng=\u001b[38;5;28mself\u001b[39m.rng,\n\u001b[32m    466\u001b[39m )\n\u001b[32m    468\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProposing N=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_instruct_candidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instructions...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m instruction_candidates = \u001b[43mproposer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpropose_instructions_for_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_instruct_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrial_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(program.predictors()):\n\u001b[32m    479\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProposed Instructions for Predictor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\propose\\grounded_proposer.py:374\u001b[39m, in \u001b[36mGroundedProposer.propose_instructions_for_program\u001b[39m\u001b[34m(self, trainset, program, demo_candidates, trial_logs, N, T)\u001b[39m\n\u001b[32m    370\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    371\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected tip: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_tip_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    373\u001b[39m         proposed_instructions[pred_i].append(\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropose_instruction_for_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m                \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdemo_set_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemo_set_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtrial_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_tip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    384\u001b[39m         )\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m proposed_instructions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\propose\\grounded_proposer.py:425\u001b[39m, in \u001b[36mGroundedProposer.propose_instruction_for_predictor\u001b[39m\u001b[34m(self, program, predictor, pred_i, T, demo_candidates, demo_set_i, trial_logs, tip)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dspy.settings.context(lm=\u001b[38;5;28mself\u001b[39m.prompt_model):\n\u001b[32m    424\u001b[39m     \u001b[38;5;28mself\u001b[39m.prompt_model.kwargs[\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m] = modified_temp\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     proposed_instruction = \u001b[43minstruction_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdemo_set_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemo_set_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprevious_instructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstruction_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_demos_in_context\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_demos_in_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.proposed_instruction\n\u001b[32m    435\u001b[39m \u001b[38;5;28mself\u001b[39m.prompt_model.kwargs[\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m] = original_temp\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Log the trace used to generate the new instruction, along with the new instruction itself\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\propose\\grounded_proposer.py:253\u001b[39m, in \u001b[36mGenerateModuleInstruction.forward\u001b[39m\u001b[34m(self, demo_candidates, pred_i, demo_set_i, program, previous_instructions, data_summary, num_demos_in_context, tip)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtask_demos \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_demos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m instruct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_module_instruction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprogram_code_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogram_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_demos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_demos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbasic_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasic_instruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprevious_instructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m proposed_instruction = strip_prefix(instruct.proposed_instruction)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dspy.Prediction(proposed_instruction=proposed_instruction)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\predict\\predict.py:85\u001b[39m, in \u001b[36mPredict.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._get_positional_args_error_message())\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\utils\\callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\primitives\\program.py:60\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\predict\\predict.py:157\u001b[39m, in \u001b[36mPredict.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m settings.context(send_stream=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         completions = \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_postprocess(completions, signature, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\chat_adapter.py:51\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omerc\\OneDrive\\Desktop\\Tasks\\NLP\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\adapters\\json_adapter.py:75\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# On any other error, we raise a RuntimeError with the original error message.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBoth structured output format and JSON mode failed. Please choose a model that supports \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`response_format` argument. Original error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "filepath = '../PragmatiCQA/data/train.jsonl'\n",
    "\n",
    "train_conversations = []\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_conversations.append(json.loads(line))\n",
    "\n",
    "train_examples = train_conversations[:20]\n",
    "\n",
    "val_examples = conversations[10:20]\n",
    "\n",
    "def conversation_to_dspy_examples(conversation_dict):\n",
    "    examples = []\n",
    "    \n",
    "    for qa_pair in conversation_dict['qas']:\n",
    "        question = qa_pair['q']\n",
    "        gold_answer = qa_pair['a']\n",
    "        \n",
    "        example = dspy.Example(\n",
    "            question=question,\n",
    "            topic=conversation_dict['topic'],\n",
    "            context=\"\", \n",
    "            previous_summary=\"New conversation starting\", \n",
    "            gold_answer=gold_answer\n",
    "        ).with_inputs('question', 'topic', 'context', 'previous_summary')\n",
    "        \n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "def qa_f1_metric(example, pred, trace=None):\n",
    "    \"\"\"F1 metric function using SemanticF1 evaluator\"\"\"\n",
    "    \n",
    "    gold_ex = dspy.Example(\n",
    "        question=example.question,\n",
    "        response=example.gold_answer \n",
    "    ).with_inputs('question')\n",
    "\n",
    "    pred_ex = dspy.Example(response=pred.answer)\n",
    "    \n",
    "    semantic_f1 = SemanticF1()\n",
    "    score = semantic_f1(gold_ex, pred_ex)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def train_qa_optimizer():\n",
    "    \n",
    "    train_dspy_examples = []\n",
    "    for conv in train_examples:\n",
    "        examples = conversation_to_dspy_examples(conv)\n",
    "        train_dspy_examples.extend(examples)\n",
    "    \n",
    "    val_dspy_examples = []\n",
    "    for conv in val_examples:\n",
    "        examples = conversation_to_dspy_examples(conv)\n",
    "        val_dspy_examples.extend(examples)\n",
    "    \n",
    "    optimizer = MIPROv2(\n",
    "        metric=qa_f1_metric,\n",
    "        max_bootstrapped_demos=3,\n",
    "        init_temperature=1.0,\n",
    "        track_stats=True,\n",
    "    )\n",
    "    \n",
    "    predictor = QAModel()\n",
    "    \n",
    "    compiled_predictor = optimizer.compile(\n",
    "        predictor,\n",
    "        trainset=train_dspy_examples,\n",
    "        valset=val_dspy_examples,\n",
    "        requires_permission_to_run=False\n",
    "    )\n",
    "    \n",
    "    return compiled_predictor\n",
    "\n",
    "compiled_qa_model = train_qa_optimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24c57c",
   "metadata": {},
   "source": [
    "Now, we'll continue with the evaluation on the first questions of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ca977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_first_questions(compiled_model, questions):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    retriever = Retriever()\n",
    "    conv_model = ConversationModel(compiled_model, retriever)\n",
    "    \n",
    "    semantic_f1 = SemanticF1()\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(questions)} questions\")\n",
    "        \n",
    "        try:\n",
    "            conv_model.reset_conversation()\n",
    "            \n",
    "            prediction = conv_model.process_question(\n",
    "                question=q['question'],\n",
    "                topic=q['topic'],\n",
    "                community=q['community']\n",
    "            )\n",
    "            \n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer']\n",
    "            ).with_inputs('question')\n",
    "            \n",
    "            pred_ex = dspy.Example(response=prediction.answer)\n",
    "            \n",
    "            score = semantic_f1(gold_ex, pred_ex)\n",
    "            scores.append(score)\n",
    "            \n",
    "            q['pred'] = prediction.answer\n",
    "            q['score'] = score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i+1}: {str(e)}\")\n",
    "            score = 0.0\n",
    "            scores.append(score)\n",
    "            q['pred'] = \"[EVALUATION_FAILED]\"\n",
    "            q['score'] = score\n",
    "    \n",
    "    avg_f1 = sum(scores) / len(scores) if scores else 0.0\n",
    "    \n",
    "    with open('first_questions_results.jsonl', 'w') as f:\n",
    "        for q in questions:\n",
    "            json.dump(q, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return avg_f1, scores\n",
    "\n",
    "avg_f1_first, scores_first = evaluate_first_questions(compiled_qa_model, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1438ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== FIRST QUESTIONS EVALUATION RESULTS ===\")\n",
    "print(f\"Average F1 Score: {avg_f1_first:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f8265",
   "metadata": {},
   "source": [
    "#### Short analysis by comparison to the DISTILBERT model\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_conversations(compiled_model, conversations):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    retriever = Retriever()\n",
    "    conv_model = ConversationModel(compiled_model, retriever)\n",
    "    \n",
    "    semantic_f1 = SemanticF1()\n",
    "    \n",
    "    all_scores = []\n",
    "    total_questions = 0\n",
    "    \n",
    "    for conv_idx, conversation in enumerate(conversations):\n",
    "        print(f\"Processing conversation {conv_idx + 1}/{len(conversations)}: {conversation['topic']}\")\n",
    "        \n",
    "        conv_model.reset_conversation()\n",
    "        \n",
    "        for q_idx, qa_pair in enumerate(conversation['qas']):\n",
    "            question = qa_pair['q']\n",
    "            gold_answer = qa_pair['a']\n",
    "            \n",
    "            try:\n",
    "                prediction = conv_model.process_question(\n",
    "                    question=question,\n",
    "                    topic=conversation['topic'],\n",
    "                    community=conversation['community']\n",
    "                )\n",
    "                \n",
    "                gold_ex = dspy.Example(\n",
    "                    question=question,\n",
    "                    response=gold_answer\n",
    "                ).with_inputs('question')\n",
    "                \n",
    "                pred_ex = dspy.Example(response=prediction.answer)\n",
    "                \n",
    "                score = semantic_f1(gold_ex, pred_ex)\n",
    "                all_scores.append(score)\n",
    "                \n",
    "                qa_pair['pred'] = prediction.answer\n",
    "                qa_pair['score'] = score\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question {q_idx+1} in conversation {conv_idx+1}: {str(e)}\")\n",
    "                score = 0.0\n",
    "                all_scores.append(score)\n",
    "                qa_pair['pred'] = \"[EVALUATION_FAILED]\"\n",
    "                qa_pair['score'] = score\n",
    "            \n",
    "            total_questions += 1\n",
    "    \n",
    "    avg_f1 = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "    \n",
    "    with open('full_conversations_results.jsonl', 'w') as f:\n",
    "        for conversation in conversations:\n",
    "            json.dump(conversation, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return avg_f1, all_scores\n",
    "\n",
    "avg_f1_conv, scores_conv = evaluate_full_conversations(compiled_qa_model, conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed38b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== FULL CONVERSATIONS EVALUATION RESULTS ===\")\n",
    "print(f\"Average F1 Score: {avg_f1_conv:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
