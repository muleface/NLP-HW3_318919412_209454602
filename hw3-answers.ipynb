{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed15338",
   "metadata": {},
   "source": [
    "## Part 0: Dataset Analysis\n",
    "\n",
    "### Motivation, Contributions and Methodology\n",
    "\n",
    "Existing datasets in the open-domain QA, up until the release of PragmaticCQA, largely focused on evaluating QA systems' accuracy regarding the literal answers to given questions. They did not examine or evaluate a system's ability to infer the questioner's unmentioned needs from context; whether they be in the form of follow-up questions, or relevant information that the questioner is not even necessarily aware of, due to the lack of knowledge in the topic being questioned. This ability to grasp intent is key to efficient and productive conversations, and the fact that it so far went largely ignored by common metrics and evaluation datasets is what motivated the paper's authors to create this dataset, along with the corresponding metrics, to allow for a meaningful examination of this ability in NLP models. \n",
    "\n",
    "Specifically, what they've produced is:\n",
    "\n",
    "* A crowd-sourcing framework that achieves \"incentive alignment\". The authors claim that many of the recent datasets that are crowd-sourced suffer from this \"incentive misalignment\", which essentially means that annotators are rewarded for producing as many examples as they can, and so they create 'basic' examples that they can churn out quickly. These examples tend to lack nuance, or are often similar, and thus allow the model to learn surface-level patterns in order to achieve good results. This naturally goes against the intent of the dataset creators, as it does not truly test for a model's reasoning abilities, which is where the supposed \"incentive misalignment\" stems from.     \n",
    "The authors claim to have solved this issue by allowing the annotators to work on topics they're interested in, and having actually discuss these topics between themselves, which ends up increasing their engagement with the task and producing examples that feel natural and resemble standard human interaction better than other datasets.\n",
    "\n",
    "* An open-domain ConvQA dataset that follows the prior framework, and features pragmatic answers and metrics that allow for the evaluation of pragmatic reasoning.\n",
    "\n",
    "* An analysis of their dataset that shows that it presents a challenge to existing models, proving its relevance in the field.\n",
    "\n",
    "I will now focus on the third point, which is their analysis of the dataset, and why it proves to be challenging to current NLP nodels.\n",
    "\n",
    "The split datasets each contains separate topics, meaning there's no overlap between the topics and thus no overlap of information between questions of different topics. This forces the model to actually generalize and rely on its internal reasoning capabilities.\n",
    "\n",
    "The answers in the dataset tend to be constructed with information from different elements, substantially more so than other, commonly used datasets in the field. This proves to be quite hard for the NLP, as it requires it to collate information from a large number of different sources.\n",
    "\n",
    "The answers are also often formed of a combination of small factoids, and a larger narrative that ties these factoids together with the answer to the original questions. The model should be able to replicate this, and that is more complex than giving literal answers, such as labelling or providing a direct, literal answer to a question without further consideration.\n",
    "\n",
    "These aspects tell us the dataset seeks specific pragmatic phenomena:\n",
    "* The recognition of potential follow-up questions and the inclusion of their answers.\n",
    "* Being cooperative in the conversation: A model should attempt to keep the conversation flowing with the provided answers, whether it be by including relevant information that allows for further discussion, or other such methods that humans employ (Another one would be trying to return the question, or other follow up questions to the student after providing sufficient information, but I'm not sure if the dataset actually covers this case as well).\n",
    "* Being selective in providing information: A model shouldn't just provide a list of connected data, but rather consider the question, the context in which it's asked, like the background of the questioner, and providing relevant information based on these elements.\n",
    "\n",
    "These aspects all serve to complicate the task for NLP models, and thus challenge them.\n",
    "\n",
    "\n",
    "### Sample Analysis\n",
    "\n",
    "1) The topic is 'Vampires', with the starting question being \"So, what is a vampire, exactly?\"\n",
    "\n",
    "   The literal answer we'd expect from a non-cooperative teacher would be something along the lines of \"An undead monster\", which doesn't elaborate greatly on what differentiates vampires from the plethora of other undead monsters in various fictions, like zombies, or even ghosts. \n",
    "\n",
    "   The given answer in the dataset was as follows: \"Vampires are a kind of undead monster that feeds on the life essence of living creatures like humans.\"\n",
    "   This answer provides the full information we expect to see from a literal interpretation of the question - \"A kind of undead monster\", and further specifies that it 'feeds on the life essence of living creatures', prompting the student to equate them to beasts of prey, as they feed on other living beings. This lets the student distinguish between vampires and say, ghosts, that are depicted as malevolent, metaphysical beings that exist to haunt people.\n",
    "\n",
    "   * I've got to say that I don't actually like this answer since life essence is such a weird term. When has anyone ever seen a depiction of vampires that sustain themselves on something other than blood? just say blood...\n",
    "\n",
    "2) The topic is 'The Wheel of Time', with the starting question being \"who was the writer of the wheel of time?\"\n",
    "\n",
    "    The literal answer we'd expect from a non-cooperative teacher could simply state that the writer was Robert Jordan, as he is both an author and the one who wrote the majority of the books, as the student asked about a singular writer. However, it is known that Brandon Sanderson is the one who wrote the latter books, so we expect a pragmatic answer to include this fact, along with the reason why Brandon Sanderson ended up writing the last few books instead of Robert Jordan (Robert Jordan died).\n",
    "    \n",
    "\n",
    "    The given answer in the dataset was as follows: \"Robert Jordan is the author but he sadly passed away and his books were finished by Brandon Sanderson.\"\n",
    "    This answer is more pragmatic as we can easily see the added information as something necessary - Most people wouldn't think beforehand that a book series was written by more than one person, and they would default to asking about a singular writer or author. This is despite them actually wanting to know about all the potential writers, if there were indeed multiple writers. We expect this basic level of inference in daily conversation, and this answer provides that. The literal answer, however, does not.\n",
    "\n",
    "\n",
    "3) The topic is 'Cats Musical Wiki', with the starting question being \"I am a student and know nothing about cats musical wiki\".\n",
    "\n",
    "    This is an interesting 'question' as it's not phrased as a question, but rather, it is a simple statement when taken literally. If used to initiate a conversation, the other party would recognize this as a request to learn about the topic, or an attempt to make small talk by giving the teacher leeway to introduce tidbits of information of their own choosing to the conversation, thus steering it in their desired direction. \n",
    "\n",
    "    Surprisingly enough, both the literal and pragmatic answer spans do not even mention the 'wiki', but rather just talk about the cats musical itself.\n",
    "\n",
    "    The text provided by the dataset for the literal answer just includes details about the creator, the source material, and a range of dates and locations when and where it was played.\n",
    "\n",
    "    The answer was as follows: \"Cats was one of the longest running plays ever, starting in London and running for 21 years. I was lucky enough to sit in the audience in New York city for a performance once.\"\n",
    "\n",
    "    We can see that the teacher actually chose to share his own experience regarding the musical, despite not being prompted for such a thing; they actively chose to steer the conversation to talk about their own experience, and thus proving to be a cooperative conversationalist (And the conversation actually continued down that path, with questions like \"Where were you seated\" and so on), rather than simply providing basic details and closing off the conversation, like we'd expect from a literal answer by a non-cooperative teacher.\n",
    "\n",
    "4) The topic is 'Edward Elric', with the starting question being \"Who is Edward Elric?\".\n",
    "\n",
    "    So a literal answer to this question would be quite succint, such as \"A fictional alchemist in 'The Fullmetal Alchemist'\", \"The main protagonist of 'The Fullmetal Alchemist' series\" and so on. We'd expect a pragmatic answer to both combine these details, and then enrich the answer by giving context; sharing information about the character's traits or background.\n",
    "\n",
    "    The answer given does indeed fulfill these expectations: \"Edward Elric is the main protagonist of the Fullmetal Alchemist series. Edward lost hist right arm and left leg due to a failed Human Transplantation attempt and became the youngest State Alchemist in history at the age of twelve.\"\n",
    "\n",
    "    The answer includes further details than we'd expect from a purely literal interpretation, in line with what one would likely want to know when asking about a fictional character - like a background that hints at the character's motivation, and thus also the main plot of the story.\n",
    "\n",
    "I will stop here since I think this section is wordy enough already.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdaf54",
   "metadata": {},
   "source": [
    "## Part 1: The \"Traditional\" NLP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a35ae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import dspy\n",
    "import numpy as np\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from dspy.evaluate import SemanticF1 #no longer necessary\n",
    "import configparser\n",
    "from dspy.evaluate.auto_evaluation import (\n",
    "    SemanticRecallPrecision,\n",
    "    DecompositionalSemanticRecallPrecision\n",
    ")\n",
    "from dspy.predict.chain_of_thought import ChainOfThought\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('grok_key.ini')\n",
    "api_key = config['DEFAULT']['XAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7689f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_questions(filepath='../PragmatiCQA/data/val.jsonl'):\n",
    "\n",
    "    questions = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            conv = json.loads(line)\n",
    "            first_qa = conv['qas'][0]\n",
    "\n",
    "            #the spans will only include the text strings, not the keys\n",
    "            literal_spans = []\n",
    "            pragmatic_spans = []\n",
    "\n",
    "            if 'literal_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['literal_obj']:\n",
    "                    literal_spans.append(span_obj['text'])\n",
    "                    \n",
    "            if 'pragmatic_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['pragmatic_obj']:\n",
    "                    pragmatic_spans.append(span_obj['text'])\n",
    "\n",
    "            questions.append({\n",
    "            'question': first_qa['q'],\n",
    "            'gold_answer': first_qa['a'],\n",
    "            'topic': conv['topic'],\n",
    "            'genre': conv.get('genre', ''),\n",
    "            'community': conv.get('community', ''),\n",
    "            'literal_spans': literal_spans,\n",
    "            'pragmatic_spans': pragmatic_spans\n",
    "            })\n",
    "\n",
    "    return questions\n",
    "\n",
    "questions = get_first_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c94acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'who is freddy krueger?', 'gold_answer': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20N'], 'pragmatic_spans': ['Cannot GET /wiki/A%20N']}\n",
      "{'question': 'who was the star on this movie?', 'gold_answer': \"Robert Englund IS Freddy Kruger, the bad guy for these films. Note to you and to Adam, the Pragmatic one, the link here is broken and I can't paste relevant things, as has always been Nightmare's case, I'm perfectly good with answering your questions and will quickly do it, but have to open a tab in another window separate from the hit, I WILL go quickly and answer at rapid speed though, don't worry.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'What is the movie about?', 'gold_answer': 'Ok, here goes, I\\'m getting \"Cannot get\"..so, Nightmare on Elm street centers around the fact that in your dreams, Freddie Kruger, a dark figure can chase you and if you are killed while sleeping you die.', 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html', 'Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Who directed the new film?', 'gold_answer': \"It was Directed by: Samuel Bayer. Note that the link here is broken. So I'm having to get some of this from memory. I copied what I have (this is ALL I have).\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Is the Batman comic similar to the movies?', 'gold_answer': 'I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane , two very wealthy and charitable Gotham City socialites'], 'pragmatic_spans': ['While returning home one night, his parents were killed by a small-time criminal named Joe Chill ']}\n",
      "{'question': \"what is batman's real name?\", 'gold_answer': 'Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne'], 'pragmatic_spans': ['Batman is a superhero co-created by artist Bob Kane and writer Bill Finger . The character made his first appearance in Detective Comics #27 (May, 1939). Batman is the secret identity of Bruce Wayne .']}\n",
      "{'question': 'How old was batman when he first became batman?', 'gold_answer': \"I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [\"I don't know\"], 'pragmatic_spans': ['his parents were killed', \"Bruce swears an oath to rid the city of the evil that had taken his parents' lives.\\n\\n\"]}\n",
      "{'question': 'Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?', 'gold_answer': 'No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['No'], 'pragmatic_spans': ['intellect, detective skills, science and technology, wealth, physical prowess, and intimidation in his war on crime.']}\n",
      "{'question': \"Who are Batman's biggest enemies?\", 'gold_answer': 'The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' the Joker and Catwoman'], 'pragmatic_spans': [' the Joker and Catwoman', 'a new supervillain called Mr. Bloom ']}\n",
      "{'question': 'What is Batmans real name?', 'gold_answer': \"Batman's real identity is Bruce Wayne. He lives in Gotham City and is the CEO of Wayne Enterprises.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' Batman is the secret identity of Bruce Wayne .'], 'pragmatic_spans': [' CEO of Wayne Enterprises, ']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(questions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775021",
   "metadata": {},
   "source": [
    "As can be seen from this excerpt, there are a few questions with no literal or pragmatic spans at all, and this is not an issue on my end as even the teachers themselves state that they cannot access these wikis in their answers (see first, third and fourth questions). \n",
    "Considering that, and considering that the NLP model requires context, I'm left with two choices:\n",
    "\n",
    "1) Filter out the problematic questions\n",
    "2) Ignore them and set the context to be the same as the question, which will likely lead to errors and underplay distilbert's performance.\n",
    "\n",
    "We'll go with the filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6def7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original questions: 179\n",
      "Valid questions: 174\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_questions(questions):\n",
    "    \n",
    "    valid_questions = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # Check if any spans are invalid (start with \"Cannot GET /wiki/\")\n",
    "        invalid_literal = any(span.startswith(\"Cannot GET /wiki/\") for span in q['literal_spans'])\n",
    "        invalid_pragmatic = any(span.startswith(\"Cannot GET /wiki/\") for span in q['pragmatic_spans'])\n",
    "        \n",
    "        # Keep only questions with valid spans in both configurations\n",
    "        if not invalid_literal and not invalid_pragmatic:\n",
    "            valid_questions.append(q)\n",
    "    \n",
    "    return valid_questions\n",
    "\n",
    "# Execute filtering\n",
    "print(f\"Original questions: {len(questions)}\")\n",
    "questions = filter_valid_questions(questions)\n",
    "print(f\"Valid questions: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b3b81",
   "metadata": {},
   "source": [
    "Five questions have been filtered, which is not a very substantial amount, so it shouldn't really affect our testing.\n",
    "\n",
    "Below is our model which will handle all three contexts - literal, pragmatic and retrieved spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertRAG:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "        retriever = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "        self.embedder = dspy.Embedder(retriever.encode)\n",
    "\n",
    "        self.search_dict = {}  # a cache for retrievers\n",
    "\n",
    "    def create_search(self, community, topk_docs_to_retrieve=5):\n",
    "        \n",
    "        if community in self.search_dict:\n",
    "            return self.search_dict[community]\n",
    "\n",
    "        if not community:\n",
    "            return \"No community given.\"\n",
    "        \n",
    "        directory = f'../PragmatiCQA-sources/{community}'\n",
    "        corpus = []\n",
    "        #just the read_html from rag.ipynb \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".html\"):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    corpus.append(soup.get_text())\n",
    "\n",
    "        search = dspy.retrievers.Embeddings(embedder=self.embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "        self.search_dict[community] = search\n",
    "\n",
    "        return search\n",
    "    \n",
    "    def answer(self, question, context_type):\n",
    "        if context_type == 'literal':\n",
    "            context = \" \".join(question['literal_spans'])\n",
    "\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = \" \".join(question['pragmatic_spans'])\n",
    "\n",
    "        elif context_type == 'rag':\n",
    "            search = self.create_search(question['community'], topk_docs_to_retrieve=3)\n",
    "            result = search(question['question'])\n",
    "            \n",
    "            # truncating each passage since we want to include multiple docs within a small token limit\n",
    "            truncated_passages = []\n",
    "            for passage in result.passages:\n",
    "                truncated_passages.append(passage[:500] + \"...\" if len(passage) > 500 else passage)\n",
    "            \n",
    "            context = \" \".join(truncated_passages)\n",
    "\n",
    "        else:\n",
    "            return \"[Invalid context_type]\"  \n",
    "\n",
    "        # calculate available space for context, this is necessary since we want to minimize context token length since we feed it to the LLM afterwards... and that costs money.\n",
    "        question_tokens = self.tokenizer.encode(question['question'], add_special_tokens=False)\n",
    "        max_context_length = 512 - len(question_tokens) - 3  # 3 for special tokens [CLS], [SEP], [SEP]\n",
    "        \n",
    "        context_tokens = self.tokenizer.encode(context, add_special_tokens=False)\n",
    "        if len(context_tokens) > max_context_length:\n",
    "            context_tokens = context_tokens[:max_context_length]\n",
    "            context = self.tokenizer.decode(context_tokens)\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question['question'], \n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512, #the model can't handle more than 512 tokens as input anyway, so we cap it to prevent errors.\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        start_idx = torch.argmax(outputs.start_logits).item()\n",
    "        end_idx = torch.argmax(outputs.end_logits).item()\n",
    "        \n",
    "        if end_idx < start_idx:\n",
    "            end_idx = start_idx\n",
    "            \n",
    "        tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "        answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer.strip() if answer.strip() else \"[No answer found]\",\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bae14",
   "metadata": {},
   "source": [
    "So I have checked out SemanticF1 - it does not compute all three scores, it really only computes the F1 score, and that's it. I checked all of its attributes, and found nothing else regarding precision and recall. So I'll also be using the function SemanticF1 calls (According to the documentation) instead.\n",
    "\n",
    "Below is a small test with two evaluator LMs as I wanted to see if I can handle using a local evaluator on my 6GB VRAM GFX. \n",
    "\n",
    "Spoilers: The local model performed pretty badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7df6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SigClass = DecompositionalSemanticRecallPrecision\n",
    "sig_module = ChainOfThought(SigClass)\n",
    "\n",
    "def semantic_scores(example, pred):\n",
    "    scores = sig_module(\n",
    "        question=example.question,\n",
    "        ground_truth=example.response,\n",
    "        system_response=pred.response\n",
    "    )\n",
    "    precision = scores.precision\n",
    "    recall = scores.recall\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def evaluate_model(model, n=5):\n",
    "    for i, q in enumerate(questions[:n]):\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Question {i+1}: {q['question']}\")\n",
    "        print(\"Gold Answer:\", q['gold_answer'])\n",
    "\n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            ans = model.answer(q, context_type)\n",
    "\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "\n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "\n",
    "            print(f\"\\n{context_type.capitalize()} Answer: {ans['answer']}\")\n",
    "            print(f\"  Precision: {scores['precision']:.2f}, Recall: {scores['recall']:.2f}, F1: {scores['f1']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c35c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== EVALUATION WITH QWEN 2.5 ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.25, Recall: 0.25, F1: 0.25\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 0.25, Recall: 0.50, F1: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================== EVALUATION WITH GROK-3-MINI ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.50, Recall: 0.25, F1: 0.33\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 1.00, Recall: 0.25, F1: 0.40\n",
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH QWEN 2.5\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_qwen = dspy.LM('ollama_chat/qwen2.5:3b', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=evaluator_lm_qwen)\n",
    "\n",
    "evaluate_model(model, n=3)\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH GROK-3-MINI\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "evaluate_model(model, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c87a9",
   "metadata": {},
   "source": [
    "As can be seen above, the qwen 2.5 model is not good at evaluating. Frankly, neither is grok-3-mini. Answering \"Bruce\" to the question \"How old was batman when he first became batman\" should get a precision of near-zero, if not zero. Definitely not '1'. Anyway, I'll be proceeding with grok-3-mini as the evaluator model. \n",
    "Woe to the grok budget..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ea9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_questions(model, questions):\n",
    "    import time\n",
    "    import warnings\n",
    "    # i'm suppressing the output format warnings since they're frequent and annoying.\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    detailed_results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions...\")\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"Processing question {i + 1}/{len(questions)}.\")\n",
    "            \n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            \n",
    "            ans = model.answer(q, context_type)\n",
    "            \n",
    "            #creating an example like in the semanticf1 example. not sure if passing the context is strictly necessary; it'll be a massive waste of tokens in part 2.\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "            \n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "            results[context_type].append(scores)\n",
    "            \n",
    "            # storing data for future analysis\n",
    "            detailed_results[context_type].append({\n",
    "                \"question\": q['question'],\n",
    "                \"gold_answer\": q['gold_answer'],\n",
    "                \"predicted_answer\": ans['answer'],\n",
    "                \"context\": ans['context'],\n",
    "                \"scores\": scores\n",
    "            })\n",
    "            \n",
    "            # short delay to avoid hitting limits\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # note that i can't use dspy.Evaluate since i need all three (or at least the first two) metrics individually...\n",
    "    avg_results = {}\n",
    "    for context_type in results:\n",
    "        if results[context_type]:  \n",
    "            avg_results[context_type] = {\n",
    "                \"precision\": sum(s[\"precision\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"recall\": sum(s[\"recall\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"f1\": sum(s[\"f1\"] for s in results[context_type]) / len(results[context_type])\n",
    "            }\n",
    "    \n",
    "    json_data = {\n",
    "        \"summary\": avg_results,\n",
    "        \"detailed_results\": detailed_results\n",
    "    }\n",
    "    \n",
    "    with open('part_1_eval.json', 'w') as f:\n",
    "        json.dump(json_data, f)\n",
    "    \n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "def print_results_table(avg_results):\n",
    "    \n",
    "    print(\"\\nEVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Context Type':<20} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "        if context_type in avg_results:\n",
    "            precision = avg_results[context_type][\"precision\"]\n",
    "            recall = avg_results[context_type][\"recall\"]\n",
    "            f1 = avg_results[context_type][\"f1\"]\n",
    "            print(f\"{context_type.capitalize():<20} {precision:<12f} {recall:<12f} {f1:<12}\")\n",
    "        else:\n",
    "            print(f\"{context_type.capitalize():<20} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179ba0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 174 questions...\n",
      "Processing question 1/174.\n",
      "Processing question 2/174.\n",
      "Processing question 3/174.\n",
      "Processing question 4/174.\n",
      "Processing question 5/174.\n",
      "Processing question 6/174.\n",
      "Processing question 7/174.\n",
      "Processing question 8/174.\n",
      "Processing question 9/174.\n",
      "Processing question 10/174.\n",
      "Processing question 11/174.\n",
      "Processing question 12/174.\n",
      "Processing question 13/174.\n",
      "Processing question 14/174.\n",
      "Processing question 15/174.\n",
      "Processing question 16/174.\n",
      "Processing question 17/174.\n",
      "Processing question 18/174.\n",
      "Processing question 19/174.\n",
      "Processing question 20/174.\n",
      "Processing question 21/174.\n",
      "Processing question 22/174.\n",
      "Processing question 23/174.\n",
      "Processing question 24/174.\n",
      "Processing question 25/174.\n",
      "Processing question 26/174.\n",
      "Processing question 27/174.\n",
      "Processing question 28/174.\n",
      "Processing question 29/174.\n",
      "Processing question 30/174.\n",
      "Processing question 31/174.\n",
      "Processing question 32/174.\n",
      "Processing question 33/174.\n",
      "Processing question 34/174.\n",
      "Processing question 35/174.\n",
      "Processing question 36/174.\n",
      "Processing question 37/174.\n",
      "Processing question 38/174.\n",
      "Processing question 39/174.\n",
      "Processing question 40/174.\n",
      "Processing question 41/174.\n",
      "Processing question 42/174.\n",
      "Processing question 43/174.\n",
      "Processing question 44/174.\n",
      "Processing question 45/174.\n",
      "Processing question 46/174.\n",
      "Processing question 47/174.\n",
      "Processing question 48/174.\n",
      "Processing question 49/174.\n",
      "Processing question 50/174.\n",
      "Processing question 51/174.\n",
      "Processing question 52/174.\n",
      "Processing question 53/174.\n",
      "Processing question 54/174.\n",
      "Processing question 55/174.\n",
      "Processing question 56/174.\n",
      "Processing question 57/174.\n",
      "Processing question 58/174.\n",
      "Processing question 59/174.\n",
      "Processing question 60/174.\n",
      "Processing question 61/174.\n",
      "Processing question 62/174.\n",
      "Processing question 63/174.\n",
      "Processing question 64/174.\n",
      "Processing question 65/174.\n",
      "Processing question 66/174.\n",
      "Processing question 67/174.\n",
      "Processing question 68/174.\n",
      "Processing question 69/174.\n",
      "Processing question 70/174.\n",
      "Processing question 71/174.\n",
      "Processing question 72/174.\n",
      "Processing question 73/174.\n",
      "Processing question 74/174.\n",
      "Processing question 75/174.\n",
      "Processing question 76/174.\n",
      "Processing question 77/174.\n",
      "Processing question 78/174.\n",
      "Processing question 79/174.\n",
      "Processing question 80/174.\n",
      "Processing question 81/174.\n",
      "Processing question 82/174.\n",
      "Processing question 83/174.\n",
      "Processing question 84/174.\n",
      "Processing question 85/174.\n",
      "Processing question 86/174.\n",
      "Processing question 87/174.\n",
      "Processing question 88/174.\n",
      "Processing question 89/174.\n",
      "Processing question 90/174.\n",
      "Processing question 91/174.\n",
      "Processing question 92/174.\n",
      "Processing question 93/174.\n",
      "Processing question 94/174.\n",
      "Processing question 95/174.\n",
      "Processing question 96/174.\n",
      "Processing question 97/174.\n",
      "Processing question 98/174.\n",
      "Processing question 99/174.\n",
      "Processing question 100/174.\n",
      "Processing question 101/174.\n",
      "Processing question 102/174.\n",
      "Processing question 103/174.\n",
      "Processing question 104/174.\n",
      "Processing question 105/174.\n",
      "Processing question 106/174.\n",
      "Processing question 107/174.\n",
      "Processing question 108/174.\n",
      "Processing question 109/174.\n",
      "Processing question 110/174.\n",
      "Processing question 111/174.\n",
      "Processing question 112/174.\n",
      "Processing question 113/174.\n",
      "Processing question 114/174.\n",
      "Processing question 115/174.\n",
      "Processing question 116/174.\n",
      "Processing question 117/174.\n",
      "Processing question 118/174.\n",
      "Processing question 119/174.\n",
      "Processing question 120/174.\n",
      "Processing question 121/174.\n",
      "Processing question 122/174.\n",
      "Processing question 123/174.\n",
      "Processing question 124/174.\n",
      "Processing question 125/174.\n",
      "Processing question 126/174.\n",
      "Processing question 127/174.\n",
      "Processing question 128/174.\n",
      "Processing question 129/174.\n",
      "Processing question 130/174.\n",
      "Processing question 131/174.\n",
      "Processing question 132/174.\n",
      "Processing question 133/174.\n",
      "Processing question 134/174.\n",
      "Processing question 135/174.\n",
      "Processing question 136/174.\n",
      "Processing question 137/174.\n",
      "Processing question 138/174.\n",
      "Processing question 139/174.\n",
      "Processing question 140/174.\n",
      "Processing question 141/174.\n",
      "Processing question 142/174.\n",
      "Processing question 143/174.\n",
      "Processing question 144/174.\n",
      "Processing question 145/174.\n",
      "Processing question 146/174.\n",
      "Processing question 147/174.\n",
      "Processing question 148/174.\n",
      "Processing question 149/174.\n",
      "Processing question 150/174.\n",
      "Processing question 151/174.\n",
      "Processing question 152/174.\n",
      "Processing question 153/174.\n",
      "Processing question 154/174.\n",
      "Processing question 155/174.\n",
      "Processing question 156/174.\n",
      "Processing question 157/174.\n",
      "Processing question 158/174.\n",
      "Processing question 159/174.\n",
      "Processing question 160/174.\n",
      "Processing question 161/174.\n",
      "Processing question 162/174.\n",
      "Processing question 163/174.\n",
      "Processing question 164/174.\n",
      "Processing question 165/174.\n",
      "Processing question 166/174.\n",
      "Processing question 167/174.\n",
      "Processing question 168/174.\n",
      "Processing question 169/174.\n",
      "Processing question 170/174.\n",
      "Processing question 171/174.\n",
      "Processing question 172/174.\n",
      "Processing question 173/174.\n",
      "Processing question 174/174.\n"
     ]
    }
   ],
   "source": [
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "avg_results = evaluate_all_questions(model, questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235f59",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68642cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Context Type         Precision    Recall       F1          \n",
      "--------------------------------------------------\n",
      "Literal              0.816571     0.289534     0.40379242303802887\n",
      "Pragmatic            0.744828     0.268122     0.3715092992171465\n",
      "Rag                  0.247222     0.085625     0.11761999779137157\n"
     ]
    }
   ],
   "source": [
    "print_results_table(avg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d533",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Below I'll be printing some excerpts of questions, and then analyzing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08bd2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS OF EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "1. RAG EXAMPLES WITH F1 SCORE = 0.0\n",
      "----------------------------------------\n",
      "Found 126 RAG examples with F1 = 0.0\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1 (Question #1):\n",
      "Question: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "RAG Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 2 (Question #3):\n",
      "Question: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "RAG Answer: February 23, 1948\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 3 (Question #4):\n",
      "Question: Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?\n",
      "Gold Answer: No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.\n",
      "RAG Answer: invisibility\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 4 (Question #5):\n",
      "Question: Who are Batman's biggest enemies?\n",
      "Gold Answer: The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 5 (Question #7):\n",
      "Question: Ok, Is batman a superhero?\n",
      "Gold Answer: Yes, he has been the protector of Gotham City for a long time.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('part_1_eval.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"ANALYSIS OF EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. RAG EXAMPLES WITH F1 SCORE = 0.0\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "rag_zero_examples = []\n",
    "for i, result in enumerate(data['detailed_results']['rag']):\n",
    "    if result['scores']['f1'] == 0.0:\n",
    "        rag_zero_examples.append((i, result))\n",
    "\n",
    "print(f\"Found {len(rag_zero_examples)} RAG examples with F1 = 0.0\")\n",
    "print(\"\\nShowing first 5 examples:\\n\")\n",
    "\n",
    "for idx, (i, example) in enumerate(rag_zero_examples[:5]):\n",
    "    print(f\"Example {idx + 1} (Question #{i + 1}):\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"RAG Answer: {example['predicted_answer']}\")\n",
    "    print(f\"F1 Score: {example['scores']['f1']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a973e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS: LITERAL OUTPERFORMING PRAGMATIC\n",
      "============================================================\n",
      "Example 1 (Question #11):\n",
      "Gap: 0.500 (Literal: 0.500, Pragmatic: 0.000)\n",
      "Question: how old is batman?\n",
      "Gold Answer: Batman made his first appearence in media in May of 1939, so he is quite old. He premiered as a vigilante.\n",
      "Literal Answer: May, 1939\n",
      "Pragmatic Answer: Batman\n",
      "Literal Scores - P: 1.000, R: 0.333\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 2 (Question #13):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: what is batman's real name? \n",
      "Gold Answer: Bruce Wayne is the real name of Batman who, after witnessing the murder of his parents as a child, donned a bat-themed costume in order to fight crime.\n",
      "Literal Answer: Bruce Wayne\n",
      "Pragmatic Answer: his parents\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 3 (Question #15):\n",
      "Gap: 0.667 (Literal: 0.667, Pragmatic: 0.000)\n",
      "Question: what year was it release? \n",
      "Gold Answer: He first appeared in May 1939. The comic's name was Detective comics.\n",
      "Literal Answer: 1939\n",
      "Pragmatic Answer: 27\n",
      "Literal Scores - P: 1.000, R: 0.500\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 4 (Question #24):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: Hi. What is Batman's name?\n",
      "Gold Answer: Batman's real name is Bruce Wayne. Bruce Wayne, born to Thomas Wayne and Martha Kane, is the CEO of Wayne Enterprise, who also protects Gotham as Batman.\n",
      "Literal Answer: Bruce Wayne\n",
      "Pragmatic Answer: Martha Kane\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 5 (Question #25):\n",
      "Gap: 0.500 (Literal: 0.500, Pragmatic: 0.000)\n",
      "Question: When did the Batman comics first appear?\n",
      "Gold Answer: The Batman first appeared in Detective Comics #27 in May 1939, and soon got the title Batman after his introduction\n",
      "Literal Answer: 1939\n",
      "Pragmatic Answer: soon after his introduction\n",
      "Literal Scores - P: 1.000, R: 0.333\n",
      "Pragmatic Scores - P: 1.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 6 (Question #30):\n",
      "Gap: 0.667 (Literal: 0.667, Pragmatic: 0.000)\n",
      "Question: I filled out the test & clicked submit.  \n",
      "Gold Answer: Batman is known for high level resources and intelligence, a master in martial arts and physical conditions and equipped with high tech gears. He is also a member of Justice League and his alter ego is Bruce Wayne\n",
      "Literal Answer: clicked submit. Genius - level intelligence Master detective Master escapologist Peak human physical condition Master martial artist Access to high tech equipment\n",
      "Pragmatic Answer: clicked\n",
      "Literal Scores - P: 0.667, R: 0.667\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 7 (Question #41):\n",
      "Gap: 1.000 (Literal: 1.000, Pragmatic: 0.000)\n",
      "Question: what year was the show release ? \n",
      "Gold Answer: Good morning!The first American Supernanny show began airing on ABC on January 7, 2005.\n",
      "Literal Answer: 2005\n",
      "Pragmatic Answer: COVID - 19 Pandemic\n",
      "Literal Scores - P: 1.000, R: 1.000\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 8 (Question #43):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: What is Supernanny?\n",
      "Gold Answer: Supernanny is a TV show that started as a British series and was successful enough to broadcast overseas versions including an the US version in 2005.\n",
      "Literal Answer: British TV series\n",
      "Pragmatic Answer: The\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 9 (Question #44):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: What is Supernanny about?\n",
      "Gold Answer: Supernanny is a British reality television programme about parents struggling with their children's behaviour, mealtime, potty training, etc. The show features professional nanny Jo Frost, who devotes each episode to helping a family where the parents are struggling with child-rearing.\n",
      "Literal Answer: The British Supernanny\n",
      "Pragmatic Answer: Moonshine music\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n",
      "Example 10 (Question #52):\n",
      "Gap: 0.400 (Literal: 0.400, Pragmatic: 0.000)\n",
      "Question: what is Supernanny?\n",
      "Gold Answer: Supernanny started off as a British TV series. The show was successful enough that overseas versions of Supernanny were broadcast in France, Germany, Brazil, Portugal, Spain, Denmark, The Netherlands, Israel, Belgium, Austria, Croatia, Poland, Romania, Russia, and Sweden with five further local format versions in production from Chile to Slovakia. Added to this, the UK and US English-language version have been broadcast in 47 different territories.\n",
      "Literal Answer: The first American Supernanny show\n",
      "Pragmatic Answer: child therapist Mike Ruggles\n",
      "Literal Scores - P: 1.000, R: 0.250\n",
      "Pragmatic Scores - P: 0.000, R: 0.000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"ANALYSIS: LITERAL OUTPERFORMING PRAGMATIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate gaps where literal F1 > pragmatic F1\n",
    "literal_results = data['detailed_results']['literal']\n",
    "pragmatic_results = data['detailed_results']['pragmatic']\n",
    "\n",
    "gaps = []\n",
    "for i, (lit_result, prag_result) in enumerate(zip(literal_results, pragmatic_results)):\n",
    "    lit_f1 = lit_result['scores']['f1']\n",
    "    prag_f1 = prag_result['scores']['f1']\n",
    "    \n",
    "    gap = lit_f1 - prag_f1\n",
    "    \n",
    "    # Only consider cases where literal is better by more than 0.2\n",
    "    if gap > 0.2:\n",
    "        gaps.append({\n",
    "            'question_idx': i,\n",
    "            'question': lit_result['question'],\n",
    "            'gold_answer': lit_result['gold_answer'],\n",
    "            'literal_answer': lit_result['predicted_answer'],\n",
    "            'pragmatic_answer': prag_result['predicted_answer'],\n",
    "            'literal_f1': lit_f1,\n",
    "            'pragmatic_f1': prag_f1,\n",
    "            'gap': gap,\n",
    "            'literal_scores': lit_result['scores'],\n",
    "            'pragmatic_scores': prag_result['scores']\n",
    "        })\n",
    "\n",
    "for idx, example in enumerate(gaps[:10]):\n",
    "    print(f\"Example {idx + 1} (Question #{example['question_idx'] + 1}):\")\n",
    "    print(f\"Gap: {example['gap']:.3f} (Literal: {example['literal_f1']:.3f}, Pragmatic: {example['pragmatic_f1']:.3f})\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"Literal Answer: {example['literal_answer']}\")\n",
    "    print(f\"Pragmatic Answer: {example['pragmatic_answer']}\")\n",
    "    print(f\"Literal Scores - P: {example['literal_scores']['precision']:.3f}, R: {example['literal_scores']['recall']:.3f}\")\n",
    "    print(f\"Pragmatic Scores - P: {example['pragmatic_scores']['precision']:.3f}, R: {example['pragmatic_scores']['recall']:.3f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89165699",
   "metadata": {},
   "source": [
    "The most glaring issue of the experiment is the low scoring on the retrieved answers. It's quite easy to see why this is the case.\n",
    "\n",
    "First, we must consider the distilbert model's token limit, which is rather small (512), together with the fact that at the end of our RAG module's embeddings, we end up with the text of entire documents per document, instead of specific, more relevant sentences or passages. Even after stripping all the HTML code and such (Through BeautifulSoup), we still end up with quite a lot of \"fluff\", like section titles and so on, that don't provide a lot of meaningful info. This ends up consuming the majority of the allocated tokens per document, making it so that even if the document is relevant, the retrieved passages are not all that likely to include the relevant information unless it's at the very start of the document.\n",
    "\n",
    "For example, the second question has a retrieved answer of 'February 23, 1948', which is nonsensical in the context of the question - a question about batman's age at a certain period of time. It becomes apparent after a little bit of searching that this is the birth date of some writer of the batman series, and this date was likely just present at the very start of one of the document the module retrieved, and one of the very few pieces of information that are even tangentially related to 'age' or 'dates'.\n",
    "\n",
    "Regarding pragmatic answers, I've printed out a few examples where the literal answer outperforms the pragmatic one, to see if the model repeatedly fails at providing more details. After looking at some examples (I've checked out more examples than shown, this is truncated), the reality is that the pragmatic answers were both short and very literal, while being wrong. I believe this shows a clear limitation of the distilbert model to extract answers, even literal answers, from a span that contains more information on top of what's immediately relevant to the question.\n",
    "\n",
    "The model seems to succeed at extracting relatively simple contextual information, like correlating \"when\" in the question with a date in the span, and so on.\n",
    "\n",
    "Overall, this model proved to be incapable of proper pragmatic inference / answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f75e81",
   "metadata": {},
   "source": [
    "## Part 2: The LLM Multi-Step Prompting Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a062c0",
   "metadata": {},
   "source": [
    "A brief explanation of the model:\n",
    "1) Retriever - it's basically the same as the retriever I've embedded directly in the previous model for the traditional approach. Using a dictionary as a corpus cache since I can spare the memory for it (Besides, we're only evaluating part of the dataset at once, and not the entire thing).\n",
    "\n",
    "2) QAModel - It's a model designed to handle a single question of a conversation, with I/O fields defined specifically for that purpose. I've decided to incorporate the reasoning and summary strategies to try and improve performance as they don't require a whole lot of effort to implement, and are likely to improve results. The summary is fed to the model with every question, and the model is expected to return an updated summary, in a form that's hopefully not a list of QA pairs, which includes the current QA pair.\n",
    "\n",
    "    There should be an emphasis on the questions, rather than the answers within the summary, as the answers stem from the model itself and are not ground_truth; they could throw off future answers.\n",
    "\n",
    "3) ConversationModel - This model is mainly just a wrapper for the QAModel that maintains the summary object across questions within a single conversation. The method of running this model is simply calling process_question iteratively for every question in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532fb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        retriever = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "        self.embedder = dspy.Embedder(retriever.encode)\n",
    "        self.search_dict = {}\n",
    "\n",
    "    def create_search(self, community, topk_docs_to_retrieve=5):\n",
    "        if community in self.search_dict:\n",
    "            return self.search_dict[community]\n",
    "\n",
    "        if not community:\n",
    "            return None\n",
    "        \n",
    "        directory = f'../PragmatiCQA-sources/{community}'\n",
    "        corpus = []\n",
    "        \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".html\"):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    corpus.append(soup.get_text())\n",
    "\n",
    "        search = dspy.retrievers.Embeddings(embedder=self.embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "        self.search_dict[community] = search\n",
    "        return search\n",
    "    \n",
    "\n",
    "class CooperativeTeaching(dspy.Signature):\n",
    "            \n",
    "            question = dspy.InputField(desc=\"The current question from the student.\")\n",
    "            topic = dspy.InputField(desc=\"The main topic being discussed.\")\n",
    "            context = dspy.InputField(desc=\"Retrieved context relevant to the question (max 10k characters).\")\n",
    "            previous_summary = dspy.InputField(desc=\"Summary of the conversation so far, focusing on student interests and queries\")\n",
    "\n",
    "            reasoning = dspy.OutputField(desc=\"A short justification for your answer, no more than a single sentence\")\n",
    "            answer = dspy.OutputField(desc=\"Cooperative answer that predicts other relevant information the student while need, while staying focused and no longer than two sentences.\")\n",
    "            updated_summary = dspy.OutputField(desc=\" A new summary that outlines both the previous summary and the current question and answer. No more than 200 words.\")\n",
    "        \n",
    "\n",
    "\n",
    "class QAModel(dspy.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base_prompt = (\n",
    "            \"You are a teacher who provides answers to their students' questions. \"\n",
    "            \"You are given a single question, which is part of on-going discussion with a specific student. \" \n",
    "            \"You are tasked with answering their question in a helpful and cooperative manner, that keeps the conversation flow going. \" \n",
    "            \"Your answer should be derived largely from the provided context, which are excerpts from relevant pages of the Fandom wiki which corresponds to the topic of discussion. \"\n",
    "            \"You are also tasked with maintaining an up-to-date summary of all student-teacher interactions in this conversation, which should include the current question and answer. \" \n",
    "            \"The summary should be natural, and be helpful to a teacher who might have to answer the next question of the student in a cooperative manner. \"\n",
    "            \"Also provide a short reasoning for your answer, no more than a sentence long. \"\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.generate_response = dspy.ChainOfThought(\n",
    "            signature=CooperativeTeaching,\n",
    "            prompt=base_prompt\n",
    "        )\n",
    "    \n",
    "    def limit_summary_chars(self, summary, max_chars=1000):\n",
    "        if len(summary) > max_chars:\n",
    "            return summary[:max_chars]\n",
    "        return summary\n",
    "    \n",
    "    def forward(self, question, topic, context, previous_summary=\"New conversation starting\"):\n",
    "        # Ensure previous_summary is within character limits\n",
    "        limited_summary = self.limit_summary_chars(previous_summary)\n",
    "\n",
    "        response = self.generate_response(\n",
    "            question=question,\n",
    "            topic=topic, \n",
    "            context=context,\n",
    "            previous_summary=limited_summary\n",
    "        )\n",
    "        \n",
    "        # Limit the updated summary to character count\n",
    "        limited_updated_summary = self.limit_summary_chars(response.updated_summary)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            reasoning=response.reasoning,\n",
    "            answer=response.answer,\n",
    "            updated_summary=limited_updated_summary\n",
    "        )\n",
    "\n",
    "\n",
    "class ConversationModel:\n",
    "    def __init__(self, qa_model, retriever):\n",
    "        self.qa_model = qa_model\n",
    "        self.retriever = retriever\n",
    "        self.conversation_summary = \"New conversation starting\"\n",
    "    \n",
    "    def process_question(self, question, topic, community):\n",
    "        \n",
    "        search = self.retriever.create_search(community, topk_docs_to_retrieve=3)\n",
    "        if search:\n",
    "            result = search(question)\n",
    "            \n",
    "            # Truncate each passage (document) to 2000 characters\n",
    "            truncated_passages = []\n",
    "            for passage in result.passages:\n",
    "                truncated_passages.append(passage[:2000] + \"...\" if len(passage) > 2000 else passage)\n",
    "            \n",
    "            context = \" \".join(truncated_passages)\n",
    "        else:\n",
    "            context = \"No context available\"\n",
    "        \n",
    "        prediction = self.qa_model(\n",
    "            question=question,\n",
    "            topic=topic,\n",
    "            context=context,\n",
    "            previous_summary=self.conversation_summary\n",
    "        )\n",
    "        \n",
    "        self.conversation_summary = prediction.updated_summary\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset for new conversation\"\"\"\n",
    "        self.conversation_summary = \"New conversation starting\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "444a2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup for the model and conversations. the model is used for both prediction and evaluation.\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "filepath = '../PragmatiCQA/data/val.jsonl'\n",
    "\n",
    "conversations = []\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "retriever = Retriever()\n",
    "qa_model = QAModel()\n",
    "conv_model = ConversationModel(qa_model, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bf5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conversation on topic: 'Batman'\n",
      "Number of questions: 9\n",
      "============================================================\n",
      "\n",
      "Question 1:\n",
      "Q: What is Batmans real name?\n",
      "Predicted Answer: Batman's real name is Bruce Wayne, the wealthy billionaire and philanthropist who uses his resources and skills to fight crime in Gotham City as a vigilante. Since you're asking about this, it seems like you might be curious about superhero originswould you like me to explain more about Bruce Wayne's backstory, his role in the Justice League, or recommendations for Batman stories to dive deeper?\n",
      "Gold Answer: Batman's real identity is Bruce Wayne. He lives in Gotham City and is the CEO of Wayne Enterprises.\n",
      "Reasoning: First, the student's question is \"What is Batman's real name?\", which is a direct inquiry about the identity of the Batman character. The topic is \"Batman\", so I reviewed the provided context, which includes multiple references to Batman's real name. In the context, under sections like \"The Batman\" and \"General Information\", it explicitly states that Batman's real name is Bruce Wayne. For example, it mentions \"Real name: Bruce Wayne\" in the character details and repeats this in the history and general descriptions. This information is consistent across the context, originating from DC Comics sources. \n",
      "\n",
      "Next, since this is a new conversation (as indicated by the previous summary), I need to ensure the answer is cooperative and anticipates the student's potential interests, such as exploring more about Batman's backstory, motivations, or related topics in superhero lore. My reasoning focuses on accuracy based on the context while keeping the response engaging to encourage further learning.\n",
      "\n",
      "Finally, for the updated summary, I must focus only on the student's questions and interests, not on the answers or any model-generated content. The student's query suggests an interest in superhero identities, so I'll update the summary to reflect this without revealing details from my response.\n",
      "Updated Summary: The student is asking about the real name of Batman, indicating an interest in superhero characters and their secret identities, possibly as part of exploring comic book lore or DC Comics in general.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 2:\n",
      "Q: How did Batman become Batman?\n",
      "Predicted Answer: Batman, whose real identity is Bruce Wayne, became the Dark Knight after a pivotal event in his childhood. As a young boy, Bruce witnessed the murder of his parents, which deeply traumatized him and ignited a lifelong vow to fight crime and protect Gotham City. Inspired by heroes like Zorro and the Gray Ghost, he dedicated years to rigorous physical and mental training, mastering martial arts, detective skills, and various disciplines around the world. Upon returning to Gotham, he adopted the Batman personausing his intellect, gadgets, and fear tacticsto strike terror into criminals' hearts. This origin story highlights themes of justice, resilience, and personal transformation, which are central to many superhero narratives. If you're fascinated by this, would you like to learn about other aspects, like his key allies (such as Robin) or iconic villains (like the Joker), to dive deeper into DC Comics?\n",
      "Gold Answer: As a child, Bruce witnessed the murder of his parents. This motivated him to train to peak performance and wear a bat costume while fighting crime. He does not have any superpowers, but instead he uses intellect, detective skills, wealth, and physical strength to be a hero.\n",
      "Reasoning: First, the student's question is \"How did Batman become Batman?\", which seeks details on Batman's origin story. The topic is \"Batman\", and the context provides relevant information from Batman's history, including his early life as Bruce Wayne, where he idolized heroes like Zorro and the Gray Ghost, and was likely influenced by traumatic events such as his parents' murder. This aligns with standard DC Comics lore, where Bruce Wayne's transformation into Batman stems from witnessing his parents' death, motivating him to fight crime.\n",
      "\n",
      "Step 1: Analyze the question and context. The context mentions Bruce Wayne's early life and his admiration for masked heroes, which sets the stage for his origin. It doesn't explicitly detail the full story, but I can infer from the provided details and general knowledge that the core elements involve his childhood trauma and subsequent training.\n",
      "\n",
      "Step 2: Ensure the answer is cooperative and anticipates student needs. Since the previous summary indicates an interest in superhero characters, secret identities, and comic book lore, I should provide a clear, engaging explanation that ties into this interest, perhaps by suggesting related topics like other DC heroes or Batman's allies/enemies to encourage further exploration.\n",
      "\n",
      "Step 3: Structure the answer to be informative yet concise, focusing on key events without overwhelming the student, and end with a question to keep the conversation going.\n",
      "\n",
      "Step 4: Update the summary. The previous summary focused on the student's interest in Batman's real name and secret identities. Now, with this new question, I need to expand it to include their curiosity about Batman's origin, while keeping the focus on questions and interests only, not providing any answers or details from the response.\n",
      "\n",
      "Step 5: Verify output constraints. The updated summary must be under 1200 characters and center solely on the student's evolving questions and interests.\n",
      "Updated Summary: The student is asking about Batman's origin story, building on their previous interest in his real name and secret identities. This suggests a growing curiosity in superhero characters, their backstories, and elements of comic book lore, possibly as part of exploring themes like justice, trauma, and heroism in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 3:\n",
      "Q: Is Batman good at his job?\n",
      "Predicted Answer: Yes, Batman is generally portrayed as exceptionally good at his job as a crimefighter! In the DC Comics universe, Bruce Wayne transforms into Batman through rigorous training, using his genius-level intellect, detective skills, and mastery of martial arts to outsmart and defeat villains. For instance, he's often called \"The World's Greatest Detective\" because of his ability to solve complex crimes and anticipate threats. This effectiveness stems from his traumatic origin, where he vowed to protect Gotham City after witnessing his parents' murder. If you're curious, we could explore specific examples, like how he handles iconic villains or uses his gadgets, to see how his skills make him such a compelling hero. What aspect of his job interests you most?\n",
      "Gold Answer: Yes, Batman is motivated by a love for humanity. He continually refrains from killing the criminals he fights. An impressive move, Batman developed a fake persona for Bruce Wayne to protect his identity as the Batman.\n",
      "Reasoning: First, the student's question is \"Is Batman good at his job?\" which refers to Batman's role as a crimefighter, based on the topic and context. The context describes Batman as Bruce Wayne, a highly skilled individual with genius-level intelligence, master detective skills, martial arts expertise, and access to advanced technology, all of which contribute to his effectiveness in combating crime. This builds on the previous summary, where the student showed interest in Batman's origin story, real name, and secret identities, suggesting a broader curiosity about his character and heroic capabilities.\n",
      "\n",
      "Step 1: Analyze the question  It's subjective but can be addressed using evidence from the context, which portrays Batman as successful in his vigilante role through his training, intellect, and tools.\n",
      "\n",
      "Step 2: Draw from context  Key points include Batman's abilities (e.g., master detective, peak physical condition) and his history of fighting crime, which indicate he is highly competent.\n",
      "\n",
      "Step 3: Make it cooperative  Anticipate the student's interests by providing an engaging, evidence-based answer that ties back to his backstory, potentially sparking follow-up questions about specific skills or challenges, while keeping it age-appropriate and encouraging.\n",
      "\n",
      "Step 4: Ensure the answer is balanced  Affirm his strengths without overgeneralizing, as the context is from comic lore, and invite critical thinking.\n",
      "\n",
      "Step 5: Prepare updated summary  Focus on the student's evolving questions (now including Batman's job performance) and interests (e.g., superhero lore, justice themes), without referencing my answer, to track conversation progression.\n",
      "Updated Summary: The student is now asking whether Batman is effective in his role as a crimefighter, expanding on their earlier questions about his origin story, real name, and secret identities. This indicates a deepening interest in superhero characters, their abilities, and how they embody themes of justice and heroism in DC Comics, possibly reflecting curiosity about personal growth, crime-fighting strategies, and moral complexities in comic book narratives.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 4:\n",
      "Q: Does Batman have any friends?\n",
      "Predicted Answer: Yes, Batman, also known as Bruce Wayne, has several key allies who can be considered friends, though his relationships are often complex and built on trust rather than traditional friendship. In DC Comics and related media, his closest companions include Alfred Pennyworth, his loyal butler and confidant who provides emotional support and logistical aid; Robin (various iterations like Dick Grayson), who serves as a sidekick and partner in crime-fighting; and Commissioner James Gordon, a reliable ally in the Gotham City Police Department. These relationships highlight how Batman balances solitude with alliances to enhance his effectiveness as a hero. If you're curious about how these dynamics affect his strategies or personal growth, I can explore that furtherlet me know!\n",
      "Gold Answer: Yes, most notably Batman interacts with his butler and legal guardian Alfred Pennyworth. As Batman, he is often aligned with Gotham City Police Commissioner James Gordon. Batman also frequently has relationships with women, the most prominent being Catwoman.\n",
      "Reasoning: First, the student's question is \"Does Batman have any friends?\" which builds on their previous interests in Batman's effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests they're exploring Batman's personal relationships, possibly to understand how his social connections impact his heroism and isolation. The topic is \"Batman,\" and the context provided focuses on Batman films, comics, and franchises, such as Batman '89, various adaptations, and Warner Bros. productions, but it doesn't directly address friends. From general DC Comics knowledge, Batman has allies like Alfred Pennyworth, Robin, Commissioner Gordon, and others, which I can use to form a cooperative response.\n",
      "\n",
      "Step 1: Analyze the question in contextit's not explicitly covered in the provided text, so I'll draw from established Batman lore to provide a helpful, accurate answer while anticipating the student's curiosity about themes like loneliness in heroism.\n",
      "\n",
      "Step 2: Connect to previous summaryThe student's interests are in superhero abilities, justice, personal growth, and moral complexities, so I'll frame the answer to link Batman's friends to his crime-fighting strategies and emotional support, encouraging deeper exploration.\n",
      "\n",
      "Step 3: Ensure the answer is cooperativeMake it engaging, balanced, and open-ended to anticipate follow-up questions, such as specific allies or their roles.\n",
      "\n",
      "Step 4: Update the summaryFocus only on the student's questions and interests (e.g., add this new query about friends) without mentioning my answers, to track their evolving curiosity in Batman's character dynamics.\n",
      "\n",
      "Step 5: Keep outputs conciseAnswer should be informative yet brief, and updated summary under 1200 characters.\n",
      "Updated Summary: The student is asking whether Batman has any friends, expanding on their previous questions about his effectiveness as a crimefighter, origin story, real name, and secret identities. This indicates a growing interest in superhero characters' personal relationships, social dynamics, and how these elements tie into themes of justice, heroism, personal growth, crime-fighting strategies, and moral complexities in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 5:\n",
      "Q: How many movies is Batman in?\n",
      "Predicted Answer: Batman has appeared in a variety of films over the years, and based on the information available, the Batman film franchise includes at least nine theatrical live-action films, plus two live-action serials. This count comes from the main franchise entries like the original anthology films (e.g., Batman from 1989), the Dark Knight trilogy, and recent ones like The Batman from 2022. He also features in crossover films such as Batman v Superman and Justice League, which might add to the total depending on how you define it. To give you a more precise number, it's generally around 9-15 feature films if we include crossovers, but this can vary with new releases. If you're curious about specific movies or how Batman's character evolves across themperhaps tying into his relationships or crime-fighting strategiesI can recommend resources or discuss that next to deepen your interest!\n",
      "Gold Answer: I am unsure of the exact number, but Batman has become one of the most popular icons in the world. He can be found in radio, television, film, comics, and more. \n",
      "Reasoning: First, the student's question is \"How many movies is Batman in?\", which seeks a count of films featuring Batman. The topic is \"Batman\", and the context provides detailed information on Batman's film franchise, including serials and theatrical films. The context explicitly states that there are \"nine theatrical live-action films and two live-action serials\" featuring Batman, and it lists various films across different eras and studios.\n",
      "\n",
      "To answer accurately, I reviewed the context: It mentions two serials (from Columbia), one film from 20th Century Fox, and several from Warner Bros., including anthologies, trilogies, and crossovers. The initial statement in the context specifies \"nine theatrical live-action films,\" which likely refers to the main feature films, excluding serials or TV-related content. However, the question uses \"movies,\" which could be interpreted broadly, so I'll clarify in the answer to avoid confusion.\n",
      "\n",
      "Step-by-step:\n",
      "1. Extract key information: The context confirms nine theatrical live-action films as the core count.\n",
      "2. Consider broader interpretations: Batman appears in additional films like crossovers (e.g., Batman v Superman), but the context doesn't provide an exact total beyond the initial statement.\n",
      "3. Anticipate student needs: The student has shown interest in Batman's personal aspects (from the previous summary), so I'll make the answer cooperative by suggesting related explorations, like how different portrayals affect his character.\n",
      "4. Ensure the answer is helpful and engaging, avoiding definitive claims that might be outdated.\n",
      "5. For the updated summary, I'll incorporate this new question into the previous one, focusing on the student's evolving interests in relationships, backstory, and now media representations, without detailing answers.\n",
      "Updated Summary: The student is asking about the number of movies Batman is in, building on previous questions about his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, media representations, backstory elements, crime-fighting strategies, and moral complexities in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 6:\n",
      "Q: Is Batman a superhero?\n",
      "Predicted Answer: Yes, Batman is indeed a superhero! He's one of the most iconic figures in the DC Comics universe, known for his dedication to justice in Gotham City. Unlike characters with superpowers, Batman relies on his intelligence, martial arts skills, and advanced technology, which makes him a fascinating example of a human-driven hero. Since you've been asking about his friends, origin story, and media appearances, you might be interested in how his \"superhero\" status influences his relationships with allies like the Justice League or his portrayal in moviesfeel free to ask more about that!\n",
      "Gold Answer: Yes, Batman is a superhero. He was created by Bob Kane and Bill Finger. He first appeared in Detective Comics #27 in 1939,\n",
      "Reasoning: First, the student's question is \"Is Batman a superhero?\", which is straightforward and directly related to the topic of Batman. From the context provided, Batman is explicitly described as a superhero multiple times, including details about his creation, abilities, and role in fighting crime in Gotham City. This confirms that the answer should be affirmative based on the retrieved information.\n",
      "\n",
      "Next, I need to make the answer cooperative and anticipate the student's needs. The previous summary indicates the student has been exploring various aspects of Batman, such as his movies, friends, effectiveness, origin story, real name, and secret identities. This suggests an ongoing interest in superhero lore, so I should craft an answer that not only addresses the question but also connects to their broader curiosity, perhaps by highlighting Batman's unique traits (like lacking superpowers) and inviting further questions to keep the conversation engaging.\n",
      "\n",
      "For the updated summary, I must focus solely on the student's questions and interests, without referencing any answers or model-generated content. I'll build on the previous summary by incorporating this new question, emphasizing how it fits into their pattern of inquiry about Batman's character, relationships, and representations.\n",
      "\n",
      "Step-by-step reasoning:\n",
      "1. Verify the question against the context: The context clearly states Batman is a superhero, so the response should affirm this.\n",
      "2. Analyze previous summary: The student is building a comprehensive interest in Batman, so link the answer to themes like character definitions and media aspects.\n",
      "3. Ensure answer is cooperative: Make it educational, positive, and forward-looking to encourage learning.\n",
      "4. Update summary: Add the new question to the existing summary, focusing on interests like character classification, while keeping it under 1200 characters.\n",
      "Updated Summary: The student is asking about whether Batman is a superhero, building on previous questions about the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, media representations, backstory elements, crime-fighting strategies, moral complexities, and core classifications in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 7:\n",
      "Q: Who is his arch enemy?\n",
      "Predicted Answer: Batman's most iconic arch enemy is the Joker, a chaotic and unpredictable villain who embodies the opposite of Batman's moral code. He's often depicted as Batman's nemesis in comics, movies, and games, challenging him through psychological battles and crimes that test Batman's limits. If you're curious about why the Joker is so central to Batman's story or want to explore other villains like those from the Secret Society, I can dive deeperlet me know!\n",
      "Gold Answer: Batman's biggest enemy is The Joker. He is a clown themed criminal who is focused on inflicting chaos in Gotham's streets. He is seen as jovial and always changing.\n",
      "Reasoning: First, the question is \"Who is his arch enemy?\" in the context of the topic \"Batman.\" From the provided context, various villains are mentioned, such as the Secret Society of Super Villains, Lex Luthor, the Joker, and Psycho-Pirate, who are all associated with Batman or DC Comics. The Joker is explicitly referenced in the plot summary of Lego DC Super-Villains as a key antagonist, and based on general DC Comics knowledge, the Joker is widely recognized as Batman's primary arch enemy due to their iconic rivalry. \n",
      "\n",
      "To provide a cooperative answer, I need to be helpful and engaging, anticipating the student's interests in Batman's relationships and dynamics (as indicated in the previous summary). I'll confirm the arch enemy, offer a brief explanation to add value, and suggest potential follow-ups, like exploring other villains or the nature of their conflicts, to keep the conversation flowing.\n",
      "\n",
      "For the updated summary, I'll focus solely on the student's questions and evolving interests (e.g., adding this query about arch enemies) without referencing my answer, as per the guidelines. The previous summary already covers interests in relationships and dynamics, so I'll integrate this new question seamlessly.\n",
      "\n",
      "Step-by-step plan:\n",
      "1. State the arch enemy clearly.\n",
      "2. Provide context to make it educational and cooperative.\n",
      "3. Update the summary by appending the new question and refining the interests list.\n",
      "4. Ensure the summary stays under 1200 characters and avoids answer details.\n",
      "Updated Summary: The student is asking about Batman's arch enemy, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This suggests an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, and core classifications in DC Comics.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 8:\n",
      "Q: Is Batman stronger than the joker?\n",
      "Predicted Answer: In the world of DC Comics, Batman is generally depicted as stronger than the Joker in terms of physical strength and combat skills. Batman has undergone intense training to achieve peak human condition, making him a master martial artist with exceptional strength, agility, and endurance. The Joker, on the other hand, relies more on his genius intellect, deadly gadgets, and unpredictable tactics rather than raw physical powerhe's a clever villain who uses deception and weapons to challenge Batman. However, \"strength\" can depend on the story; the Joker often outsmarts opponents, which makes him a dangerous foe. If you're curious about this, we could explore more about Batman's training routines or the Joker's schemes, or even how their rivalry affects Gotham's dynamics, based on your interest in superhero relationships and adversaries!\n",
      "Gold Answer: Yes, Batman is stronger than the joker. Although, the joker has been known to cause much havoc in Batman's life including paralyzing Barbara Gordon and killing Jason Todd, Robin. \n",
      "Reasoning: First, the question asks whether Batman is stronger than the Joker, which involves comparing their physical and overall capabilities. From the context provided, the Joker is described as having abilities like genius intellect, brilliant chemistry skills, an arsenal of weapons, deception, expert hand-to-hand combat, and pain resistance, making him a formidable adversary through cunning and gadgets rather than raw physical power. Batman, based on general knowledge of the character, is a highly trained human with peak physical condition, martial arts expertise, and strategic mind, but without superhuman abilities. In DC Comics lore, Batman is typically portrayed as superior in direct physical confrontations due to his rigorous training and discipline, while the Joker excels in psychological warfare and trickery. To answer cooperatively, I need to address the comparison directly, explain it simply for a student's level, and anticipate further interests by linking to themes like adversaries and character dynamics from the previous summary. Step-by-step: 1) Analyze the question as a comparison of strengths. 2) Use context to highlight Joker's attributes. 3) Draw from Batman's established traits to contrast. 4) Provide a balanced, engaging answer that encourages learning. 5) Update the summary to include this new question on character abilities, aligning with the student's growing interest in adversaries and comparisons.\n",
      "Updated Summary: The student is asking about a direct strength comparison between Batman and the Joker, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, and secret identities. This indicates an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, core classifications in DC Comics, and now specific ability comparisons like physical strength.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question 9:\n",
      "Q: What is Batmans favourite gadget?\n",
      "Predicted Answer: That's a great question about Batman's gearit's fascinating how his gadgets play a big role in his adventures! Based on the information from various Batman portrayals, the Grapple Gun (or Grappling Hook Pistol) stands out as one of his most iconic and frequently used tools. It's described as a hi-tech device that helps him navigate cities quickly, pull objects, or even engage in combat, making it essential for his crime-fighting style. While Batman uses many gadgets, this one often appears in comics, films, and animations, suggesting it's a favorite for its versatility. If you're curious about why he relies on it or how it compares to other tools, or even want to explore more about his tech in specific stories, feel free to askI'm here to dive deeper into Batman's world with you!\n",
      "Gold Answer: The gadget that Batman most often uses is a Batarang, which is a small bat shaped throwing weapon. These were based on Australian boomerangs. He keeps the batarangs and other gadgets in his yellow utility belt.\n",
      "Reasoning: First, the student's question is about Batman's favorite gadget, which builds on their ongoing interest in Batman's character details, as seen in the previous summary. The context provided focuses heavily on the Grappling Hook Pistol (also called the Grapple Gun), describing it as a key tool used by Batman for mobility, offense, and various missions, making it one of his most prominent gadgets across comics, films, and other media. While the context doesn't explicitly state Batman's \"favorite\" gadget, the Grapple Gun is highlighted as constantly used by Batman and the Batman Family, suggesting it's a central element. I need to provide a cooperative answer that is helpful, draws from the context, and anticipates further questions, such as why certain gadgets are important or how they relate to his crime-fighting strategies. Step 2: Acknowledge the student's curiosity and tie it to their broader interests in Batman's abilities, relationships, and media representations to keep the response engaging. Step 3: For the updated summary, I must focus only on the student's questions and interests (e.g., adding this gadget inquiry to their existing pattern), without referencing my answer, to maintain accuracy and avoid potential errors. Step 4: Ensure the summary remains concise, under 1200 characters, by integrating the new question into the previous one. Finally, structure the output as required.\n",
      "Updated Summary: The student is asking about Batman's favorite gadget, building on previous questions about whether Batman is a superhero, the number of movies he is in, his friends, effectiveness as a crimefighter, origin story, real name, secret identities, and a direct strength comparison with the Joker. This indicates an expanding interest in superhero characters' personal relationships, social dynamics, key adversaries, media representations, backstory elements, crime-fighting strategies, moral complexities, core classifications in DC Comics, specific ability comparisons like physical strength, and now equipment and gadgets used in his operations.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Conversation complete! Processed 9 questions.\n"
     ]
    }
   ],
   "source": [
    "# We'll do an evaluation of a single conversation to check the model's validity.\n",
    "\n",
    "conversation = conversations[9] \n",
    "\n",
    "print(f\"Testing conversation on topic: '{conversation['topic']}'\")\n",
    "print(f\"Number of questions: {len(conversation['qas'])}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, qa_pair in enumerate(conversation['qas']):\n",
    "    question = qa_pair['q']\n",
    "    gold_answer = qa_pair['a']\n",
    "    \n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    \n",
    "    try:\n",
    "        prediction = conv_model.process_question(\n",
    "            question=question,\n",
    "            topic=conversation['topic'],\n",
    "            community=conversation['community']\n",
    "        )\n",
    "        \n",
    "        predicted_answer = prediction.answer\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print(f\"Gold Answer: {gold_answer}\")\n",
    "        \n",
    "        print(f\"Reasoning: {prediction.reasoning}\")\n",
    "        print(f\"Updated Summary: {prediction.updated_summary}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {str(e)}\")\n",
    "        predicted_answer = \"[ERROR]\"\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print(f\"Gold Answer: {gold_answer}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nConversation complete! Processed {len(conversation['qas'])} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8e257",
   "metadata": {},
   "source": [
    "#### Two main issues can be spotted:\n",
    "\n",
    "1) The model's answers and reasoning are both too long on average. this doesn't really matter in regards to reasoning, but we should try to keep the predicted answers shorter on average, as they are seemingly twice to thrice the gold answers' length.\n",
    "\n",
    "2) The model isn't actually updating its summary to include the entire conversation - the summary is only about the latest question.\n",
    "\n",
    "Both issues stem from the prompt, and so I've adjusted the prompt (and the field descriptions) to hopefully improve on these problems. Now, we'll proceed with the prompt optimization.\n",
    "\n",
    "For the optimization, I've elected to go with using the F1 score on a question-by-question basis, instead of scores over a full conversation. The main reason is honestly because it's the simplest metric to implement, and it's also likely to be the one to produce the best overall results. A metric that looks at entire conversations might help at engineering a prompt that results in better summaries, but it is also likely that it will overly-emphasize the historical context of the conversation, and cause the model to underperform when given questions that are somewhat \"out of left field\", so to speak. \n",
    "\n",
    "All in all, the conversation-based metric potential advantages over a regular question-based metric are nebulous at best, and harder to implement besides. This line of thought is what also dissuaded me from other similar metrics.\n",
    "\n",
    "This also makes it so this optimizer actually optimizes for the 'first questions', since it won't have the carryover summary because we'll be evaluating on a per-question basis (I am optimizing the QAModel, not the full ConversationModel which handles the summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "filepath = '../PragmatiCQA/data/train.jsonl'\n",
    "\n",
    "train_conversations = []\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_conversations.append(json.loads(line))\n",
    "\n",
    "train_examples = train_conversations[:20]\n",
    "\n",
    "val_examples = conversations[10:20]\n",
    "\n",
    "def conversation_to_dspy_examples(conversation_dict):\n",
    "    examples = []\n",
    "    \n",
    "    for qa_pair in conversation_dict['qas']:\n",
    "        question = qa_pair['q']\n",
    "        gold_answer = qa_pair['a']\n",
    "        \n",
    "        example = dspy.Example(\n",
    "            question=question,\n",
    "            topic=conversation_dict['topic'],\n",
    "            context=\"\", \n",
    "            previous_summary=\"New conversation starting\", \n",
    "            gold_answer=gold_answer\n",
    "        ).with_inputs('question', 'topic', 'context', 'previous_summary')\n",
    "        \n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "def qa_f1_metric(example, pred, trace=None):\n",
    "    \"\"\"F1 metric function using SemanticF1 evaluator\"\"\"\n",
    "    \n",
    "    gold_ex = dspy.Example(\n",
    "        question=example.question,\n",
    "        response=example.gold_answer \n",
    "    ).with_inputs('question')\n",
    "\n",
    "    pred_ex = dspy.Example(response=pred.answer)\n",
    "    \n",
    "    semantic_f1 = SemanticF1()\n",
    "    score = semantic_f1(gold_ex, pred_ex)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def train_qa_optimizer():\n",
    "    \n",
    "    train_dspy_examples = []\n",
    "    for conv in train_examples:\n",
    "        examples = conversation_to_dspy_examples(conv)\n",
    "        train_dspy_examples.extend(examples)\n",
    "    \n",
    "    val_dspy_examples = []\n",
    "    for conv in val_examples:\n",
    "        examples = conversation_to_dspy_examples(conv)\n",
    "        val_dspy_examples.extend(examples)\n",
    "    \n",
    "    optimizer = MIPROv2(\n",
    "        metric=qa_f1_metric,\n",
    "        max_bootstrapped_demos=3,\n",
    "        init_temperature=1.0,\n",
    "        track_stats=True,\n",
    "    )\n",
    "    \n",
    "    predictor = QAModel()\n",
    "    \n",
    "    compiled_predictor = optimizer.compile(\n",
    "        predictor,\n",
    "        trainset=train_dspy_examples,\n",
    "        valset=val_dspy_examples,\n",
    "        requires_permission_to_run=False\n",
    "    )\n",
    "    \n",
    "    return compiled_predictor\n",
    "\n",
    "compiled_qa_model = train_qa_optimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056804f7",
   "metadata": {},
   "source": [
    "The optimizer had finished, but it seems like it failed to actually improve upon my original prompt. I'm not sure if this is because I gave it too few trials, or if my prompt was originally pretty good (Though the scores are still quite low, an average of roughly 0.3 on the F1 score).\n",
    "\n",
    "Either way, I'll simply be proceeding with the model as-is, I don't see a reason to spend many more tokens on what's likely to result in minimal improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24c57c",
   "metadata": {},
   "source": [
    "Now, we'll continue with the evaluation on the first questions of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ca977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_first_questions(qa_model, questions, num_threads=4):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    devset = [\n",
    "        dspy.Example(\n",
    "            question=q['question'],\n",
    "            topic=q['topic'],\n",
    "            context=\"\",  \n",
    "            previous_summary=\"New conversation starting\",\n",
    "            gold_answer=q['gold_answer']\n",
    "        ).with_inputs('question', 'topic', 'context', 'previous_summary')\n",
    "        for q in questions\n",
    "    ]\n",
    "    \n",
    "    def qa_f1_metric(example, pred, trace=None):\n",
    "        gold_ex = dspy.Example(\n",
    "            question=example.question,\n",
    "            response=example.gold_answer\n",
    "        ).with_inputs('question')\n",
    "        \n",
    "        pred_ex = dspy.Example(response=pred.answer)\n",
    "        \n",
    "        semantic_f1 = dspy.evaluate.SemanticF1()\n",
    "        return semantic_f1(gold_ex, pred_ex)\n",
    "    \n",
    "    evaluator = Evaluate(\n",
    "        devset=devset,\n",
    "        metric=qa_f1_metric,\n",
    "        display_progress=True,\n",
    "        display_table=True,\n",
    "        num_threads=num_threads\n",
    "    )\n",
    "    \n",
    "    return evaluator(qa_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1438ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58.78 / 174 (33.8%): 100%|| 174/174 [00:00<00:00, 1189.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 15:01:37 INFO dspy.evaluate.evaluate: Average Metric: 58.78399556618422 / 174 (33.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>topic</th>\n",
       "      <th>context</th>\n",
       "      <th>previous_summary</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>answer</th>\n",
       "      <th>updated_summary</th>\n",
       "      <th>qa_f1_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is the Batman comic similar to the movies?</td>\n",
       "      <td>Batman</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>I would say the movie and comics has same story line, as Batmans p...</td>\n",
       "      <td>Using general knowledge of Batman since no specific context is pro...</td>\n",
       "      <td>Batman comics and movies share core elements like the character's ...</td>\n",
       "      <td>Previously, this was a new conversation starting on the topic of B...</td>\n",
       "      <td> [0.286]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is batman's real name?</td>\n",
       "      <td>Batman</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>Batman was created by Bob Kane and Bill Finger. His real identity ...</td>\n",
       "      <td>Based on general knowledge from DC Comics, as no specific context ...</td>\n",
       "      <td>Batman's real name is Bruce Wayne, a billionaire philanthropist wh...</td>\n",
       "      <td>This conversation is just starting, with the student asking about ...</td>\n",
       "      <td> [0.286]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How old was batman when he first became batman?</td>\n",
       "      <td>Batman</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>I don't know. It is not clear when Bruce Wayne becomes Batman, but...</td>\n",
       "      <td>Since the provided context is empty, I am relying on general knowl...</td>\n",
       "      <td>Batman, or Bruce Wayne, first became Batman at the age of 25 after...</td>\n",
       "      <td>The conversation started as a new discussion on Batman. The studen...</td>\n",
       "      <td> [0.286]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does Batman Have super powers, like invisibility, or the ability t...</td>\n",
       "      <td>Batman</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>No, Batman has no super powers like other super heroes because he ...</td>\n",
       "      <td>Based on established Batman lore from DC Comics, he is depicted as...</td>\n",
       "      <td>No, Batman does not have superpowers like invisibility or the abil...</td>\n",
       "      <td>This conversation began with a new query about whether Batman has ...</td>\n",
       "      <td> [0.615]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who are Batman's biggest enemies?</td>\n",
       "      <td>Batman</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>The Joker and Catwoman are original enemies of Batman. However, th...</td>\n",
       "      <td>Based on general knowledge of Batman lore from comics and media, a...</td>\n",
       "      <td>Batman's biggest enemies include the Joker, his chaotic arch-nemes...</td>\n",
       "      <td>Previous summary: New conversation starting. The student asked abo...</td>\n",
       "      <td> [0.250]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Who creat the game of thrones universe?</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>Game of Thrones was produced by the HBO cable network. It is based...</td>\n",
       "      <td>Based on general knowledge of the Game of Thrones series, the crea...</td>\n",
       "      <td>The Game of Thrones universe was created by author George R.R. Mar...</td>\n",
       "      <td>This conversation began with a new query about the creator of the ...</td>\n",
       "      <td> [0.571]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Where was the Game of Thrones shot?</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>The pilot episode was filmed in Northern Ireland and Morocco. This...</td>\n",
       "      <td>Since the provided context is empty, I am drawing from general kno...</td>\n",
       "      <td>Game of Thrones was primarily filmed in Northern Ireland, with add...</td>\n",
       "      <td>New conversation starting. The student asked about the filming loc...</td>\n",
       "      <td> [0.444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>who is the protagonist of the show?</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>Super tough one, many different episodes focus on different protag...</td>\n",
       "      <td>Game of Thrones features an ensemble cast without a single clear p...</td>\n",
       "      <td>In Game of Thrones, there isn't one definitive protagonist due to ...</td>\n",
       "      <td>This conversation is starting on Game of Thrones. The student aske...</td>\n",
       "      <td> [0.286]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>when was the firs series released?</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>It feels crazy that it's this long ago, but the first season aired...</td>\n",
       "      <td>Based on general knowledge of the Game of Thrones series, I can co...</td>\n",
       "      <td>The first season of Game of Thrones was released on April 17, 2011...</td>\n",
       "      <td>This conversation is about Game of Thrones, starting with the stud...</td>\n",
       "      <td> [0.284]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>What is Game of thrones its real or not?</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td></td>\n",
       "      <td>New conversation starting</td>\n",
       "      <td>It is based on the novel series A Song of Ice and Fire</td>\n",
       "      <td>Based on general knowledge of the topic, Game of Thrones is a fict...</td>\n",
       "      <td>Game of Thrones is a fictional television series created by David ...</td>\n",
       "      <td>This conversation is just starting, with the student asking if Gam...</td>\n",
       "      <td> [0.333]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  question  \\\n",
       "0                               Is the Batman comic similar to the movies?   \n",
       "1                                              what is batman's real name?   \n",
       "2                          How old was batman when he first became batman?   \n",
       "3    Does Batman Have super powers, like invisibility, or the ability t...   \n",
       "4                                        Who are Batman's biggest enemies?   \n",
       "..                                                                     ...   \n",
       "169                               Who creat the game of thrones universe?    \n",
       "170                                    Where was the Game of Thrones shot?   \n",
       "171                                   who is the protagonist of the show?    \n",
       "172                                     when was the firs series released?   \n",
       "173                               What is Game of thrones its real or not?   \n",
       "\n",
       "               topic context           previous_summary  \\\n",
       "0             Batman          New conversation starting   \n",
       "1             Batman          New conversation starting   \n",
       "2             Batman          New conversation starting   \n",
       "3             Batman          New conversation starting   \n",
       "4             Batman          New conversation starting   \n",
       "..               ...     ...                        ...   \n",
       "169  Game of Thrones          New conversation starting   \n",
       "170  Game of Thrones          New conversation starting   \n",
       "171  Game of Thrones          New conversation starting   \n",
       "172  Game of Thrones          New conversation starting   \n",
       "173  Game of Thrones          New conversation starting   \n",
       "\n",
       "                                                               gold_answer  \\\n",
       "0    I would say the movie and comics has same story line, as Batmans p...   \n",
       "1    Batman was created by Bob Kane and Bill Finger. His real identity ...   \n",
       "2    I don't know. It is not clear when Bruce Wayne becomes Batman, but...   \n",
       "3    No, Batman has no super powers like other super heroes because he ...   \n",
       "4    The Joker and Catwoman are original enemies of Batman. However, th...   \n",
       "..                                                                     ...   \n",
       "169  Game of Thrones was produced by the HBO cable network. It is based...   \n",
       "170  The pilot episode was filmed in Northern Ireland and Morocco. This...   \n",
       "171  Super tough one, many different episodes focus on different protag...   \n",
       "172  It feels crazy that it's this long ago, but the first season aired...   \n",
       "173                 It is based on the novel series A Song of Ice and Fire   \n",
       "\n",
       "                                                                 reasoning  \\\n",
       "0    Using general knowledge of Batman since no specific context is pro...   \n",
       "1    Based on general knowledge from DC Comics, as no specific context ...   \n",
       "2    Since the provided context is empty, I am relying on general knowl...   \n",
       "3    Based on established Batman lore from DC Comics, he is depicted as...   \n",
       "4    Based on general knowledge of Batman lore from comics and media, a...   \n",
       "..                                                                     ...   \n",
       "169  Based on general knowledge of the Game of Thrones series, the crea...   \n",
       "170  Since the provided context is empty, I am drawing from general kno...   \n",
       "171  Game of Thrones features an ensemble cast without a single clear p...   \n",
       "172  Based on general knowledge of the Game of Thrones series, I can co...   \n",
       "173  Based on general knowledge of the topic, Game of Thrones is a fict...   \n",
       "\n",
       "                                                                    answer  \\\n",
       "0    Batman comics and movies share core elements like the character's ...   \n",
       "1    Batman's real name is Bruce Wayne, a billionaire philanthropist wh...   \n",
       "2    Batman, or Bruce Wayne, first became Batman at the age of 25 after...   \n",
       "3    No, Batman does not have superpowers like invisibility or the abil...   \n",
       "4    Batman's biggest enemies include the Joker, his chaotic arch-nemes...   \n",
       "..                                                                     ...   \n",
       "169  The Game of Thrones universe was created by author George R.R. Mar...   \n",
       "170  Game of Thrones was primarily filmed in Northern Ireland, with add...   \n",
       "171  In Game of Thrones, there isn't one definitive protagonist due to ...   \n",
       "172  The first season of Game of Thrones was released on April 17, 2011...   \n",
       "173  Game of Thrones is a fictional television series created by David ...   \n",
       "\n",
       "                                                           updated_summary  \\\n",
       "0    Previously, this was a new conversation starting on the topic of B...   \n",
       "1    This conversation is just starting, with the student asking about ...   \n",
       "2    The conversation started as a new discussion on Batman. The studen...   \n",
       "3    This conversation began with a new query about whether Batman has ...   \n",
       "4    Previous summary: New conversation starting. The student asked abo...   \n",
       "..                                                                     ...   \n",
       "169  This conversation began with a new query about the creator of the ...   \n",
       "170  New conversation starting. The student asked about the filming loc...   \n",
       "171  This conversation is starting on Game of Thrones. The student aske...   \n",
       "172  This conversation is about Game of Thrones, starting with the stud...   \n",
       "173  This conversation is just starting, with the student asking if Gam...   \n",
       "\n",
       "    qa_f1_metric  \n",
       "0      [0.286]  \n",
       "1      [0.286]  \n",
       "2      [0.286]  \n",
       "3      [0.615]  \n",
       "4      [0.250]  \n",
       "..           ...  \n",
       "169    [0.571]  \n",
       "170    [0.444]  \n",
       "171    [0.286]  \n",
       "172    [0.284]  \n",
       "173    [0.333]  \n",
       "\n",
       "[174 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_f1_first = evaluate_first_questions(compiled_qa_model, questions, num_threads=4)\n",
    "# i ran the evaluation once before while trying to make the table somewhat more presentable (less truncated), and i couldn't really get it to work well;\n",
    "# i either removed the truncation entirely, and also kept the string truncation on a column-basis in the table.. this thing is quite a pain to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f8265",
   "metadata": {},
   "source": [
    "Comparison to the traditional model will be written below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5048758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "def evaluate_single_conversation(conv_idx, conversation, compiled_qa_model):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    retriever = Retriever()\n",
    "    conv_model = ConversationModel(compiled_qa_model, retriever)\n",
    "    \n",
    "    pred_conversation = copy.deepcopy(conversation)\n",
    "    \n",
    "    semantic_f1 = dspy.evaluate.SemanticF1()\n",
    "    conversation_scores = []\n",
    "    \n",
    "    conv_model.reset_conversation()\n",
    "    \n",
    "    for qa_idx, qa_pair in enumerate(pred_conversation['qas']):\n",
    "        try:\n",
    "            prediction = conv_model.process_question(\n",
    "                question=qa_pair['q'],\n",
    "                topic=pred_conversation['topic'],\n",
    "                community=pred_conversation.get('community', '')\n",
    "            )\n",
    "            \n",
    "            gold_ex = dspy.Example(\n",
    "                question=qa_pair['q'],\n",
    "                response=qa_pair['a']\n",
    "            ).with_inputs('question')\n",
    "            \n",
    "            pred_ex = dspy.Example(response=prediction.answer)\n",
    "            score = semantic_f1(gold_ex, pred_ex)\n",
    "            \n",
    "            qa_pair['predicted_answer'] = prediction.answer\n",
    "            qa_pair['reasoning'] = prediction.reasoning\n",
    "            qa_pair['updated_summary'] = prediction.updated_summary\n",
    "            qa_pair['f1_score'] = score\n",
    "            \n",
    "            conversation_scores.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            qa_pair['predicted_answer'] = f\"[ERROR: {str(e)}]\"\n",
    "            qa_pair['reasoning'] = \"[ERROR]\"\n",
    "            qa_pair['updated_summary'] = \"[ERROR]\"\n",
    "            qa_pair['f1_score'] = 0.0\n",
    "            conversation_scores.append(0.0)\n",
    "    \n",
    "    return conv_idx, pred_conversation, conversation_scores\n",
    "\n",
    "def evaluate_full_conversations(qa_model, conversations, num_threads=4):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    pred_conversations = [None] * len(conversations)\n",
    "    all_scores = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        future_to_conv = {executor.submit(evaluate_single_conversation, i, conv, qa_model): i \n",
    "                         for i, conv in enumerate(conversations)}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_conv), \n",
    "                          total=len(conversations), \n",
    "                          desc=\"Evaluating conversations\"):\n",
    "            conv_idx = future_to_conv[future]\n",
    "            try:\n",
    "                conv_idx, pred_conversation, conversation_scores = future.result()\n",
    "                pred_conversations[conv_idx] = pred_conversation\n",
    "                all_scores.extend(conversation_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing conversation {conv_idx}: {str(e)}\")\n",
    "                pred_conversations[conv_idx] = copy.deepcopy(conversations[conv_idx])\n",
    "    \n",
    "    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "    \n",
    "    return avg_score, pred_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed38b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating conversations: 100%|| 179/179 [2:09:59<00:00, 43.57s/it]   \n"
     ]
    }
   ],
   "source": [
    "avg_f1_conv, eval_results_conv = evaluate_full_conversations(qa_model, conversations)\n",
    "\n",
    "with open('llm_conv_eval.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for conversation in eval_results_conv:\n",
    "        json.dump(conversation, f, indent=2, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_conversations = []\n",
    "\n",
    "with open('llm_conv_eval.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pred_conversations = json.loads(line.strip())\n",
    "            pred_conversations.append(conversation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8bacdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== AVERAGE F1 SCORE ====================\n",
      "SCORE:  0.2727486136325308\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20, \"AVERAGE F1 SCORE\", \"=\"*20)\n",
    "print(\"SCORE: \", avg_f1_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca5601",
   "metadata": {},
   "source": [
    "## 5. Discussion Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a3a49",
   "metadata": {},
   "source": [
    "#### Let's first address the gap between the average score for the first questions and the average score of the full conversations - 0.338 vs 0.272."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1741b",
   "metadata": {},
   "source": [
    "In a way, this result is expected; In line with standard conversations humans have, the longer it goes on, the more context the conversation itself holds beyond what's being directly stated at that very moment. In regards to this dataset, this means that the later questions within each conversation are essentially \"saddled\" with prior context that shapes the subsequent responses of either party, whether it be the teacher or the student (Though in our case, we care more about emulating the teacher's responses given the student's). \n",
    "\n",
    "This issue makes the task of emulating a cooperative answer progressively harder, and it is expected that the resulting score would also be progressively lower.\n",
    "\n",
    "Let's look at a bit of an extreme example of this:\n",
    "\n",
    "- \"q\": \"tell me about season 1 - 4?\",\n",
    "\n",
    "- \"a\": \"As of 2016, five books have been published in the A Song of Ice and Fire series, and author George R.R. Martin has predicted that there will be two more (though he is struggling not to let the final book run long, in which case it would have to be split, for a total of eight books).  The third novel, A Storm of Swords , was so long that it pushed the limits of how large a published book could physically be without pages falling out. Because the third novel was so long, the production team realiz\",\n",
    "\n",
    "- \"predicted_answer\": \"Season 1 of Game of Thrones introduces the complex political intrigue in Westeros, focusing on the Stark and Lannister families, the fight for the Iron Throne, and Daenerys Targaryen's beginnings in Essos. Seasons 2 through 4 escalate the conflicts with the War of the Five Kings, key battles, betrayals, and Daenerys's conquests, building toward major character developments that set up the rest of the seriesyou might also want to explore how these seasons influence later storylines in Seasons 5 and beyond.\",\n",
    "\n",
    "- \"updated_summary\": \"This conversation began with the student asking if Game of Thrones is real, and I explained it's a fictional TV series based on George R.R. Martin's books, highlighting its fantasy elements. They then inquired about the number of seasons and episodes (8 seasons with varying counts), suggested resources like streaming platforms, and asked about the cast size, describing the large ensemble and recommending IMDb. For Peter Dinklage's character, I confirmed he played Tyrion Lannister, a clever nobleman central to the plot, and for Lord Tywin Lannister, I described him as the powerful head of House Lannister with Andal culture, suggesting related episodes and comparisons to other cultures. Most recently, the student asked about Essos, which I defined as a vast continent east of Westeros with diverse regions and cultural influences, and noted its relevance to character interactions. Now, the student asked about Seasons 1-4, which I summarized as introducing political intrigue and escalating \",\n",
    "\n",
    "The score for this predicted answer was a surprising 0, despite it actually answering the question in a literal manner, while providing an avenue for further discussion by tying in the later seasons. By all means, if taken out of context of the entire conversation, this is a pretty good answer. So why did it get a score of 0?\n",
    "\n",
    "Looking at the gold answer (which was sadly kind of cut off for some reason), it's quite obvious that the topic of the conversation had veered into the realm of the Song of Ice and Fire books (Unless the teacher decided to take a rather sharp turn in discussion and mention the books out of nowhere), which is why the answer focused on the books. By the end, when the production team was mentioned, the answer was cut off, meaning the evaluator could find no real mention of the TV series in the ground truth, and thus found very little relation between it and the predicted answer, giving it an understandable score of 0.\n",
    "\n",
    "Admittedly, this is kind of my own fault, as I'd decided to NOT forward context to the evaluator model. I don't know if it would've made a substantial difference in scoring, but logically speaking, forwarding the context would've allowed the evaluator to avoid this pitfall.\n",
    "\n",
    "However, this also justifies what I've explained above; as the conversation progresses, the topic at hand diverges from the original, and the questions become more 'loaded' than they literally are. A language model, without sufficient context, is simply ill-equipped to handle that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de706b",
   "metadata": {},
   "source": [
    "#### Comparing to the traditional model\n",
    "\n",
    "Compared to the DISTILBERT model, our RAG score with the LLM model is significantly higher - an average F1 score of 0.338.\n",
    "\n",
    "This score approaches the scores achieved by DISTILBERT using the given information spans for literal (0.4) and pragmatic (0.37) answers. The closeness in score indicates that the information the LLM worked with and supplied in its answers is not too far apart from what the spans had. Meaning, the retriever was likely successful in finding relevant information, and the LLM was successful at sieving through that information to find the information needed to answer the questions. \n",
    "\n",
    "The semantic evaluation is a bit iffy, since it's hard for me to tell if the score is low because the answer was strictly bad, or if it was a generally good answer, just very different from the gold answer - some gold answers can consist of some random anecdotes or strong opinions that a model isn't really supposed to be able to replicate. This means that a good answer by the model can be scored low because of this difference.\n",
    "\n",
    "#### So which approach is better?\n",
    "\n",
    "I think the answer to this question is pretty self evident. The LLM model performed much better using retrieved context (0.338 by the LLM vs ~0.11 by DISTILBERT), and while we do see that the DISTILBERT model had better scores on the first questions with the dataset context spans, it means practically nothing. Given that the traditional model only extracts spans that it finds relevant to the question, and it doesn't really do any word processing, all it can output are dry answers that are inherently uncooperative. There is no reason to believe it has advantages over the LLM beyond exceedingly niche use cases like \"I only want the model to output a succint, literal answer when I already have a short passage containing all the necessary information\" - if I already have the relevant context neatly summarized like so, I have very little need for a model to extract the answers, so it's not really a use case I bother considering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fbda4",
   "metadata": {},
   "source": [
    "### Theory of Mind\n",
    "\n",
    "So let's go over whether our model exhibits aspects of ToM. Immediately, I want to say 'no', however, let's examine this a bit more thoroughly - after all, a good part of the reason why I've decided to include the 'summary' field is because it really helps with following the model's so-called \"understanding\" of the conversation, and by extension, the student's intentions and beliefs. I'll take a few random examples (I'm just going to scroll through the jsonl file until I find something interesting; I'm not going to use some elaborate filtering scheme or anything of the sort) and try to analyze them:\n",
    "\n",
    "- \"updated_summary\": \"This conversation began with the student inquiring about Jo Frost's role in Supernanny, including techniques like the Naughty Bench for behavioral issues, episodes on tantrums and diets, and the show's global success, while expressing nostalgia and seeking streaming options. Discussions emphasized its practical parenting advice, such as the Naughty Step with age-based timeouts, and addressed effectiveness by recommending adaptations and positive reinforcement. In the current exchange, the student, a fan of the timeout chair, asked about other methods Jo Frost advocated, and the response highlighted the \\\"Go to Bed Early\\\" technique for bedtime issues while suggesting exploration of positive discipline strategies like reward charts.\",\n",
    "\n",
    "So, while it seems like the summary really is just a summary of the questions and subsequent answers by the model (not the gold answers), we can spot something really important: It described the student as a fan of the timeout chair! Presumably, the model inferred this through curisoity on behalf of the student in regards to the timeout chair, rather than explicit declaration of them being a fan of that timeout chair. I'm actually quite surprised I found something like this, so I will look back and see if the student actually made such a declaration.\n",
    "\n",
    "To my great disappointment, this was the question: \"q\": \"I am a big fan of the timeout chair.. Any other methods of parenting she advocated?\"\n",
    "\n",
    "So clearly, in this case, the model doesn't really exhibit any aspects of the ToM; it did not really attribute any mental state to the student by its own volition. Eather, it simply utilized the information supplied literally within the question. No inference seems to have happened, and there's nothing else that exhibits ToM  within the summary, in my opinion.\n",
    "\n",
    "Another example:\n",
    "\n",
    "- \"updated_summary\": \"In this ongoing discussion on Alexander Hamilton, the student previously asked about his significance as a Founding Father, roles like Secretary of the Treasury, influence on American finance, cultural impact through the musical \\\"Hamilton\\\" by Lin-Manuel Miranda, family life, and the duel with Aaron Burr, with follow-ups on the musical's hip-hop style and the American Revolution's role in its storyline. The response explained the Revolution as the 1775-1783 war central to Act One, focusing on Hamilton's rise and themes like ambition, and later noted Hamilton's death from duel wounds in 1804. Now, the student asked why Aaron Burr wished to kill Hamilton, with the answer highlighting Burr's motivations from Hamilton's public insults and political rivalries, as portrayed in the musical, potentially leading to questions about Burr's aftermath or dueling's societal effects.\"\n",
    "\n",
    "Once again, we do not actually see inference on the model's side regarding the student's potential interests, let alone something harder to grasp like their inclinations or how they seem to favor answers (Though without feedback, a model can't really determine the latter). This could once again be my fault, as I should've outlined the goals of the summary better, perhaps by using the very same keywords (Interests, inclinations) that I've mentioned here.\n",
    "\n",
    "Last example:\n",
    "\n",
    "- \"updated_summary\": \"This conversation about Popeye started with questions on the first episode's year (1978 for The All-New Popeye Hour, with origins in 1929 comics and 1930s shorts) and the character's creation from Elzie Crisler Segar's experiences. The student then asked about the idea's origin, which was linked to Segar's background, and now inquires if the cartoon referenced any controversial issues of the day. Based on available context, Popeye touched on themes like nutrition without major controversies, and this builds on the student's interest in Popeye's history, origins, and cultural influences, potentially leading to discussions on character development or societal themes.\",\n",
    "\n",
    "Surprisingly, this summary actually directly addresses the student's interest. Admittedly, this is a very surface level \"inference\", and I'm hesitant to consider this a proper example of ToM... \n",
    "\n",
    "Overall, I'd consider this \"sophisticated pattern-matching\" as you'd referred to it. It is very likely that the model added the line about interest after the student directly stated something along the lines of \"I'm interested to know if...\" in one of the earlier questions. However, we must take into account the possibility that my own prompt steered the model towards a more objective analysis / summarization of interarction, rather than one that allows inference to occur freely. I would not make an assumption based on my singular experiment. \n",
    "\n",
    "It would be much better to run this experiment in small batches with multiple prompt strategies to actually reach a satisfactory conclusion.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
