{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed15338",
   "metadata": {},
   "source": [
    "## Part 0: Dataset Analysis\n",
    "\n",
    "### Motivation, Contributions and Methodology\n",
    "\n",
    "Existing datasets in the open-domain QA, up until the release of PragmaticCQA, largely focused on evaluating QA systems' accuracy regarding the literal answers to given questions. They did not examine or evaluate a system's ability to infer the questioner's unmentioned needs from context; whether they be in the form of follow-up questions, or relevant information that the questioner is not even necessarily aware of, due to the lack of knowledge in the topic being questioned. This ability to grasp intent is key to efficient and productive conversations, and the fact that it so far went largely ignored by common metrics and evaluation datasets is what motivated the paper's authors to create this dataset, along with the corresponding metrics, to allow for a meaningful examination of this ability in NLP models. \n",
    "\n",
    "Specifically, what they've produced is:\n",
    "\n",
    "* A crowd-sourcing framework that achieves \"incentive alignment\". The authors claim that many of the recent datasets that are crowd-sourced suffer from this \"incentive misalignment\", which essentially means that annotators are rewarded for producing as many examples as they can, and so they create 'basic' examples that they can churn out quickly. These examples tend to lack nuance, or are often similar, and thus allow the model to learn surface-level patterns in order to achieve good results. This naturally goes against the intent of the dataset creators, as it does not truly test for a model's reasoning abilities, which is where the supposed \"incentive misalignment\" stems from.     \n",
    "The authors claim to have solved this issue by allowing the annotators to work on topics they're interested in, and having actually discuss these topics between themselves, which ends up increasing their engagement with the task and producing examples that feel natural and resemble standard human interaction better than other datasets.\n",
    "\n",
    "* An open-domain ConvQA dataset that follows the prior framework, and features pragmatic answers and metrics that allow for the evaluation of pragmatic reasoning.\n",
    "\n",
    "* An analysis of their dataset that shows that it presents a challenge to existing models, proving its relevance in the field.\n",
    "\n",
    "I will now focus on the third point, which is their analysis of the dataset, and why it proves to be challenging to current NLP nodels.\n",
    "\n",
    "The split datasets each contains separate topics, meaning there's no overlap between the topics and thus no overlap of information between questions of different topics. This forces the model to actually generalize and rely on its internal reasoning capabilities.\n",
    "\n",
    "The answers in the dataset tend to be constructed with information from different elements, substantially more so than other, commonly used datasets in the field. This proves to be quite hard for the NLP, as it requires it to collate information from a large number of different sources.\n",
    "\n",
    "The answers are also often formed of a combination of small factoids, and a larger narrative that ties these factoids together with the answer to the original questions. The model should be able to replicate this, and that is more complex than giving literal answers, such as labelling or providing a direct, literal answer to a question without further consideration.\n",
    "\n",
    "These aspects tell us the dataset seeks specific pragmatic phenomena:\n",
    "* The recognition of potential follow-up questions and the inclusion of their answers.\n",
    "* Being cooperative in the conversation: A model should attempt to keep the conversation flowing with the provided answers, whether it be by including relevant information that allows for further discussion, or other such methods that humans employ (Another one would be trying to return the question, or other follow up questions to the student after providing sufficient information, but I'm not sure if the dataset actually covers this case as well).\n",
    "* Being selective in providing information: A model shouldn't just provide a list of connected data, but rather consider the question, the context in which it's asked, like the background of the questioner, and providing relevant information based on these elements.\n",
    "\n",
    "These aspects all serve to complicate the task for NLP models, and thus challenge them.\n",
    "\n",
    "\n",
    "### Sample Analysis\n",
    "\n",
    "1) The topic is 'Vampires', with the starting question being \"So, what is a vampire, exactly?\"\n",
    "\n",
    "   The literal answer we'd expect from a non-cooperative teacher would be something along the lines of \"An undead monster\", which doesn't elaborate greatly on what differentiates vampires from the plethora of other undead monsters in various fictions, like zombies, or even ghosts. \n",
    "\n",
    "   The given answer in the dataset was as follows: \"Vampires are a kind of undead monster that feeds on the life essence of living creatures like humans.\"\n",
    "   This answer provides the full information we expect to see from a literal interpretation of the question - \"A kind of undead monster\", and further specifies that it 'feeds on the life essence of living creatures', prompting the student to equate them to beasts of prey, as they feed on other living beings. This lets the student distinguish between vampires and say, ghosts, that are depicted as malevolent, metaphysical beings that exist to haunt people.\n",
    "\n",
    "   * I've got to say that I don't actually like this answer since life essence is such a weird term. When has anyone ever seen a depiction of vampires that sustain themselves on something other than blood? just say blood...\n",
    "\n",
    "2) The topic is 'The Wheel of Time', with the starting question being \"who was the writer of the wheel of time?\"\n",
    "\n",
    "    The literal answer we'd expect from a non-cooperative teacher could simply state that the writer was Robert Jordan, as he is both an author and the one who wrote the majority of the books, as the student asked about a singular writer. However, it is known that Brandon Sanderson is the one who wrote the latter books, so we expect a pragmatic answer to include this fact, along with the reason why Brandon Sanderson ended up writing the last few books instead of Robert Jordan (Robert Jordan died).\n",
    "    \n",
    "\n",
    "    The given answer in the dataset was as follows: \"Robert Jordan is the author but he sadly passed away and his books were finished by Brandon Sanderson.\"\n",
    "    This answer is more pragmatic as we can easily see the added information as something necessary - Most people wouldn't think beforehand that a book series was written by more than one person, and they would default to asking about a singular writer or author. This is despite them actually wanting to know about all the potential writers, if there were indeed multiple writers. We expect this basic level of inference in daily conversation, and this answer provides that. The literal answer, however, does not.\n",
    "\n",
    "\n",
    "3) The topic is 'Cats Musical Wiki', with the starting question being \"I am a student and know nothing about cats musical wiki\".\n",
    "\n",
    "    This is an interesting 'question' as it's not phrased as a question, but rather, it is a simple statement when taken literally. If used to initiate a conversation, the other party would recognize this as a request to learn about the topic, or an attempt to make small talk by giving the teacher leeway to introduce tidbits of information of their own choosing to the conversation, thus steering it in their desired direction. \n",
    "\n",
    "    Surprisingly enough, both the literal and pragmatic answer spans do not even mention the 'wiki', but rather just talk about the cats musical itself.\n",
    "\n",
    "    The text provided by the dataset for the literal answer just includes details about the creator, the source material, and a range of dates and locations when and where it was played.\n",
    "\n",
    "    The answer was as follows: \"Cats was one of the longest running plays ever, starting in London and running for 21 years. I was lucky enough to sit in the audience in New York city for a performance once.\"\n",
    "\n",
    "    We can see that the teacher actually chose to share his own experience regarding the musical, despite not being prompted for such a thing; they actively chose to steer the conversation to talk about their own experience, and thus proving to be a cooperative conversationalist (And the conversation actually continued down that path, with questions like \"Where were you seated\" and so on), rather than simply providing basic details and closing off the conversation, like we'd expect from a literal answer by a non-cooperative teacher.\n",
    "\n",
    "4) The topic is 'Edward Elric', with the starting question being \"Who is Edward Elric?\".\n",
    "\n",
    "    So a literal answer to this question would be quite succint, such as \"A fictional alchemist in 'The Fullmetal Alchemist'\", \"The main protagonist of 'The Fullmetal Alchemist' series\" and so on. We'd expect a pragmatic answer to both combine these details, and then enrich the answer by giving context; sharing information about the character's traits or background.\n",
    "\n",
    "    The answer given does indeed fulfill these expectations: \"Edward Elric is the main protagonist of the Fullmetal Alchemist series. Edward lost hist right arm and left leg due to a failed Human Transplantation attempt and became the youngest State Alchemist in history at the age of twelve.\"\n",
    "\n",
    "    The answer includes further details than we'd expect from a purely literal interpretation, in line with what one would likely want to know when asking about a fictional character - like a background that hints at the character's motivation, and thus also the main plot of the story.\n",
    "\n",
    "I will stop here since I think this section is wordy enough already.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdaf54",
   "metadata": {},
   "source": [
    "## Part 1: The \"Traditional\" NLP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35ae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import dspy\n",
    "import numpy as np\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from dspy.evaluate import SemanticF1 #no longer necessary\n",
    "import configparser\n",
    "from dspy.evaluate.auto_evaluation import (\n",
    "    SemanticRecallPrecision,\n",
    "    DecompositionalSemanticRecallPrecision\n",
    ")\n",
    "from dspy.predict.chain_of_thought import ChainOfThought\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('grok_key.ini')\n",
    "api_key = config['DEFAULT']['XAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7689f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_questions(filepath='../PragmatiCQA/data/val.jsonl'):\n",
    "\n",
    "    questions = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            conv = json.loads(line)\n",
    "            first_qa = conv['qas'][0]\n",
    "\n",
    "            #the spans will only include the text strings, not the keys\n",
    "            literal_spans = []\n",
    "            pragmatic_spans = []\n",
    "\n",
    "            if 'literal_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['literal_obj']:\n",
    "                    literal_spans.append(span_obj['text'])\n",
    "                    \n",
    "            if 'pragmatic_obj' in first_qa['a_meta']:\n",
    "                for span_obj in first_qa['a_meta']['pragmatic_obj']:\n",
    "                    pragmatic_spans.append(span_obj['text'])\n",
    "\n",
    "            questions.append({\n",
    "            'question': first_qa['q'],\n",
    "            'gold_answer': first_qa['a'],\n",
    "            'topic': conv['topic'],\n",
    "            'genre': conv.get('genre', ''),\n",
    "            'community': conv.get('community', ''),\n",
    "            'literal_spans': literal_spans,\n",
    "            'pragmatic_spans': pragmatic_spans\n",
    "            })\n",
    "\n",
    "    return questions\n",
    "\n",
    "questions = get_first_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c94acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'who is freddy krueger?', 'gold_answer': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20N'], 'pragmatic_spans': ['Cannot GET /wiki/A%20N']}\n",
      "{'question': 'who was the star on this movie?', 'gold_answer': \"Robert Englund IS Freddy Kruger, the bad guy for these films. Note to you and to Adam, the Pragmatic one, the link here is broken and I can't paste relevant things, as has always been Nightmare's case, I'm perfectly good with answering your questions and will quickly do it, but have to open a tab in another window separate from the hit, I WILL go quickly and answer at rapid speed though, don't worry.\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'What is the movie about?', 'gold_answer': 'Ok, here goes, I\\'m getting \"Cannot get\"..so, Nightmare on Elm street centers around the fact that in your dreams, Freddie Kruger, a dark figure can chase you and if you are killed while sleeping you die.', 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html', 'Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Who directed the new film?', 'gold_answer': \"It was Directed by: Samuel Bayer. Note that the link here is broken. So I'm having to get some of this from memory. I copied what I have (this is ALL I have).\", 'topic': 'A Nightmare on Elm Street (2010 film)', 'genre': 'Movies', 'community': 'A Nightmare on Elm Street', 'literal_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html'], 'pragmatic_spans': ['Cannot GET /wiki/A%20Nightmare%20on%20Elm%20Street/A%20Nightmare%20on%20Elm%20Street%20(2010%20film).html']}\n",
      "{'question': 'Is the Batman comic similar to the movies?', 'gold_answer': 'I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane , two very wealthy and charitable Gotham City socialites'], 'pragmatic_spans': ['While returning home one night, his parents were killed by a small-time criminal named Joe Chill ']}\n",
      "{'question': \"what is batman's real name?\", 'gold_answer': 'Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['Bruce Wayne'], 'pragmatic_spans': ['Batman is a superhero co-created by artist Bob Kane and writer Bill Finger . The character made his first appearance in Detective Comics #27 (May, 1939). Batman is the secret identity of Bruce Wayne .']}\n",
      "{'question': 'How old was batman when he first became batman?', 'gold_answer': \"I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [\"I don't know\"], 'pragmatic_spans': ['his parents were killed', \"Bruce swears an oath to rid the city of the evil that had taken his parents' lives.\\n\\n\"]}\n",
      "{'question': 'Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?', 'gold_answer': 'No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': ['No'], 'pragmatic_spans': ['intellect, detective skills, science and technology, wealth, physical prowess, and intimidation in his war on crime.']}\n",
      "{'question': \"Who are Batman's biggest enemies?\", 'gold_answer': 'The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.', 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' the Joker and Catwoman'], 'pragmatic_spans': [' the Joker and Catwoman', 'a new supervillain called Mr. Bloom ']}\n",
      "{'question': 'What is Batmans real name?', 'gold_answer': \"Batman's real identity is Bruce Wayne. He lives in Gotham City and is the CEO of Wayne Enterprises.\", 'topic': 'Batman', 'genre': 'Comics', 'community': 'Batman', 'literal_spans': [' Batman is the secret identity of Bruce Wayne .'], 'pragmatic_spans': [' CEO of Wayne Enterprises, ']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(questions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775021",
   "metadata": {},
   "source": [
    "As can be seen from this excerpt, there are a few questions with no literal or pragmatic spans at all, and this is not an issue on my end as even the teachers themselves state that they cannot access these wikis in their answers (see first, third and fourth questions). \n",
    "Considering that, and considering that the NLP model requires context, I'm left with two choices:\n",
    "\n",
    "1) Filter out the problematic questions\n",
    "2) Ignore them and set the context to be the same as the question, which will likely lead to errors and underplay distilbert's performance.\n",
    "\n",
    "We'll go with the filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6def7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original questions: 179\n",
      "Valid questions: 174\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_questions(questions):\n",
    "    \n",
    "    valid_questions = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # Check if any spans are invalid (start with \"Cannot GET /wiki/\")\n",
    "        invalid_literal = any(span.startswith(\"Cannot GET /wiki/\") for span in q['literal_spans'])\n",
    "        invalid_pragmatic = any(span.startswith(\"Cannot GET /wiki/\") for span in q['pragmatic_spans'])\n",
    "        \n",
    "        # Keep only questions with valid spans in both configurations\n",
    "        if not invalid_literal and not invalid_pragmatic:\n",
    "            valid_questions.append(q)\n",
    "    \n",
    "    return valid_questions\n",
    "\n",
    "# Execute filtering\n",
    "print(f\"Original questions: {len(questions)}\")\n",
    "questions = filter_valid_questions(questions)\n",
    "print(f\"Valid questions: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b3b81",
   "metadata": {},
   "source": [
    "Five questions have been filtered, which is not a very substantial amount, so it shouldn't really affect our testing.\n",
    "\n",
    "Below is our model which will handle all three contexts - literal, pragmatic and retrieved spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertRAG:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "        retriever = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "        self.embedder = dspy.Embedder(retriever.encode)\n",
    "\n",
    "        self.search_dict = {}  # a cache for retrievers\n",
    "\n",
    "    def create_search(self, community, topk_docs_to_retrieve=5):\n",
    "        \n",
    "        if community in self.search_dict:\n",
    "            return self.search_dict[community]\n",
    "\n",
    "        if not community:\n",
    "            return \"No community given.\"\n",
    "        \n",
    "        directory = f'../PragmatiCQA-sources/{community}'\n",
    "        corpus = []\n",
    "        #just the read_html from rag.ipynb \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".html\"):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    corpus.append(soup.get_text())\n",
    "\n",
    "        search = dspy.retrievers.Embeddings(embedder=self.embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "        self.search_dict[community] = search\n",
    "\n",
    "        return search\n",
    "    \n",
    "    def answer(self, question, context_type):\n",
    "        if context_type == 'literal':\n",
    "            context = \" \".join(question['literal_spans'])\n",
    "\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = \" \".join(question['pragmatic_spans'])\n",
    "\n",
    "        elif context_type == 'rag':\n",
    "            search = self.create_search(question['community'], topk_docs_to_retrieve=3)\n",
    "            result = search(question['question'])\n",
    "            \n",
    "            # truncating each passage since we want to include multiple docs within a small token limit\n",
    "            truncated_passages = []\n",
    "            for passage in result.passages:\n",
    "                truncated_passages.append(passage[:500] + \"...\" if len(passage) > 500 else passage)\n",
    "            \n",
    "            context = \" \".join(truncated_passages)\n",
    "\n",
    "        else:\n",
    "            return \"[Invalid context_type]\"  \n",
    "\n",
    "        # calculate available space for context, this is necessary since we want to minimize context token length since we feed it to the LLM afterwards... and that costs money.\n",
    "        question_tokens = self.tokenizer.encode(question['question'], add_special_tokens=False)\n",
    "        max_context_length = 512 - len(question_tokens) - 3  # 3 for special tokens [CLS], [SEP], [SEP]\n",
    "        \n",
    "        context_tokens = self.tokenizer.encode(context, add_special_tokens=False)\n",
    "        if len(context_tokens) > max_context_length:\n",
    "            context_tokens = context_tokens[:max_context_length]\n",
    "            context = self.tokenizer.decode(context_tokens)\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question['question'], \n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512, #the model can't handle more than 512 tokens as input anyway, so we cap it to prevent errors.\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        start_idx = torch.argmax(outputs.start_logits).item()\n",
    "        end_idx = torch.argmax(outputs.end_logits).item()\n",
    "        \n",
    "        if end_idx < start_idx:\n",
    "            end_idx = start_idx\n",
    "            \n",
    "        tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "        answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer.strip() if answer.strip() else \"[No answer found]\",\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bae14",
   "metadata": {},
   "source": [
    "So I have checked out SemanticF1 - it does not compute all three scores, it really only computes the F1 score, and that's it. I checked all of its attributes, and found nothing else regarding precision and recall. So I'll also be using the function SemanticF1 calls (According to the documentation) instead.\n",
    "\n",
    "Below is a small test with two evaluator LMs as I wanted to see if I can handle using a local evaluator on my 6GB VRAM GFX. \n",
    "\n",
    "Spoilers: The local model performed pretty badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7df6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SigClass = DecompositionalSemanticRecallPrecision\n",
    "sig_module = ChainOfThought(SigClass)\n",
    "\n",
    "def semantic_scores(example, pred):\n",
    "    scores = sig_module(\n",
    "        question=example.question,\n",
    "        ground_truth=example.response,\n",
    "        system_response=pred.response\n",
    "    )\n",
    "    precision = scores.precision\n",
    "    recall = scores.recall\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def evaluate_model(model, n=5):\n",
    "    for i, q in enumerate(questions[:n]):\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Question {i+1}: {q['question']}\")\n",
    "        print(\"Gold Answer:\", q['gold_answer'])\n",
    "\n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            ans = model.answer(q, context_type)\n",
    "\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "\n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "\n",
    "            print(f\"\\n{context_type.capitalize()} Answer: {ans['answer']}\")\n",
    "            print(f\"  Precision: {scores['precision']:.2f}, Recall: {scores['recall']:.2f}, F1: {scores['f1']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c35c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== EVALUATION WITH QWEN 2.5 ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.25, Recall: 0.25, F1: 0.25\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 0.25, Recall: 0.50, F1: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 22:41:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================== EVALUATION WITH GROK-3-MINI ==============================\n",
      "============================================================\n",
      "Question 1: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "\n",
      "Literal Answer: Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha Kane, two very wealthy and charitable Gotham City socialites\n",
      "  Precision: 0.50, Recall: 0.25, F1: 0.33\n",
      "\n",
      "Pragmatic Answer: his parents were killed by a small - time criminal named Joe Chill\n",
      "  Precision: 1.00, Recall: 0.25, F1: 0.40\n",
      "\n",
      "Rag Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "============================================================\n",
      "Question 2: what is batman's real name?\n",
      "Gold Answer: Batman was created by Bob Kane and Bill Finger. His real identity is Bruce Wayne.\n",
      "\n",
      "Literal Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Pragmatic Answer: Bruce Wayne\n",
      "  Precision: 1.00, Recall: 0.50, F1: 0.67\n",
      "\n",
      "Rag Answer: Bruce Wayne Aliases\n",
      "  Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "============================================================\n",
      "Question 3: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "\n",
      "Literal Answer: I don't know\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Pragmatic Answer: Bruce\n",
      "  Precision: 1.00, Recall: 0.33, F1: 0.50\n",
      "\n",
      "Rag Answer: February 23, 1948\n",
      "  Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH QWEN 2.5\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_qwen = dspy.LM('ollama_chat/qwen2.5:3b', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=evaluator_lm_qwen)\n",
    "\n",
    "evaluate_model(model, n=3)\n",
    "\n",
    "print(\"=\"*30, \"EVALUATION WITH GROK-3-MINI\", \"=\"*30)\n",
    "\n",
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "evaluate_model(model, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c87a9",
   "metadata": {},
   "source": [
    "As can be seen above, the qwen 2.5 model is not good at evaluating. Frankly, neither is grok-3-mini. Answering \"Bruce\" to the question \"How old was batman when he first became batman\" should get a precision of near-zero, if not zero. Definitely not '1'. Anyway, I'll be proceeding with grok-3-mini as the evaluator model. \n",
    "Woe to the grok budget..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ea9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_questions(model, questions):\n",
    "    import time\n",
    "    import warnings\n",
    "    # i'm suppressing the output format warnings since they're frequent and annoying.\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Failed to use structured output format\")\n",
    "    \n",
    "    results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    detailed_results = {\"literal\": [], \"pragmatic\": [], \"rag\": []}\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions...\")\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"Processing question {i + 1}/{len(questions)}.\")\n",
    "            \n",
    "        for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "            \n",
    "            ans = model.answer(q, context_type)\n",
    "            \n",
    "            #creating an example like in the semanticf1 example. not sure if passing the context is strictly necessary; it'll be a massive waste of tokens in part 2.\n",
    "            gold_ex = dspy.Example(\n",
    "                question=q['question'],\n",
    "                response=q['gold_answer'],\n",
    "                inputs={'context': ans['context']}\n",
    "            )\n",
    "            pred_ex = dspy.Example(response=ans['answer'])\n",
    "            \n",
    "            scores = semantic_scores(gold_ex, pred_ex)\n",
    "            results[context_type].append(scores)\n",
    "            \n",
    "            # storing data for future analysis\n",
    "            detailed_results[context_type].append({\n",
    "                \"question\": q['question'],\n",
    "                \"gold_answer\": q['gold_answer'],\n",
    "                \"predicted_answer\": ans['answer'],\n",
    "                \"context\": ans['context'],\n",
    "                \"scores\": scores\n",
    "            })\n",
    "            \n",
    "            # short delay to avoid hitting limits\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # note that i can't use dspy.Evaluate since i need all three (or at least the first two) metrics individually...\n",
    "    avg_results = {}\n",
    "    for context_type in results:\n",
    "        if results[context_type]:  \n",
    "            avg_results[context_type] = {\n",
    "                \"precision\": sum(s[\"precision\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"recall\": sum(s[\"recall\"] for s in results[context_type]) / len(results[context_type]),\n",
    "                \"f1\": sum(s[\"f1\"] for s in results[context_type]) / len(results[context_type])\n",
    "            }\n",
    "    \n",
    "    json_data = {\n",
    "        \"summary\": avg_results,\n",
    "        \"detailed_results\": detailed_results\n",
    "    }\n",
    "    \n",
    "    with open('part_1_eval.json', 'w') as f:\n",
    "        json.dump(json_data, f)\n",
    "    \n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "def print_results_table(avg_results):\n",
    "    \n",
    "    print(\"\\nEVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Context Type':<20} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for context_type in [\"literal\", \"pragmatic\", \"rag\"]:\n",
    "        if context_type in avg_results:\n",
    "            precision = avg_results[context_type][\"precision\"]\n",
    "            recall = avg_results[context_type][\"recall\"]\n",
    "            f1 = avg_results[context_type][\"f1\"]\n",
    "            print(f\"{context_type.capitalize():<20} {precision:<12f} {recall:<12f} {f1:<12}\")\n",
    "        else:\n",
    "            print(f\"{context_type.capitalize():<20} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179ba0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 174 questions...\n",
      "Processing question 1/174.\n",
      "Processing question 2/174.\n",
      "Processing question 3/174.\n",
      "Processing question 4/174.\n",
      "Processing question 5/174.\n",
      "Processing question 6/174.\n",
      "Processing question 7/174.\n",
      "Processing question 8/174.\n",
      "Processing question 9/174.\n",
      "Processing question 10/174.\n",
      "Processing question 11/174.\n",
      "Processing question 12/174.\n",
      "Processing question 13/174.\n",
      "Processing question 14/174.\n",
      "Processing question 15/174.\n",
      "Processing question 16/174.\n",
      "Processing question 17/174.\n",
      "Processing question 18/174.\n",
      "Processing question 19/174.\n",
      "Processing question 20/174.\n",
      "Processing question 21/174.\n",
      "Processing question 22/174.\n",
      "Processing question 23/174.\n",
      "Processing question 24/174.\n",
      "Processing question 25/174.\n",
      "Processing question 26/174.\n",
      "Processing question 27/174.\n",
      "Processing question 28/174.\n",
      "Processing question 29/174.\n",
      "Processing question 30/174.\n",
      "Processing question 31/174.\n",
      "Processing question 32/174.\n",
      "Processing question 33/174.\n",
      "Processing question 34/174.\n",
      "Processing question 35/174.\n",
      "Processing question 36/174.\n",
      "Processing question 37/174.\n",
      "Processing question 38/174.\n",
      "Processing question 39/174.\n",
      "Processing question 40/174.\n",
      "Processing question 41/174.\n",
      "Processing question 42/174.\n",
      "Processing question 43/174.\n",
      "Processing question 44/174.\n",
      "Processing question 45/174.\n",
      "Processing question 46/174.\n",
      "Processing question 47/174.\n",
      "Processing question 48/174.\n",
      "Processing question 49/174.\n",
      "Processing question 50/174.\n",
      "Processing question 51/174.\n",
      "Processing question 52/174.\n",
      "Processing question 53/174.\n",
      "Processing question 54/174.\n",
      "Processing question 55/174.\n",
      "Processing question 56/174.\n",
      "Processing question 57/174.\n",
      "Processing question 58/174.\n",
      "Processing question 59/174.\n",
      "Processing question 60/174.\n",
      "Processing question 61/174.\n",
      "Processing question 62/174.\n",
      "Processing question 63/174.\n",
      "Processing question 64/174.\n",
      "Processing question 65/174.\n",
      "Processing question 66/174.\n",
      "Processing question 67/174.\n",
      "Processing question 68/174.\n",
      "Processing question 69/174.\n",
      "Processing question 70/174.\n",
      "Processing question 71/174.\n",
      "Processing question 72/174.\n",
      "Processing question 73/174.\n",
      "Processing question 74/174.\n",
      "Processing question 75/174.\n",
      "Processing question 76/174.\n",
      "Processing question 77/174.\n",
      "Processing question 78/174.\n",
      "Processing question 79/174.\n",
      "Processing question 80/174.\n",
      "Processing question 81/174.\n",
      "Processing question 82/174.\n",
      "Processing question 83/174.\n",
      "Processing question 84/174.\n",
      "Processing question 85/174.\n",
      "Processing question 86/174.\n",
      "Processing question 87/174.\n",
      "Processing question 88/174.\n",
      "Processing question 89/174.\n",
      "Processing question 90/174.\n",
      "Processing question 91/174.\n",
      "Processing question 92/174.\n",
      "Processing question 93/174.\n",
      "Processing question 94/174.\n",
      "Processing question 95/174.\n",
      "Processing question 96/174.\n",
      "Processing question 97/174.\n",
      "Processing question 98/174.\n",
      "Processing question 99/174.\n",
      "Processing question 100/174.\n",
      "Processing question 101/174.\n",
      "Processing question 102/174.\n",
      "Processing question 103/174.\n",
      "Processing question 104/174.\n",
      "Processing question 105/174.\n",
      "Processing question 106/174.\n",
      "Processing question 107/174.\n",
      "Processing question 108/174.\n",
      "Processing question 109/174.\n",
      "Processing question 110/174.\n",
      "Processing question 111/174.\n",
      "Processing question 112/174.\n",
      "Processing question 113/174.\n",
      "Processing question 114/174.\n",
      "Processing question 115/174.\n",
      "Processing question 116/174.\n",
      "Processing question 117/174.\n",
      "Processing question 118/174.\n",
      "Processing question 119/174.\n",
      "Processing question 120/174.\n",
      "Processing question 121/174.\n",
      "Processing question 122/174.\n",
      "Processing question 123/174.\n",
      "Processing question 124/174.\n",
      "Processing question 125/174.\n",
      "Processing question 126/174.\n",
      "Processing question 127/174.\n",
      "Processing question 128/174.\n",
      "Processing question 129/174.\n",
      "Processing question 130/174.\n",
      "Processing question 131/174.\n",
      "Processing question 132/174.\n",
      "Processing question 133/174.\n",
      "Processing question 134/174.\n",
      "Processing question 135/174.\n",
      "Processing question 136/174.\n",
      "Processing question 137/174.\n",
      "Processing question 138/174.\n",
      "Processing question 139/174.\n",
      "Processing question 140/174.\n",
      "Processing question 141/174.\n",
      "Processing question 142/174.\n",
      "Processing question 143/174.\n",
      "Processing question 144/174.\n",
      "Processing question 145/174.\n",
      "Processing question 146/174.\n",
      "Processing question 147/174.\n",
      "Processing question 148/174.\n",
      "Processing question 149/174.\n",
      "Processing question 150/174.\n",
      "Processing question 151/174.\n",
      "Processing question 152/174.\n",
      "Processing question 153/174.\n",
      "Processing question 154/174.\n",
      "Processing question 155/174.\n",
      "Processing question 156/174.\n",
      "Processing question 157/174.\n",
      "Processing question 158/174.\n",
      "Processing question 159/174.\n",
      "Processing question 160/174.\n",
      "Processing question 161/174.\n",
      "Processing question 162/174.\n",
      "Processing question 163/174.\n",
      "Processing question 164/174.\n",
      "Processing question 165/174.\n",
      "Processing question 166/174.\n",
      "Processing question 167/174.\n",
      "Processing question 168/174.\n",
      "Processing question 169/174.\n",
      "Processing question 170/174.\n",
      "Processing question 171/174.\n",
      "Processing question 172/174.\n",
      "Processing question 173/174.\n",
      "Processing question 174/174.\n"
     ]
    }
   ],
   "source": [
    "evaluator_lm_grok = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=evaluator_lm_grok)\n",
    "\n",
    "model = DistilbertRAG()\n",
    "\n",
    "avg_results = evaluate_all_questions(model, questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235f59",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68642cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Context Type         Precision    Recall       F1          \n",
      "--------------------------------------------------\n",
      "Literal              0.816571     0.289534     0.40379242303802887\n",
      "Pragmatic            0.744828     0.268122     0.3715092992171465\n",
      "Rag                  0.247222     0.085625     0.11761999779137157\n"
     ]
    }
   ],
   "source": [
    "print_results_table(avg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d533",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Below I'll be printing some excerpts of questions, and then analyzing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08bd2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS OF EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "1. RAG EXAMPLES WITH F1 SCORE = 0.0\n",
      "----------------------------------------\n",
      "Found 126 RAG examples with F1 = 0.0\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1 (Question #1):\n",
      "Question: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "RAG Answer: The Batman film franchise consists of a total of nine theatrical live - action films and two live - action serials featuring the DC Comics superhero Batman\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 2 (Question #3):\n",
      "Question: How old was batman when he first became batman?\n",
      "Gold Answer: I don't know. It is not clear when Bruce Wayne becomes Batman, but he becomes Batman sometime after his parents die.\n",
      "RAG Answer: February 23, 1948\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 3 (Question #4):\n",
      "Question: Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?\n",
      "Gold Answer: No, Batman has no super powers like other super heroes because he only relies on his intellect, detective skills, his wealth, physical prowess, aggressiveness, science and technology when he fights crime.\n",
      "RAG Answer: invisibility\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 4 (Question #5):\n",
      "Question: Who are Batman's biggest enemies?\n",
      "Gold Answer: The Joker and Catwoman are original enemies of Batman. However, there are numerous others one such being the super villain Mr. Bloom.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "Example 5 (Question #7):\n",
      "Question: Ok, Is batman a superhero?\n",
      "Gold Answer: Yes, he has been the protector of Gotham City for a long time.\n",
      "RAG Answer: Bruce\n",
      "F1 Score: 0.0\n",
      "----------------------------------------\n",
      "\n",
      "2. PRAGMATIC EXAMPLES WITH F1 SCORE <= 0.2\n",
      "----------------------------------------\n",
      "Found 44 Pragmatic examples with F1 <= 0.2\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1 (Question #11):\n",
      "Question: how old is batman?\n",
      "Gold Answer: Batman made his first appearence in media in May of 1939, so he is quite old. He premiered as a vigilante.\n",
      "Pragmatic Answer: Batman\n",
      "Precision: 1.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n",
      "----------------------------------------\n",
      "Example 2 (Question #13):\n",
      "Question: what is batman's real name? \n",
      "Gold Answer: Bruce Wayne is the real name of Batman who, after witnessing the murder of his parents as a child, donned a bat-themed costume in order to fight crime.\n",
      "Pragmatic Answer: his parents\n",
      "Precision: 1.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n",
      "----------------------------------------\n",
      "Example 3 (Question #15):\n",
      "Question: what year was it release? \n",
      "Gold Answer: He first appeared in May 1939. The comic's name was Detective comics.\n",
      "Pragmatic Answer: 27\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n",
      "----------------------------------------\n",
      "Example 4 (Question #20):\n",
      "Question: Who is batman?\n",
      "Gold Answer: http://learning-environment-optimisation.us-east-2.elasticbeanstalk.com/experiment/?mTurkId=A3G8JHYINIOP64&surveyId=https://cambridge.eu.qualtrics.com/jfe/form/SV_9snnYxpUcF3ZreK\n",
      "Pragmatic Answer: Batman\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n",
      "----------------------------------------\n",
      "Example 5 (Question #24):\n",
      "Question: Hi. What is Batman's name?\n",
      "Gold Answer: Batman's real name is Bruce Wayne. Bruce Wayne, born to Thomas Wayne and Martha Kane, is the CEO of Wayne Enterprise, who also protects Gotham as Batman.\n",
      "Pragmatic Answer: Martha Kane\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('part_1_eval.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"ANALYSIS OF EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Find RAG examples with F1 score of 0.0\n",
    "print(\"\\n1. RAG EXAMPLES WITH F1 SCORE = 0.0\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "rag_zero_examples = []\n",
    "for i, result in enumerate(data['detailed_results']['rag']):\n",
    "    if result['scores']['f1'] == 0.0:\n",
    "        rag_zero_examples.append((i, result))\n",
    "\n",
    "print(f\"Found {len(rag_zero_examples)} RAG examples with F1 = 0.0\")\n",
    "print(\"\\nShowing first 5 examples:\\n\")\n",
    "\n",
    "for idx, (i, example) in enumerate(rag_zero_examples[:5]):\n",
    "    print(f\"Example {idx + 1} (Question #{i + 1}):\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"RAG Answer: {example['predicted_answer']}\")\n",
    "    print(f\"F1 Score: {example['scores']['f1']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# 2. Find Pragmatic examples with F1 score <= 0.2\n",
    "print(\"\\n2. PRAGMATIC EXAMPLES WITH F1 SCORE <= 0.2\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "pragmatic_low_examples = []\n",
    "for i, result in enumerate(data['detailed_results']['pragmatic']):\n",
    "    if result['scores']['f1'] <= 0.2:\n",
    "        pragmatic_low_examples.append((i, result))\n",
    "\n",
    "print(f\"Found {len(pragmatic_low_examples)} Pragmatic examples with F1 <= 0.2\")\n",
    "print(\"\\nShowing first 5 examples:\\n\")\n",
    "\n",
    "for idx, (i, example) in enumerate(pragmatic_low_examples[:5]):\n",
    "    print(f\"Example {idx + 1} (Question #{i + 1}):\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "    print(f\"Pragmatic Answer: {example['predicted_answer']}\")\n",
    "    print(f\"Precision: {example['scores']['precision']:.3f}\")\n",
    "    print(f\"Recall: {example['scores']['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {example['scores']['f1']:.3f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
